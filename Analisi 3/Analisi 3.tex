\documentclass[a4paper,12pt]{report}
\usepackage[a4paper=true,pagebackref=true]{hyperref}
\usepackage[italian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\theoremstyle{plain}
\newtheorem{thm}{Teorema}[section]
\newtheorem{cor}{Corollario}[thm]
\newtheorem{lem}{Lemma}[section]
\newtheorem{prop}{Proposizione}[section]
\newtheorem{rem}{Nota}[section]
\newtheorem{exm}{Esempio}[section]
\theoremstyle{definition}
\newtheorem{defn}{Definizione}[section]
\theoremstyle{remark}
\newtheorem{oss}{Osservazione}[section]
\hyphenation{sot-traia-mo}
\hyphenation{li-mi-te}
\hyphenation{ge-ne-ra-le}
\newcommand{\Div}{\mathrm{div}\,}
\newcommand{\Rot}{\mathrm{rot}\,}
\newcommand{\ov}[1]{\overline{#1}}
\newcommand{\BF}[1]{\boldsymbol{#1}}
\newcommand{\diff}[1]{\mathrm{d}#1}
\newcommand{\dev}[3][]{\frac{\mathrm{d}^{#1}#2}{\mathrm{d}#3^{#1}}}
\newcommand{\pdev}[3][]{\frac{\partial^{#1}#2}{\partial #3^{#1}}}
\numberwithin{equation}{section}


\begin{document}
 \begin{titlepage}
\centering
{\Huge Analisi Matematica III}\\
\vspace*{0.5cm}
{\small Appunti (non rivisti) delle lezioni del professor Paolo Acquistapace}
\vspace*{\stretch{0.5}} \\
\includegraphics[width=250pt,keepaspectratio=true]{Addons/eigenLibrichiaro}
\begin{center}
un progetto di
\end{center}
\includegraphics[width=250pt,keepaspectratio=true]{Addons/eigenlabinvertito2.png} \\
\url{www.eigenlab.org}
\vspace*{\stretch{1}} \\
{\small a cura di}\\
\vspace*{0.5cm}
{\normalsize Francesco Cicciarella\par}
\end{titlepage}
\pagebreak

\section*{Note legali}
\begin{center}
\begin{figure}[htbp]
\centering
\includegraphics[scale=1]{Addons/88x31.png}
\end{figure}
\vspace{0.5cm}
Copyright \copyright \; 2011-2012 di Francesco Cicciarella \\
\textit{Appunti di Analisi Matematica 3} \\	
è rilasciato sotto i termini della licenza \\
Creative Commons Attribuzione - Non commerciale - Condividi allo stesso modo 3.0 Italia. \\
Per visionare una copia completa della licenza, visita \\
\url{http://creativecommons.org/licenses/by-nc-sa/3.0/it/legalcode}
\end{center}
\section*{Liberatoria, mantenimento e segnalazione errori}
Questo documento viene pubblicato, in formato elettronico, senza alcuna garanzia di correttezza del suo contenuto. Il testo, nella sua interezza, è opera di \\

\vspace{0.3cm}
\begin{flushleft}
\texttt{Francesco Cicciarella}\\
\texttt{<f[DOT]cicciarella[AT]inventati[DOT]org>}
\end{flushleft}
\vspace{0.3cm}
e viene mantenuto dallo stesso, a cui possono essere inviate eventuali segnalazioni di errori.
\vspace{1cm}
\begin{flushright}
Pisa, 10 Ottobre 2012
\end{flushright}
\pagebreak

\tableofcontents
\chapter{Equazioni differenziali}
\section{Introduzione}
Un'equazione differenziale è un'identità che lega fra di loro, per ogni valore della variabile $x$ in un dato insieme, i valori della 
funzione incognita $u(x)$ e quelli delle sue derivate $u'(x),u''(x),\ldots$. Un'equazione differenziale si presenta nella forma:
\begin{equation}
f(\left(x,u(x),\ldots,u^{(m)}(x)\right)=0\;, \qquad \forall x\in I\;,
\end{equation}
dove $m$ è detto \textit{ordine} dell'equazione.
\begin{defn} Un'equazione differenziale di ordine $m$ si dice in \textit{forma normale} se è della forma:
\begin{equation}
u^{(m)}(x)=g(x,u(x),u'(x),\ldots,u^{(m-1)}(x))\;,
\end{equation}
\end{defn}
Accanto alle equazioni si considerano anche i sistemi differenziali di prim'ordine:
\begin{equation}
\mathbf{f}(x,\mathbf{u}(x),\mathbf{u}'(x))=0\;, \qquad x\in I\;,
\end{equation}
e in forma normale:
\begin{equation}
\mathbf{u}'(x)=\mathbf{g}(x,\mathbf{u}(x))\;, \qquad \mathbf{g}:I\to\mathbb{R}^m\;.
\end{equation}
\begin{prop} Un'equazione differenziale di ordine $m$ è sempre equivalente ad un sistema differenziale del prim'ordine in $m$ equazioni.
\end{prop}
\proof Se $y \in C^m(I)$ risolve l'equazione $f(x,y,y',\ldots,y^{(m)})=0$, introducendo le $m$ funzioni:
\begin{equation}
\begin{matrix}
&u_0(x)=y(x)\;, &u_1(x)=y'(x)\;, &\ldots\;, &u_{m-1}(x)=y^{(m-1)}(x)\;,
\end{matrix}
\end{equation}
si ottiene una funzione $\mathbf{u}=(u_0,u_1,\ldots,u_{m-1}) \in C^1(I, \mathbb{R}^m)$ che risolve il sistema differenziale:
\begin{equation}
\begin{cases}
 u_0'=u_1\;, \\
u_1'=u_2\;, \\
\ldots \ldots \ldots \\
u_{m-2}'=u_{m-1}\;, \\
u_{m-1}'=u_m=g(x,u_0,u_1,\ldots,u_{m-1})\;,
\end{cases}
\end{equation}
che è un sistema differenziale del primo ordine in $m$ equazioni.
\endproof
\section{Teorema di esistenza e unicità locale delle soluzioni di un'equazione differenziale}
 Consideriamo il sistema:
\begin{equation}
\mathbf{u}'=\mathbf{g}(x,\mathbf{u})\;, \qquad x \in I\;,
\end{equation}
sotto le seguenti ipotesi:
\begin{enumerate}
 \item $\mathbf{g}:A \to \mathbb{R}^{m}$ è un'assegnata funzione continua, definita su un aperto $A \subseteq \mathbb{R}^{m+1}$;
 \item $\mathbf{g}$ è localmente Lipschitziana in $A$ rispetto alla variabile vettoriale $\mathbf{u}$ e uniformemente rispetto alla 
variabile $x$, ossia per ogni compatto $K \subset A,\; \exists H_K \ge 0$ tale che:
\begin{equation}
|\mathbf{g}(x,\mathbf{y})-\mathbf{g}(x,\mathbf{u})|_m \le H_K |\mathbf{y}-\mathbf{u}|_m\;, \qquad \forall (x,\mathbf{y}),(x,\mathbf{u}) \in K\;.
\end{equation}
\end{enumerate}
Fissiamo un punto $(x_0,\mathbf{u}_0) \in A$ e consideriamo il problema di Cauchy:
\begin{equation}
\begin{cases}
 \mathbf{u}'=\mathbf{g}(x,\mathbf{u})\;, \\
 \\
\mathbf{u}(x_0)=\mathbf{u}_0\;.
\end{cases}
\end{equation}
Dato che $A$ è un aperto, esisterà un 'cilindro' $(m+1)$-dimensionale compatto, che denotiamo $R$, di centro $(x_0,\mathbf{u}_0)$, 
strettamente contenuto in $A$. Esso sarà della forma:
\begin{equation}
R=\left\{|x-x_0|\le a, |\mathbf{u}-\mathbf{u}_0|_m\le b\;, \mbox{con}\; (x,\mathbf{u})\in\mathbb{R}\times\mathbb{R}^m\right\}\;.
\end{equation}
Poiché $\mathbf{g}$ è continua nel compatto $R$, per il teorema di Weierstrass $\exists M\ge 0$ tale che:
\begin{equation}
|\mathbf{g}(x,\mathbf{u})|_{m} \le M\;, \qquad \forall (x,\mathbf{u}) \in R\;.
\end{equation}
Inoltre, poiché $R$ è compatto, per l'ipotesi 2 si ha che $\exists H \ge 0$ tale che:
\begin{equation}
|\mathbf{g}(x,\mathbf{y})-\mathbf{g}(x,\mathbf{u})|_m \le H |\mathbf{y}-\mathbf{u}|_m\;, \qquad \forall (x,\mathbf{y}),(x,\mathbf{u}) \in R\;.
\end{equation}
\begin{thm}[Esistenza ed unicità locale della soluzione di un'equazione differenziale]
 Nelle ipotesi siffatte, $\exists J=[x_0-h,x_0+h]$ con $0<h\le a$ e $\exists!\; \mathbf{u}:J \to \mathbb{R}^m$ di classe $C^1$ tali che:
\begin{equation}
\begin{cases}
 \mathbf{u}'=\mathbf{g}(x,\mathbf{u})\;, \qquad \forall x \in J\;, \\
 \\
\mathbf{u}(x_0)=\mathbf{u}_0\;,
\end{cases}
\end{equation}
e inoltre:
\begin{equation}
|\mathbf{u}(x)-\mathbf{u}_0|_{m} \le b \;, \qquad \forall x \in J\;.
\end{equation}
\end{thm}
\proof (Esistenza). Trasformiamo il problema di Cauchy in un sistema di equazioni integrali, cioè dimostriamo che:
\begin{equation}
\begin{cases}
	\mathbf{u}'=\mathbf{g}(x,\mathbf{u})\qquad \forall x\in J\;, \\
	\\
	\mathbf{u}(x_0)=\mathbf{u}_0\;,
	\end{cases}\; \mbox{con}\; \mathbf{u}\in C^1(J,\mathbb{R}^m)\;, \label{ch1_differenziale}
\end{equation}
e:
\begin{equation}
\mathbf{u}(x)=\mathbf{u}_0+\int_{x_0}^x\mathbf{g}(t,\mathbf{u}\,\diff{t}\;,\qquad \mbox{con}\; x\in J\,, \mathbf{u}\in C^0(J,\mathbb{R}^m)\;, \label{ch1_integrale}
\end{equation}
sono equivalenti. Infatti, sia $\mathbf{u}$ soluzione del problema \eqref{ch1_differenziale}. Integrando a membro a membro da $x_0$ e $x \in J$:
\begin{equation}
\int_{x_0}^x \mathbf{u}'(t)\;\diff{t} = \int_{x_0}^x \mathbf{g}(t,\mathbf{u})\;\diff{t}\qquad \Longleftrightarrow\qquad \mathbf{u}(x)-\mathbf{u}(x_0)=\int_{x_0}^x \mathbf{g}(t, \mathbf{u})\;\diff{t}\;.
\end{equation}
Da cui, ricordando che $\mathbf{u}(x_0)=\mathbf{u}_0$, segue:
\begin{equation}
\mathbf{u}(x)=\mathbf{u_0}+\int_{x_0}^x \mathbf{g}(t,\mathbf{u})\;\diff{t}\;,
\end{equation}
e quindi $\mathbf{u}$ è soluzione del problema \eqref{ch1_integrale}. Viceversa, sia $\mathbf{u} \in C^0(J,\mathbb{R}^m)$ soluzione del problema \eqref{ch1_integrale}, allora:
\begin{equation}
\mathbf{u}(x)=\mathbf{u}_0+\int_{x_0}^x \mathbf{g}(t,\mathbf{u})\;\diff{t}\;.
\end{equation}
Ma per ipotesi, $\mathbf{u}_0$ e $\int_{x_0}^x \mathbf{g}(t,\mathbf{u})\diff{t}$ sono di classe $C^1$, quindi automaticamente anche $\mathbf{u}(x) \in C^1$, quindi, derivando ambo i membri, si ottiene:
\begin{align}
\mathbf{u}'(x) &= \mathbf{g}(x,\mathbf{u})\;, \notag \\
\mathbf{u}(x_0)&=\mathbf{u}_0\;,
\end{align}
da cui segue che $\mathbf{u}$ è soluzione del problema \eqref{ch1_differenziale}. Dimostrata l'equivalenza, risolviamo il problema \eqref{ch1_integrale}:
\begin{equation}
\begin{cases}
 \mathbf{u} \in C^0(J,\mathbb{R}^m)\;, \\
 \\
 \mathbf{u}(x)=\mathbf{u}_0+\displaystyle{\int_{x_0}^x \mathbf{g}(t,\mathbf{u})\;\diff{t}}\;.
\end{cases}
\end{equation}
Posto $h=\min\{a,b/M,1/H\}$, usiamo il \textit{metodo delle approssimazioni successive}. Definiamo la successione
$\{ \mathbf{u}_n(x)\} \in C^1(J,\mathbb{R}^m)\; \forall n$ per ricorrenza:
\begin{equation}
\begin{cases}
 \mathbf{u}_0(x)=\mathbf{u}_0 \qquad \forall x \in J\;, \\
 \\
 \mathbf{u}_{n+1}(x)=\mathbf{u}_0+\displaystyle{\int_{x_0}^x \mathbf{g}(t,\mathbf{u}_n)\;\diff{t}} \qquad \forall n \in \mathbb{N}\;.
\end{cases}
\end{equation}
Valgono le seguenti relazioni:
\begin{align}
&\sup_{x\in J}|\mathbf{u}_n(x)-\mathbf{u}_0|_m\le b\qquad \forall n\in\mathbb{N}\;, \label{ch1_rel1} \\
&|\mathbf{u}_{n+1}(x)-\mathbf{u}_n(x)|_m\le M\frac{H^n}{(n+1)!}|x-x_0|^{n+1}\;. \label{ch2_rel2}
\end{align}
Dimostriamo la \eqref{ch1_rel1} per induzione su $n$. Per $n=0$ è banalmente vera, dimostriamo quindi che $n \Longrightarrow n+1$.
\begin{equation}
|\mathbf{u}_{n+1}(x)-\mathbf{u}_0|_m = \left|\int_{x_0}^x \mathbf{g}(t,\mathbf{u}_n)\;\diff{t}\right|_m \le \left|\int_{x_0}^x |\mathbf{g}
(t,\mathbf{u}_n)\;\diff{t}|_m \right|\;.
\end{equation}
Poiché $(t,\mathbf{u}_n(t))\in R$, per ipotesi si ha:
\begin{equation}
|\mathbf{u}_{n+1}(x)-\mathbf{u}_0|_m \le \left| \int_{x_0}^x M\;dt \right|=M|x-x_0|\le M\cdot h \le M \frac{b}{M}=b\;.
\end{equation}
Dimostriamo anche la \eqref{ch2_rel2} per induzione su $n$. Per $n=0$:
\begin{equation}
|\mathbf{u}_1(x)-\mathbf{u}_0|_m = \left| \int_{x_0}^x \mathbf{g}(t,\mathbf{u}_0)\;\diff{t}\right|_m \le \left| \int_{x_0}^x |\mathbf{g}(t,
\mathbf{u}_0)|_m\;\diff{t}\right| \le M|x-x_0|=M\frac{H^0}{1!}|x-x_0|^{0+1}\;,
\end{equation}
la proposizione è vera. Dimostriamo che $n \Longrightarrow n+1$:
\begin{align}
|\mathbf{u}_{n+2}(x)-\mathbf{u}_{n+1}(x)|_m &= \left|\int_{x_0}^x\mathbf{g}(t,\mathbf{u}_{n+1})\;\diff{t}-\int_{x_0}^x\mathbf{g}(t,\mathbf{u}_n)\;\diff{t}\right|_m \notag \\
&\le \left|\int_{x_0}^x|\mathbf{g}(t,\mathbf{u}_{n+1})-\mathbf{g}(t,\mathbf{u}_n)|_m\;\diff{t}\right|\;.
\end{align}
Per l'ipotesi di locale Lipschitzianità, si ha:
\begin{equation}
|\mathbf{u}_{n+2}(x)-\mathbf{u}_{n+1}(x)|_m \le H \left|\int_{x_0}^x |\mathbf{u}_{n+1}(t)-\mathbf{u}_n(t)|_m\;\diff{t}\right|\;.
\end{equation}
Per l'ipotesi induttiva, segue che:
\begin{align}
|\mathbf{u}_{n+2}-\mathbf{u}_{n+1}(x)|_m &\le H\left|\int_{x_0}^x\frac{MH^n}{(n+1)!}|t-x_0|^{n+1}\,\diff{t}\right|=M\frac{H^{n+1}}{(n+1)!}\int_{x_0}^x|t-x_0|^{n+1}\,\diff{t} \notag \\
&= M\frac{H^{n+1}}{(n+1)!}\frac{|x-x_0|^{n+2}}{n+2}=M\frac{H^{n+1}}{(n+2)!}|x-x_0|^{n+2}\;.
\end{align}
Dalla seconda relazione appena dimostrata segue che:
\begin{equation}
\sup_{x \in J} |\mathbf{u}_{n+1}(x)-\mathbf{u}_n(x)|_m \le \frac{MH^n}{(n+1)!}h^{n+1}\;.
\end{equation}
Allora $\forall \epsilon>0\; \exists \nu_{\epsilon}$ tale che $\forall p,n>\nu_{\epsilon}, p>n$ si abbia:
\begin{equation}
\sup_{x \in J} |\mathbf{u}_p(x)-\mathbf{u}_n(x)|_m \le \sum_{i=n}^{p-1} \sup_{x \in J} |\mathbf{u}_{i+1}(x)-\mathbf{u}_i(x)|_m
\le M \sum_{i=n}^{p-1} \frac{H^i h^{i+1}}{(i+1)!} < \epsilon\;.
\end{equation}
Concludiamo che $\forall x \in J$ la successione $\{ \mathbf{u}_n(x)\}_{n \in \mathbb{N}}$ è di Cauchy in $\mathbb{R}^m$, quindi esiste:
\begin{equation}
\lim_{n \to +\infty} \mathbf{u}_n(x)=\mathbf{u}(x) \qquad \forall x \in J\;,
\end{equation}
e inoltre, $\forall n>\nu_{\epsilon}$ si ha:
\begin{equation}
\sup_{x \in J} |\mathbf{u}_n(x)-\mathbf{u}(x)|_m < \epsilon\;.
\end{equation}
Quindi la successione $\{ \mathbf{u}_n(x)\}$ converge uniformemente a $\mathbf{u}(x)$ in $J$. Eseguendo il limite per $n \to +\infty$
nella relazione:
\begin{equation}
\mathbf{u}_{n+1}(x)=\mathbf{u}_0+\int_{x_0}^x \mathbf{g}(t,\mathbf{u}_n)\;\diff{t}\;,
\end{equation}
si ha $\mathbf{u}_{n+1}(x) \to \mathbf{u}(x)$, mentre:
\begin{equation}
\left| \int_{x_0}^x \mathbf{g}(t,\mathbf{u}_n)\;\diff{t} - \int_{x_0}^x \mathbf{g}(t,\mathbf{u})\;\diff{t}\right|_m \le
\left| \int_{x_0}^x |\mathbf{g}(t,\mathbf{u}_n) - \mathbf{g}(t,\mathbf{u})|_m\;\diff{t}\right|\;.
\end{equation}
Per l'ipotesi di locale Lipschitzianità si ha:
\begin{equation}
\left|\int_{x_0}^x\mathbf{g}(t,\mathbf{u}_n)\,\diff{t}-\int_{x_0}^x\mathbf{g}(t,\mathbf{u})\,\diff{t}\right|_m \le H \left|\int_{x_0}^x |\mathbf{u}_n(t)-\mathbf{u}(t)|_m\;dt\right| \le Hh\epsilon\;,
\end{equation}
per ogni $n>\nu_{\epsilon}$. Si conclude pertanto che:
\begin{equation}
\lim_{n \to +\infty} \int_{x_0}^x \mathbf{g}(t,\mathbf{u}_n)\;\diff{t} = \int_{x_0}^x \mathbf{g}(t,\mathbf{u})\;\diff{t}\;,
\end{equation}
che dimostra l'esistenza.
\endproof
\proof (Unicità). Siano $\mathbf{u},\mathbf{v}\in C^0(J,\mathbb{R}^m)$ che risolvono il problema \eqref{ch2_rel2} con $\mathbf{u} \ne \mathbf{v}$ e che soddisfano:
\begin{align}
&|\mathbf{u}(t)-\mathbf{u}_0|_m\le b\;, &|\mathbf{v}(t)-\mathbf{v}_0|_m\le b\;.
\end{align}
Siano $h'<h$, $J'=[x_0-h',x_0+h']$ e $x \in J'$. Allora:
\begin{align}
|\mathbf{u}(x)-\mathbf{v}(x)|_m &=\left|\int_{x_0}^x(\mathbf{g}(t,\mathbf{u})-\mathbf{g}(t,\mathbf{v}))\,\diff{t}\right|_m\le H \left|\int_{x_0}^x |\mathbf{u}(t)-\mathbf{v}(t)|_m\,\diff{t}\right| \notag \\
&\le Hh'\sup_{x\in J'}|\mathbf{u}(x)-\mathbf{v}(x)|_m\;,
\end{align}
da cui segue:
\begin{equation}
\sup_{x \in J'} |\mathbf{u}(x)-\mathbf{v}(x)|_m \le Hh' \sup_{x \in J'} |\mathbf{u}(x)-\mathbf{v}(x)|_m\;.
\end{equation}
Ma $Hh'<1$, e quindi la diseguaglianza è assurda. Allora $\mathbf{u} \equiv \mathbf{v}$ su tutto $J'$ per ogni $ J'\subset J$ e quindi
su tutto $J$. Il che dimostra l'unicità.
\endproof
\begin{oss} La soluzione di un'equazione differenziale può essere prolungata fino alla frontiera di un qualunque rettangolo contenuto nell'aperto.
\end{oss}
\begin{prop}[Dipendenza continua dal dato iniziale] Consideriamo i problemi di Cauchy:
\begin{align}
&\begin{cases}
\mathbf{u}'(x)=\mathbf{g}(x,\mathbf{u})\qquad x\in J\;, \\
\\
\mathbf{u}(x_0)=\mathbf{u}_0\;,
\end{cases}
&\begin{cases}
\mathbf{v}'(x)=\mathbf{g}(x,\mathbf{v})\qquad x\in J'\;,
\end{cases}
\end{align}
Sia $J \cap J'=J''=[x_0-h,x_0+h]$. Allora $\exists c>0$ tale che:
\begin{equation}
\sup_{x \in J''} |\mathbf{u}(x)-\mathbf{v}(x)|_m \le c|\mathbf{u_0}-\mathbf{v_0}|_m\;.
\end{equation}
\end{prop}
\proof Siano:
\begin{align}
\mathbf{u}(x) &= \mathbf{u}_0+\int_{x_0}^x\mathbf{g}(t,\mathbf{u})\,\diff{t}\;,\qquad x\in J''\;, \notag \\
\mathbf{v}(x)&= \mathbf{v}_0+\int_{x_0}^x\mathbf{g}(t,\mathbf{v})\,\diff{t}\;,\qquad x\in J''\;.
\end{align}
Allora:
\begin{align}
|\mathbf{u}(x)-\mathbf{v}(x)|_m &\le |\mathbf{u}_0-\mathbf{v}_0|_m+\left|\int_{x_0}^x[\mathbf{g}(t,\mathbf{u})-\mathbf{g}(t,\mathbf{v})]\;\diff{t}\right|_m \notag \\
&\le |\mathbf{u}_0-\mathbf{v}_0|_m+\left|\int_{x_0}^x |\mathbf{g}(t,\mathbf{u})-\mathbf{g}(t,\mathbf{v})|_m\diff{t}\right|\;.
\end{align}
Per l'ipotesi di Lipschitzianità, segue:
\begin{equation}
|\mathbf{u}(x)-\mathbf{v}(x)|_m \le |\mathbf{u}_0-\mathbf{v}_0|_m + H\left|\int_{x_0}^x |\mathbf{u}(t)-\mathbf{v}(t)|_m\;\diff{t}\right|\;.
\end{equation}
Maggiorando l'integrale con il sup, si ottiene:
\begin{equation}
|\mathbf{u}(x)-\mathbf{v}(x)|_m \le |\mathbf{u}_0-\mathbf{v}_0|_m+hH \sup_{x \in J''} |\mathbf{u}(x)-\mathbf{v}(x)|_m\;,
\end{equation}
da cui si ha:
\begin{equation}
\sup_{x \in J''} |\mathbf{u}(x)-\mathbf{v}(x)|_m \le |\mathbf{u}_0-\mathbf{v}_0|_m + hH \sup_{x \in J''} |\mathbf{u}(x)-\mathbf{v}(x)|_m\;,
\end{equation}
cioè:
\begin{equation}
\sup_{x \in J''} |\mathbf{u}(x)-\mathbf{v}(x)|_m \le \frac{1}{1-hH}|\mathbf{u}_0-\mathbf{v}_0|_m\;.
\end{equation}
Posto $c=1/(1-hH)$, si ottiene la tesi per $J''$ sufficientemente piccolo.
\endproof
\begin{defn} Sia $\gimel(x_0,\mathbf{u_0})$ la famiglia di tutti gli intervalli $J$ di centro $x_0$ tali che il problema di Cauchy abbia soluzione $\mathbf{u}_J$ definita su $J$. L'intervallo:
\begin{equation}
J_0 = \bigcup_{J \in \gimel(x_0,\mathbf{u_0})} J\;,
\end{equation}
si definisce \textit{intervallo massimale} di esistenza della soluzione. La soluzione \textit{massimale} sarà:
\begin{equation}
\begin{cases}
 \mathbf{u}(x)=\mathbf{u}_J(x) \qquad \forall x \in J\;, \\
 \\
 \mathbf{u}_{J'}(x)= \mathbf{u}_J(x) \qquad \forall x \in J\cap J'\;.
\end{cases}
\end{equation}
\end{defn}
\begin{oss} Sia $A=[c,d]\times \mathbb{R}^m$. Supponiamo che $\mathbf{g}$ sia globalmente Lipschitziana in $A$, cioè che $\exists H>0$ tale che per ogni $x\in [c,d]$ e per ogni $\mathbf{u},\mathbf{v}\in\mathbb{R}^m$ si abbia:
\begin{equation}
|\mathbf{g}(x,\mathbf{u})-\mathbf{g}(x,\mathbf{v})|_m \le H |\mathbf{u}-\mathbf{v}|_m\;.
\end{equation}
Sia infine:
\begin{equation}
M_0= \sup_{x \in [c,d]} |\mathbf{g}(x,\mathbf{u}_0)|_m\;.
\end{equation}
Fissata una palla di centro $\mathbf{u}_0$ e raggio $b$, si ha che:
\begin{equation}
|\mathbf{g}(x,\mathbf{u})|_m \le Hb+M_0\;, \qquad \forall(x,\mathbf{u}) \in [c,d]\times B(\mathbf{u}_0,b)\;.
\end{equation}
Sia $h=\min\{(d-c),1/H,1/(H+M_0)\}$. Allora è possibile, prendendo intervalli di ampiezza $h$, estendere la soluzione dell'equazione differenziale a tutta la striscia $A$.
\end{oss}
\section{Equazioni differenziali del primo ordine}
\subsection{Equazioni differenziali del primo ordine a variabili separabili}
\begin{defn} Un'equazione differenziale del primo ordine a variabili separabili si presenta nella forma:
\begin{equation}
y'=f(x)g(y)\;,
\end{equation}
con $f:I \to \mathbb{R}$, $g:J \to \mathbb{R}$ funzioni di classe $C^1$ e quindi $y:I \to J$.
\end{defn}
\subsection*{Risoluzione}
\subsubsection*{Passo I - Ricerca di soluzioni costanti}
Ogni $y_0$ tale che $g(y_0)=0$ è una soluzione costante. Se $y_0 \in J$ allora $J=J'\cup J''\cup \{y_0\}$.
\subsubsection*{Passo II - Ricerca di soluzioni non costanti}
Escludendo $y_0$, si ha che $g(y) \ne 0 \quad \forall y \in J'\cup J''$, dunque posso dividere per $g(y)$, ottenendo:
\begin{equation}
\frac{y'}{g(y)}=f(x)\;.
\end{equation}
Siano $F(x)$ una primitiva di $f(x)$ e $\gamma(y)$ una primitiva di $1/g(y)$. Allora:
\begin{equation}
\gamma'(y)y'(x)=f(x) \qquad \Longleftrightarrow \qquad D(\gamma(y(x)))=f(x)\;.
\end{equation}
Integrando ambo i membri:
\begin{equation}
\gamma(y(x))=F(x)+C\;.
\end{equation}
Poiché $g(y)$ è sempre positiva o negativa, allora $\gamma$ ha derivata di segno costante, dunque è monotona e quindi invertibile. Allora si avrà:
\begin{equation}
y=\gamma^{-1}(F(x)+C)\;.
\end{equation}
\subsection{Equazioni differenziali riconducibili a variabili separabili}
Consideriamo l'equazione:
\begin{equation}
y'=f\left(\frac{y}{x}\right)\;.
\end{equation}
È un'equazione omogenea. Poniamo $u(x)=y(x)/x$, da cui segue che:
\begin{equation}
u'(x)=\frac{y'(x)x-y(x)}{x^2}=\frac{f\left(\frac{y}{x}\right)x-y(x)}{x^2}=\frac{x[f(u)-u]}{x^2}\;,
\end{equation}
cioè:
\begin{equation}
u'(x)=\frac{f(u)-u}{x}\;,
\end{equation}
ossia un'equazione differenziale a variabili separabili nell'incognita $u(x)$. Nota $u$, si ha che $y(x)=u(x)\cdot x$.
\subsection{Equazioni del tipo $y'=f(ax+by)$}
Consideriamo l'equazione:
\begin{equation}
y'=f(ax+by)\;.
\end{equation}
Poniamo $u(x)=ax+by$, da cui $u'(x)=a+by'=a+bf(u)$. Quindi l'equazione diventa:
\begin{equation}
u'(x)=a+bf(u)\;,
\end{equation}
ossia un'equazione differenziale a variabili separabili nell'incognita $u(x)$. Nota $u$, si ha che:
\begin{equation}
y(x)=\frac{u(x)-ax}{b}\;.
\end{equation}
\subsection{Equazioni lineari del primo ordine}
\begin{defn} Un'equazione differenziale lineare del primo ordine si presenta nella forma:
\begin{equation}
y'=\alpha(x)y+\beta(x)\;,
\end{equation}
con $\alpha,\beta$ continue su un intervallo $J$ e $\beta(x) \ne 0 \quad \forall x \in J$.
\end{defn}
\subsubsection*{Risoluzione}
Sia $A(x)$ una qualunque primitiva di $\alpha(x)$. Moltiplichiamo l'equazione per $e^{-A(x)}$, ottenendo:
\begin{align}
e^{-A(x)}y' &= e^{-A(x)}\alpha(x)y+e^{-A(x)}\beta(x)\;, \notag \\
e^{-A(x)}y'-e^{-A(x)}\alpha(x)y&=e^{-A(x)}\beta(x)\;. \label{ch1_firstorder}
\end{align}
Ma $e^{-A(x)}y'-e^{-A(x)}\alpha(x)y=D[e^{-A(x)}y]$. Allora, integrando membro a membro la \eqref{ch1_firstorder}:
\begin{equation}
e^{-A(x)}y=\int_a^x e^{-A(t)}\beta(t)\;\diff{t} + C\;, \qquad a \in J \setminus \{0\}\;,
\end{equation}
da cui:
\begin{equation}
y(x)=c\cdot e^{A(x)}+\int_a^x e^{A(x)-A(t)} \beta(t)\;\diff{t}\;.
\end{equation}
Posto:
\begin{equation}
A(x)=\int_a^x \alpha(s)\;\diff{s}\;,
\end{equation}
si ha:
\begin{equation}
y(x)=c \cdot \exp\left(\int_a^x \alpha(s)\;\diff{s}\right)+\int_a^x \exp\left(\int_t^x \alpha(s)\;\diff{s}\right)\beta(t)\;\diff{t}\;.
\end{equation}
\begin{oss} Lo spazio vettoriale delle soluzioni di un'equazione differenziale non omogenea è affine allo spazio delle soluzioni dell'equazione omogenea.
\end{oss}
\subsection{Equazione di Bernoulli}
L'equazione di Bernoulli è:
\begin{equation}
y'=\alpha(x)y+\beta(x)y^{\gamma}\;,
\end{equation}
con $\alpha(x),\beta(x)$ continue e $\gamma \in \mathbb{R}$. Cerchiamo per semplicità soluzioni $y(x)>0$. Notiamo che se $\gamma=\{0,1\}$, l'equazione è lineare non omogenea. Per $\gamma \ne \{0,1\}$, poniamo $u(x)=y^{1-\gamma}(x)$. Dunque:
\begin{align}
u'(x) &= (1-\gamma)y^{-\gamma}(x)y'(x)=(1-\gamma)y^{-\gamma}(\alpha(x)y+\beta(x)y^{\gamma}) \notag \\
&=(1-\gamma)\alpha(x)y^{1-\gamma}+(1-\gamma)\beta(x)=(1-\gamma)u(x)+(1-\gamma)\beta(x)\;,
\end{align}
che è un'equazione differenziale lineare del primo ordine nella variabile $u$. Trovata $u$, si ha che $y=u^{1/(1-\gamma)}$.
\chapter{Sistemi lineari del primo ordine}
\section{Introduzione}
Un sistema lineare differenziale del primo ordine si presenta nella forma:
\begin{equation}
\mathbf{u}'(t)=A(t)\mathbf{u}(t)+\mathbf{f}(t)\;,
\end{equation}
con $t \in I$ intervallo, $\mathbf{u} \in C^1(I,\mathbb{C}^n)$, $A(t) \in M(n)$ e $\mathbf{f}:I \to \mathbb{C}^n$ continua. \\
\begin{oss}[Principio di sovrapposizione] Se $\mathbf{u},\mathbf{v}$ risolvono i sistemi lineari:
\begin{align}
&\begin{cases}
\mathbf{u}'=A\mathbf{u}+\mathbf{f}(t)\;, \\
\\
\mathbf{u}(t_0)=\mathbf{u}_0\;,
\end{cases}
&\begin{cases}
\mathbf{v}'=A\mathbf{v}+\mathbf{g}\;, \\
\\
\mathbf{v}(t_0)=\mathbf{v}_0\;.
\end{cases}
\end{align}
Allora $\forall \lambda,\mu \in \mathbb{C}$, la funzione $\lambda \mathbf{u}+\mu \mathbf{v}$ risolve il sistema lineare:
\begin{equation}
\begin{cases}
 \lambda \mathbf{u}'+\mu \mathbf{v}'=A(\lambda \mathbf{u}+\mu \mathbf{v})+\lambda \mathbf{f}+\mu \mathbf{g}\;, \\
 \\
 (\lambda \mathbf{u}+\mu \mathbf{v})(t_0)= \lambda \mathbf{u}_0+\mu \mathbf{v}_0\;.
\end{cases}
\end{equation}
\end{oss}
\begin{prop} Sia:
\begin{equation}
V_0=\{\mathbf{u} \in C^1(I,\mathbb{C}^n)\;|\; \mathbf{u}'=A\mathbf{u}\}\;,
\end{equation}
lo spazio vettoriale delle soluzioni del sistema lineare omogeneo. Allora:
\begin{equation}
V_f=\{\mathbf{u} \in C^1(I,\mathbb{C}^n)\;|\; \mathbf{u}'=A\mathbf{u}+\mathbf{f}\}\;,
\end{equation}
è uno spazio affine, cioè se $\mathbf{z} \in V_f$ allora $V_0+\mathbf{z}=V_f$.
\end{prop}
\proof $(\subseteq)$ Sia $\mathbf{u} \in V_0$. Allora, per il principio di sovrapposizione, segue immediatamente che $\mathbf{u}+\mathbf{z} \in V_f$, e quindi $V_0+\mathbf{z} \subseteq V_f$.
\endproof
\proof $(\supseteq)$ Sia $\mathbf{w} \in V_f$. Cerchiamo $\mathbf{u} \in V_0$ tale che $\mathbf{u}+\mathbf{z}=\mathbf{w}$. Consideriamo $\mathbf{w}-\mathbf{z}$. Per il principio di sovrapposizione, $\mathbf{w}-\mathbf{z} \in V_0$. Posto $\mathbf{u}=\mathbf{w}-\mathbf{z}$, segue che ogni $\mathbf{w}\in V_f$ è scrivibile come somma di una certa $\mathbf{u} \in V_0$ e $\mathbf{z}$ e dunque $V_f \subseteq V_0+\mathbf{z}$.
\endproof
\begin{prop} $\dim{V_0}=n$.
\end{prop}
\proof Sia $\mathbf{S}:C^n \to V_0$ un'applicazione lineare definita da $\mathbf{S}(\mathbf{x})=\mathbf{u}_x$ tale che:
\begin{equation}
\begin{cases}
 \mathbf{u}_x'(t)=A\mathbf{u}_x(t)\;, \\
 \\
 \mathbf{u}(t_0)=\mathbf{x}\;.
\end{cases}
\end{equation}
Per il teorema di esistenza ed unicità, $\mathbf{u}_x$ è unica. Notiamo che:
\begin{itemize}
 \item se $\mathbf{x}\equiv \mathbf{0}$ allora $\mathbf{S}(\mathbf{x})=\mathbf{0}$. Quindi, per l'unicità, ogni $\mathbf{x} \in \mathbb{C
}^n$ che risolve $\mathbf{S}(\mathbf{x})=\mathbf{0}$ è identicamente nullo, dunque $\mathbf{S}$ è iniettiva;
 \item sia $\mathbf{x}=\mathbf{v}(t_0)$. Allora $\mathbf{u}_x$ e $\mathbf{v}$ risolvono lo stesso sistema lineare. Dunque, nuovamente per l'unicità, $\mathbf{u}_x \equiv \mathbf{v}$, da cui segue che $\mathbf{S}$ è surgettiva.
\end{itemize}
Poiché $\mathbf{S}$ è contemporaneamente iniettiva e surgettiva, $\mathbf{S}$ è un isomorfismo, pertanto:
\begin{equation}
\dim{\mathbb{C}^n}=n=\dim{V_0}\;.
\end{equation}
\endproof
\section{Matrice Wronskiana}
\begin{defn} Siano $\mathbf{u}_1,\ldots,\mathbf{u}_n \in V_0$. Si definisce \textit{matrice Wronskiana} la matrice $W(t)$ data da
\begin{equation}
W(t)=\begin{pmatrix}
      {u}_1^1(t) & \cdots & {u}_n^1(t) \\
      \vdots & \ddots & \vdots \\
      {u}_1^{n}(t) & \cdots & {u}_n^{n}(t)
     \end{pmatrix}\;.
\end{equation}
\end{defn}
\begin{prop} Siano $\mathbf{u}_1,\ldots,\mathbf{u}_n \in V_0$. Allora la loro matrice Wronskiana verifica:
\begin{equation}
W'(t)=A(t)W(t)\;, \qquad t \in I\;.
\end{equation}
Inoltre, sono fatti equivalenti:
\begin{enumerate}
 \item $\mathbf{u}_1,\ldots,\mathbf{u}_n$ sono linearmente indipendenti in $V_0$;
 \item $\exists t_0 \in I$ tale che $\det{W(t_0)} \ne 0$;
 \item $\forall t \in I$ si ha $\det{W(t)} \ne 0$.
\end{enumerate}
\end{prop}
\proof Indicando con $w_{ij}(t)$ e $a_{ij}(t)$ i coefficienti rispettivamente di $W(t)$ e $A(t)$ si ha:
\begin{equation}
\dev{w_{ij}}{t}(t)=\frac{d}{dt}u_j^i(t)=\sum_{k=1}^na_{ik}(t)u_j^k(t)=\sum_{k=1}^na_{ik}(t)w_{kj}(t)\;.
\end{equation}
\endproof
\proof $(1) \Longrightarrow (3)$ \\

Siano $\mathbf{u}_1,\ldots,\mathbf{u}_n \in V_0$ linearmente indipendenti. Supponiamo per assurdo che $\exists t_0 \in I$ tale che
$\det W(t_0)=0$, allora le colonne di $W(t_0)$ sarebbero linearmente dipendenti, quindi $\exists c_1,\ldots,c_n \in \mathbb{C}^n$ non 
tutti nulli tali che:
\begin{equation}
\sum_{k=1}^n c_k \mathbf{u}_k(t_0)=0\;.
\end{equation}
Sia:
\begin{equation}
\mathbf{v}(t)=\sum_{k=1}^n c_k \mathbf{u}_k(t)\;,
\end{equation}
si ha che $\mathbf{v} \in V_0$ e quindi risolve il sistema lineare:
\begin{equation}
\begin{cases}
 \mathbf{v}'(t)=A(t)\mathbf{v}(t) \\
 \\
 \mathbf{v}(t_0)=\mathbf{0}
\end{cases}\;.
\end{equation}
Ma $\mathbf{v} \equiv \mathbf{0}$ è anch'essa soluzione del sistema. Allora, per l'unicità segue che:
\begin{equation}
\mathbf{v}(t)=\sum_{k=1}^n c_k \mathbf{u}_k(t) \equiv \mathbf{0}\;.
\end{equation}
Poiché le $\mathbf{u}_k$ sono linearmente indipendenti, si avrà $c_k=0$ per ogni $ k$, il che contraddice l'ipotesi che le colonne di 
$W(t_0)$ siano linearmente dipendenti per un certo $t_0$, e dunque è un assurdo.
\endproof
\proof $(2) \Longrightarrow (1)$ \\

Sia $\det W(t_0) \ne 0$ per certe $\mathbf{u}_1,\ldots,\mathbf{u}_n \in V_0$. Se per assurdo, $\mathbf{u}_1,\ldots,\mathbf{u}_n$ fossero 
linearmente dipendenti in $V_0$, allora $\exists c_1,\ldots,c_n \in \mathbb{C}$ non tutti nulli tali che:
\begin{equation}
\sum_{k=1}^n c_k \mathbf{u}_k =0\;.
\end{equation}
Allora $\forall t \in I$ i vettori $\mathbf{u}_1(t),\ldots,\mathbf{u}_n(t)$ sarebbero linearmente dipendenti anche in $\mathbb{C}^n$. In 
particolare, $\det W(t)=0 \quad \forall t \in I$, il che contraddice l'ipotesi e dunque costituisce un assurdo.
\endproof
\begin{defn} Una base $\{\mathbf{u}_1,\ldots,\mathbf{u}_n\}$ di $V_0$ si dice \textit{sistema fondamentale} di soluzioni. Si ha inoltre:
\begin{equation}
V_0=\{c_1\mathbf{u}_1+\ldots+c_n\mathbf{u}_n, c_1,\ldots,c_n \in \mathbb{C}\}=\{W(\cdots) \mathbf{z}, \mathbf{z} \in \mathbb{C}
^n\}\;.
\end{equation}
\end{defn}
\begin{oss} È un sistema fondamentale di soluzioni la famiglia $\mathbf{u}_1,\ldots,\mathbf{u}_n$ tali che:
\begin{equation}
\mathbf{u}_j \quad \mbox{è soluzione di} \quad
\begin{cases}
 \mathbf{u}_j'(t)=A(t)\mathbf{u}_j(t) \\
 \\
 \mathbf{u}_j(t_0)=e_j
\end{cases}\;.
\end{equation}
\end{oss}
\section{Metodo di variazione delle costanti}
Sia $\{\mathbf{u}_1,\ldots,\mathbf{u}_n\}$ un sistema fondamentale di soluzioni e $W(t)\mathbf{z} \in V_0$. Facciamo variare $\mathbf{z} 
\equiv \mathbf{z}(t)$. Sia $\mathbf{v}(t)=W(t)\mathbf{z}(t)$. Impongo che $\mathbf{v} \in V_f$, cioè:
$$
\mathbf{v}'(t)=A(t)\mathbf{v}(t)+\mathbf{f}(t)
$$
Si ha, per costruzione,
\begin{align}
\mathbf{v}'(t) &=\left(W(t)\mathbf{z}(t)\right)'=W'(t)\mathbf{z}(t)+W(t)\mathbf{z}'(t)=A(t)W(t)\mathbf{z}(t)+W(t)\mathbf{z}'(t) \notag \\
&=A(t)\mathbf{v}(t)+W(t)\mathbf{z}'(t)\;.
\end{align}
Da cui quindi segue:
\begin{equation}
A(t)\mathbf{v}(t)+W(t)\mathbf{z}'(t)=A(t)\mathbf{v}(t)+\mathbf{f}(t)\;.
\end{equation}
Dall'uguaglianza, otteniamo:
\begin{equation}
\mathbf{f}(t)=W(t)\mathbf{z}'(t)\;.
\end{equation}
Il Wronskiano è invertibile, quindi:
\begin{equation}
\mathbf{z}'(t)=W^{-1}(t)\mathbf{f}(t)\;,
\end{equation}
e di conseguenza:
\begin{equation}
\mathbf{z}(t)=\int_{t_0}^t W^{-1}(s)\mathbf{f}(s)\;\diff{s} \in V_f, \qquad W(t_0)=I\;.
\end{equation}
In definitiva, si ha:
\begin{align}
&\mathbf{v}(t)=W(t)\int_{t_0}^t W^{-1}(s)\mathbf{f}(s)\;\diff{s}\; \in\; V_f\;, \\
&V_f=V_0+\mathbf{v}=\left\{\mathbf{c}\cdot W(t)+W(t)\int_{t_0}^t W^{-1}(s)\mathbf{f}(s)\,\diff{s},\; \mathbf{c}\in\mathbb{C}^n\right\}\;.
\end{align}
\section{Sistemi lineari a coefficienti costanti}
Un sistema lineare a coefficienti costanti è della forma:
\begin{equation}
\mathbf{u}'(t)=A \mathbf{u}(t), \qquad A \in M(n,\mathbb{C})\;.
\end{equation}
$A$ ha autovalori $\lambda_1,\ldots,\lambda_r$ di molteplicità rispettivamente $k_1,\ldots,k_r$ tali che:
\begin{equation}
\sum_{i=1}^r k_i = n\;.
\end{equation}
\subsection{Caso I}
Siano $\lambda_1,\ldots,\lambda_n \in C$ gli autovalori di $A$. Cerchiamo una soluzione del tipo $\mathbf{u}=\mathbf{v}e^{\lambda t}$. Si ha 
quindi:
\begin{equation}
\mathbf{u}'=\lambda \mathbf{v} e^{\lambda t}= A \mathbf{v} e^{\lambda t} \Longleftrightarrow A\mathbf{v}=\lambda \mathbf{v}\;,
\end{equation}
quindi $\lambda$ è autovalore relativo all'autovettore $\mathbf{v}$. Quindi le soluzioni saranno del tipo $\{\mathbf{v}_1e^{\lambda t},
\ldots,\mathbf{v}_ne^{\lambda t}\}$. Verifichiamo la lineare indipendenza costruendo il Wronskiano:
\begin{equation}
Y(t)=(e^{\lambda_1 t}\mathbf{v}_1|\ldots|e^{\lambda_n t}\mathbf{v}_n)\;.
\end{equation}
Si ha dunque:
\begin{equation}
\det Y(t)=e^{\lambda_1+\ldots+\lambda_n}\cdot \det (\mathbf{v}_1|\ldots|\mathbf{v}_n) \ne 0\;,
\end{equation}
in quanto $\mathbf{v}_1,\ldots,\mathbf{v}_n$ sono autovettori appartenenti ad autospazi distinti. Allora possiamo caratterizzare $V_0$:
\begin{equation}
V_0=\left\{ \mathbf{z}(t)=c_1\mathbf{v}_1e^{\lambda_1 t}+\cdots+c_n\mathbf{v}_ne^{\lambda_n t},\;c_1,\ldots,c_n \in \mathbb{C}\right\}\;.
\end{equation}
\subsection{Caso II}
Sia $\lambda_0$ un autovalore di molteplicità algebrica $\mu_a (\lambda_0)=r>1$ e molteplicità geometrica $\mu_g (\lambda_0)=r$. In 
corrispondenza di $\lambda_0$ trovo $r$ soluzioni $\{e^{\lambda_0 t}\mathbf{v}_1,\ldots,e^{\lambda_0 t}\mathbf{v}_r\}$ con 
$\{\mathbf{v}_1,\ldots,\mathbf{v}_r\}$ base di $\ker(A-\lambda_0 I)$. Aggiungendo le soluzioni proveniente dagli altri autospazi, trovo 
una base di $V_0$.
\subsection{Caso III}
Sia $\lambda_0$ un autovalore di molteplicità algebrica $\mu_a (\lambda_0)=r>1$ e molteplicità geometrica $\mu_g (\lambda_0)=s<r$. In 
corrispondenza di $\lambda_0$ troviamo solamente $s$ soluzioni del tipo $\{e^{\lambda_0 t}\mathbf{v}_1,\ldots,e^{\lambda_0 t}\mathbf{v}_s\}$, con $\{\mathbf{v}_1,\ldots,\mathbf{v}_s\}$ base di $\ker(A-\lambda_0 I)$. Le restanti $r-s$ soluzioni le scegliamo nella forma:
\begin{equation}
\mathbf{p}_1(t)e^{\lambda_0 t},\ldots,\mathbf{p}_{r-s}(t)e^{\lambda_0 t}\;,
\end{equation}
con $\deg \mathbf{p}_j \le j$. In questo modo, riusciamo nuovamente a trovare un sistema fondamentale di soluzioni.
\subsection{Sistemi di Eulero}
Un sistema di Eulero è della forma:
\begin{equation}
\mathbf{u}'(t)=\frac{A\mathbf{u}(t)}{t}, \qquad t>0\;.
\end{equation}
Posto $\mathbf{v}(s)=\mathbf{u}(e^s)$ si ha:
\begin{equation}
\mathbf{v}'(s)=\mathbf{u}'(e^s)\cdot e^s=\frac{A\mathbf{u}(e^s)\cdot e^s}{e^s}=A\mathbf{u}(e^s)=A\mathbf{v}(s)\;,
\end{equation}
con $\mathbf{u}(t)=\mathbf{v}(\log t)$.
\chapter{Equazioni differenziali di ordine n}
Un'equazione differenziale di ordine $n$ si presenta nella forma:
\begin{equation}
u^{(n)}(t)=\sum_{k=0}^{n-1} a_k(t)u^{(k)}(t) \quad+ f(t)\;,
\end{equation}
con $t \in J$, $a_k,f \in C^1(J,\mathbb{C})$ e $u \in C^n(J,\mathbb{C}^n)$.
\section{Problema di Cauchy}
Consideriamo il problema di Cauchy:
\begin{equation}
\begin{cases}
 u^{(n)}=\displaystyle{\sum_{k=0}^{n-1} a_k(t)u^{(k)}} \quad + f(t) \\
 u(t_0)=u_1 \\
 u'(t_0)=u_2 \\
\ldots \ldots \ldots \ldots \\
 u^{(n-1)}(t_0)=u_n 
\end{cases}\;,
\end{equation}
con $u_1,\ldots,u_n \in V_0$. Costruiamo il Wronskiano di $\{u_1,\ldots,u_n\}$:
\begin{equation}
W(t)= \begin{pmatrix}
       u_1(t) & \cdots & u_n(t) \\
       u'_1(t) & \cdots & u'_n(t) \\
       \vdots & \vdots & \vdots \\
       u_n^{(n-1)}(t) & \cdots & u_n^{(n-1)}(t)
      \end{pmatrix}
\end{equation}
Si ha ovviamente che $\det W(t) \equiv 0 \lor \det W(t) \ne 0\; \forall t \in J$. Cerchiamo una $u_f \in V_f$. Sia:
\begin{equation}
\mathbf{v}_f = (u_f,u'_f,\ldots,u_f^{(n-1)})\;.
\end{equation}
Allora si ha:
\begin{equation}
u_f \in V_f \qquad \Longleftrightarrow\qquad \mathbf{v}'_f = A\mathbf{v}_f + \mathbf{F}\;,
\end{equation}
dove $\mathbf{F}=(0,\ldots,0,f(t))$. Da ciò segue che:
\begin{align}
\mathbf{v}_f &= W(t)\mathbf{z}(t)\;, \\
\mathbf{z}'(t) &= W^{-1}(t)\begin{pmatrix}
0 \\
\vdots \\
f(t)
\end{pmatrix} \\
W^{-1}(t)=\frac{1}{\det W(t)}\left(\{(-1)^{i+j}\det W_{ij}\right\}\;,
\end{align}
da cui si ottiene:
\begin{equation}
\mathbf{z}'(t)=\frac{f(t)}{\det W(t)} \cdot \begin{pmatrix}
                                             (-1)^{n+1} \det W_{n1}(t) \\
\vdots \\
(-1)^{n+n} \det W_{nn}(t)
                                            \end{pmatrix}\;,
\end{equation}
e quindi:
\begin{equation}
\mathbf{z}(t)=\int_a^t{\frac{f(s)}{\det W(s)} \cdot \begin{pmatrix}
                                             (-1)^{n+1} \det W_{n1}(s) \\
\vdots \\
(-1)^{n+n} \det W_{nn}(s)
                                            \end{pmatrix}}\; \diff{s}\;.
\end{equation}
Si conclude pertanto che:
\begin{equation}
u_f(t)=\langle \mathbf{v}_f(t),e_1\rangle_{\mathbb{C}^n}=\left(W(t) \mathbf{z}(t)\right)_1\;.
\end{equation}
\section{Equazioni differenziali di ordine $n$ a coefficienti costanti}
Nel caso di un'equazione differenziale di ordine $n$ a coefficienti costanti:
\begin{equation}
u^{(n)}= \sum_{k=0}^{n-1} a_k u^{(k)} \qquad (+f(t))\;,
\end{equation}
cerchiamo soluzioni del tipo $u_{\lambda}(t)=t^h e^{\lambda t}$. Definiamo il \textit{polinomio caratteristico associato} $p(\lambda)$ come:
\begin{equation}
p(\lambda)=\lambda^n-\sum_{k=0}^{n-1}a_k \lambda^k\;.
\end{equation}
Osserviamo che:
\begin{equation}
p\left(\frac{d}{dt}\right)(t^h e^{\lambda t}) = p\left(\frac{d}{dt}\right)\left(\frac{d^h}{d\lambda^h}\right) e^{\lambda t} =
\frac{d^h}{d\lambda^h}p\left(\frac{d}{dt}\right) e^{\lambda t} = \frac{d^h}{d\lambda^h}p(\lambda)e^{\lambda t}=0\;.
\end{equation}
Allora $u_{\lambda}(t)$ è soluzione se e solo se $\frac{d^h}{d\lambda^h}p(\lambda)e^{\lambda t}=0$ cioè $p^{(h)}(\lambda)=0$. $p$ è un 
polinomio con $r$ radici $\lambda_1,\ldots,\lambda_r$ di molteplicità $k_1,\ldots,k_r$ tali che $k_1+\cdots+k_r=n$. Si osserva che:
\begin{align}
&u=e^{\lambda t}\quad \mbox{è soluzione}\quad \Longrightarrow\quad p(\lambda)=0\;, \notag \\
&u=te^{\lambda t}\quad \mbox{è soluzione} \quad \Longrightarrow\quad p(\lambda),p'(\lambda)=0\;.
\end{align}
In generale, dunque, le soluzioni saranno:
\begin{equation}
\begin{cases}
 e^{\lambda_1 t}, te^{\lambda_1 t},\ldots, t^{k_1-1}e^{\lambda_1 t} \\
 \vdots \\
 e^{\lambda_r t}, te^{\lambda_r t},\ldots, t^{k_f-1}e^{\lambda_r t}
\end{cases}\;.
\end{equation}
Se $f(t)=P(t)e^{\beta t}$ con $P(t)$ polinomio e $\beta \in \mathbb{C}$, allora la soluzione particolare è del tipo:
\begin{equation}
u_f(t)=t^m Q(t) e^{\beta t}\;,
\end{equation}
dove $m=\mu_a (\beta)$ come radice di $P(t)$ e $\deg Q(t) \le \deg P(t)$.
\chapter{Derivate parziali e differenziabilità}
\section{Derivate parziali}
Siano $A \subseteq \mathbb{R}^N$ un aperto, $f:A\to \mathbb{R}$ e $\mathbf{x}_0 \in A$. Sia inoltre $\{e_1,\ldots,e_N\}$ la base 
canonica di $\mathbb{R}^{N}$.
\begin{defn} Si dice che $f$ ha in $\mathbf{x}_0$ la \textit{derivata parziale $i$-esima} se:
\begin{equation}
\exists \lim_{t \to 0} \frac{f(\mathbf{x}_0+t\mathbf{e}_i)-f(\mathbf{x}_0)}{t} \in \mathbb{R}\;,
\end{equation}
e si denota con:
\begin{equation}
\frac{\partial f}{\partial x^i}(\mathbf{x}_0), \quad f_{x^i}(\mathbf{x}_0), \quad D_i f(\mathbf{x}_0)\;.
\end{equation}
\end{defn}
\begin{oss} Se una funzione $f$ ha tutte le derivate parziali in un punto $\mathbf{x}_0$, non è detto che sia continua in $\mathbf{x_0}$.
\end{oss}
\section{Differenziabilità}
Siano $A \subseteq \mathbb{R}^N$ un aperto, $f:A \to \mathbb{R}$ e $\mathbf{x}_0 \in A$.
\begin{defn} Si dice che $f$ è \textit{differenziabile} in $\mathbf{x}_0$ se $\exists \mathbf{a} \in \mathbb{R}^N$ tale che:
\begin{equation}
\lim_{|\mathbf{h}|_N\to 0} \frac{f(\mathbf{x}_0+\mathbf{h})-f(\mathbf{x}_0)-\langle \mathbf{a},\mathbf{h}\rangle_N}{|\mathbf{h}|_N}=0\;.
\end{equation}
\end{defn}
\begin{prop} Se $f$ è differenziabile in un punto $\mathbf{x}_0$, allora:
\begin{enumerate}
 \item $f$ è continua in $\mathbf{x}_0$;
 \item $\exists D_i f(\mathbf{x}_0)\; \forall i$ e $D_i f(\mathbf{x}_0)=a^i$, cioè $\mathbf{a}=\nabla f(\mathbf{x}_0)$.
\end{enumerate}
\end{prop}
\proof $1$ \\

Bisogna dimostrare che:
\begin{equation}
\lim_{\mathbf{h}\to \mathbf{0}} [f(\mathbf{x}_0+\mathbf{h})-f(\mathbf{x}_0)]=0\;.
\end{equation}
Si ha:
\begin{equation}
f(\mathbf{x}_0+\mathbf{h})-f(\mathbf{x}_0) = [f(\mathbf{x}_0+\mathbf{h})-f(\mathbf{x}_0) - \langle \mathbf{a},\mathbf{h}\rangle_N]+
\langle \mathbf{a},\mathbf{h}\rangle_N\;.
\end{equation}
Ma:
\begin{equation}
f(\mathbf{x}_0+\mathbf{h})-f(\mathbf{x}_0) - \langle \mathbf{a},\mathbf{h}\rangle_N \to 0\quad\mbox{per}\quad |\mathbf{h}|_N \to 0\;,
\end{equation}
poiché $f$ è per ipotesi differenziabile in $\mathbf{x}_0$. Inoltre:
\begin{equation}
\langle \mathbf{a},\mathbf{h}\rangle_N \le |\mathbf{a}|_N \cdot |\mathbf{h}|_N \to 0\quad \mbox{per}\quad |\mathbf{h}|_N \to 0\;.
\end{equation}
Da queste due relazioni, si ottiene la tesi.
\endproof
\proof $2$ \\
Fissato $\mathbf{h}=t\mathbf{e}_i$, per ipotesi di differenziabilità si ha:
\begin{equation}
\lim_{t \to 0} \frac{f(\mathbf{x}_0+t\mathbf{e}_i-f(\mathbf{x}_0)-\langle \mathbf{a},t\mathbf{e}_i\rangle_N}{|t|} =0\;.
\end{equation}
Moltiplicando per $|t|/t=\pm 1$, il limite rimane inalterato, dunque si ottiene:
\begin{equation}
\lim_{t \to 0} \frac{f(\mathbf{x}_0+t\mathbf{e}_i-f(\mathbf{x}_0)-\langle \mathbf{a},t\mathbf{e}_i\rangle_N}{t} =0\;,
\end{equation}
da cui, spezzando la frazione, si ha:
\begin{equation}
\lim_{t \to 0} \frac{f(\mathbf{x_0}+t\mathbf{e}_i)-f(\mathbf{x}_0)}{t}=\langle \mathbf{a},\mathbf{e}_i\rangle_N\;.
\end{equation}
Dunque si conclude, per la definizione di derivata parziale, che:
\begin{equation}
\exists D_i f(\mathbf{x}_0)=\langle \mathbf{a},\mathbf{e}_i \rangle_N = a^i\;,
\end{equation}
e di conseguenza:
\begin{equation}
\mathbf{a} = \nabla f(\mathbf{x}_0)\;.
\end{equation}
\endproof
\begin{defn} Si chiama \textit{piano N-dimensionale tangente} al grafico di $f$ in $(\mathbf{x}_0,f(\mathbf{x}_0))$ il piano di equazione:
\begin{equation}
X^{N+1}=f(\mathbf{x}_0)+\langle \nabla f(\mathbf{x}_0),\mathbf{x}-\mathbf{x}_0 \rangle_N\;.
\end{equation}
\end{defn}
\begin{defn}[Derivata direzionale] Sia $\mathbf{v} \in \mathbb{R}^N$ tale che $|\mathbf{v}|_N=1$. Si definisce \textit{derivata direzionale} di $f$ in $\mathbf{x}_0$ nella direzione $\mathbf{v}$ il limite (qualora esista finito):
\begin{equation}
\lim_{t \to 0} \frac{f(\mathbf{x}_0+t\mathbf{v})-f(\mathbf{x}_0)}{t}\;,
\end{equation}
e si denota:
\begin{equation}
\frac{\partial f}{\partial \mathbf{v}}(\mathbf{x}_0),\quad f_{\mathbf{v}}(\mathbf{x}_0),\quad D_{\mathbf{v}} f(\mathbf{x}_0)\;.
\end{equation}
\end{defn}
\begin{prop} Se $f$ è una funzione differenziabile in un punto $\mathbf{x}_0$, allora $\exists D_{\mathbf{v}}f(\mathbf{x}_0)$ per ogni direzione $\mathbf{v}$ e inoltre si ha:
\begin{equation}
D_{\mathbf{v}}f(\mathbf{x}_0)=\langle \nabla f(\mathbf{x}_0),\mathbf{v}\rangle_N\;.
\end{equation}
\end{prop}
\proof Fissato $\mathbf{h}=t\mathbf{v}$, si ha, per l'ipotesi di differenziabilità:
\begin{equation}
\lim_{t \to 0} \frac{f(\mathbf{x}_0+t\mathbf{v})-f(\mathbf{x}_0)-\langle \nabla f(\mathbf{x}_0),t\mathbf{v}\rangle_N}{t}=0\;,
\end{equation}
da cui, spezzando la frazione, si ottiene:
\begin{equation}
\lim_{t \to 0} \frac{f(\mathbf{x}_0+t\mathbf{v})-f(\mathbf{x}_0)}{t}=\frac{t\langle \nabla f(\mathbf{x}_0),\mathbf{v}\rangle_N}{t}=
\langle \nabla f(\mathbf{x}_0),\mathbf{v}\rangle_N\;,
\end{equation}
da cui si conclude che $\exists D_{\mathbf{v}} f(\mathbf{x}_0) = \langle \nabla f(\mathbf{x}_0),\mathbf{v}\rangle_N$.
\endproof
\begin{defn} Si definisce \textit{differenziale} della funzione $f$ nel punto $\mathbf{x}_0$ l'applicazione $\varphi:\mathbb{R}^N \to \mathbb{R}$ data da:
\begin{equation}
\varphi(\mathbf{v})=\langle \nabla f(\mathbf{x}_0),\mathbf{v}\rangle_N \qquad \forall \mathbf{v} \in \mathbb{R}^N\;,
\end{equation}
e si denota con $\varphi \equiv \diff{f}(\mathbf{x}_0)$.
\end{defn}
\begin{thm}[del differenziale totale] Siano $A \subseteq \mathbb{R}^N$ un aperto, $f:A \to \mathbb{R}$ e $\mathbf{x}_0 \in A$. Supponiamo che:
\begin{enumerate}
 \item $\exists D_i f(\mathbf{x}) \quad \forall i$ e $\forall \mathbf{x} \in B(\mathbf{x}_0,r)\subseteq A$;
 \item le derivate parziali siano continue in $\mathbf{x}_0$.
\end{enumerate}
Allore $f$ è differenziabile in $\mathbf{x}_0$.
\end{thm}
\proof $(N=2)$ \\
Siano $\mathbf{x}_0=(x_0,y_0)$ e $\mathbf{h}=(h,k)$. Bisogna dimostrare che:
\begin{equation}
\lim_{(h,k) \to (0,0)} \frac{f(x_0+h,y_0+k)-f(x_0,y_0)-f_x(x_0,y_0)h-f_y(x_0,y_0)k}{\sqrt{h^2+k^2}}=0\;. \qquad \label{ch4_difftot}
\end{equation}
Consideriamo:
\begin{equation}
f(x_0+h,y_0+k)-f(x_0,y_0)=[f(x_0+h,y_0+k)-f(x_0,y_0+k)]+[f(x_0,y_0+k)-f(x_0,y_0)]\;.
\end{equation}
L'applicazione:
\begin{equation}
\mathbf{x} \longmapsto f(x,y_0+k)\;,
\end{equation}
è continua e derivabile rispetto a $x$ in $B(\mathbf{x}_0,r) \subset A$. Allora, per il teorema di Lagrange, $\exists \xi \in\; ]x_0,x_0+h[$ 
tale che:
\begin{equation}
f(x_0+h,y_0+k)-f(x_0,y_0+k)=f_x(\xi,y_0+k)\cdot h\;. \label{ch4_difftot2}
\end{equation}
L'applicazione:
\begin{equation}
\mathbf{y} \longmapsto f(x_0,y)\;,
\end{equation}
è continua e derivabile rispetto a $y$ in $B(\mathbf{x}_0,r) \subset A$. Allora, per il teorema di Lagrange, $\exists \eta \in\; ]y_0,y_0+k[
$ tale che:
\begin{equation}
f(x_0,y_0+k)-f(x_0,y_0)=f_y(x_0,\eta)\cdot k\;.
\end{equation}
Dunque il numeratore della \eqref{ch4_difftot} diventa:
\begin{equation}
[f_x(\xi,y_0+k)-f_x(x_0,y_0)]h-[f_y(x_0,\eta)-f_y(x_0,y_0)]k\;.
\end{equation}
Se $\sqrt{h^2+k^2}$ è sufficientemente piccolo, per la continuità di $f_x$ e $f_y$ in $\mathbf{x}_0$ si ha che $\forall \epsilon>0\; 
\exists B(\mathbf{x}_0,r)$ tale che $\forall \mathbf{x} \in B(\mathbf{x}_0,r)$ si abbia:
\begin{equation}
[f_x(\xi,y_0+k)-f_x(x_0,y_0)]h < \epsilon |h| \qquad [f_y(x_0,\eta)-f_y(x_0,y_0)]k < \epsilon |k|\;,
\end{equation}
ed inoltre:
\begin{equation}
\epsilon(|h|+|k|) \le 2\epsilon\sqrt{h^2+k^2}\;.
\end{equation}
Quindi:
\begin{align}
\frac{f(x_0+h,y_0+k)-f(x_0,y_0)-f_x(x_0,y_0)h-f_y(x_0,y_0)k}{\sqrt{h^2+k^2}} &< \frac{\epsilon(|h|+|k|)}{\sqrt{h^2+k^2}} \notag \\
&\le \frac{2\epsilon\sqrt{h^2+k^2}}{\sqrt{h^2+k^2}} \notag \\
&=2\epsilon\;,
\end{align}
che è esattamente la definizione di limite uguale a zero.
\endproof
\begin{thm}[Differenziabilità di funzioni composte] Sia $A \subseteq \mathbb{R}^N$ un aperto, $\mathbf{u}:[a,b]\to A$, $f:A \to \mathbb{R}$. Se $\mathbf{u}$ è derivabile in $t_0 \in [a,b]$ e $f$ è differenziabile in $\mathbf{x}_0=\mathbf{u}(t_0)$, allora $f \circ \mathbf{u}:[a,b] \to \mathbb{R}$ è derivabile in $t_0$ e si ha:
\begin{equation}
D(f \circ \mathbf{u})(t_0)=\langle \nabla f(\mathbf{u}(t_0)),\mathbf{u}'(t_0)\rangle_N\;.
\end{equation}
\end{thm}
\proof Sia $k \in \mathbb{R}$ tale che $t_0+k \in [a,b]$. Poiché $\mathbf{u}$ è derivabile in $t_0$, si ha:
\begin{align}
&\mathbf{u}(t_0+k)-\mathbf{u}(t_0)=\mathbf{u}'(t_0)\cdot k+\boldsymbol{\omega}(k)\cdot k, &\lim_{k\to 0}\boldsymbol{\omega}(k)=0\;.
\end{align}
Se $|\mathbf{h}|_N$ è sufficientemente piccola, poiché $f$ è differenziabile in $\mathbf{x}_0$, si ha:
\begin{align}
&f(\mathbf{x}_0+\mathbf{h})=f(\mathbf{x}_0)+\langle\nabla f(\mathbf{x}_0),\mathbf{h}\rangle_N+\eta(\mathbf{h})\cdot |\mathbf{h}|_N, &\lim_{|\mathbf{h}|_N\to 0}\eta(\mathbf{h})=0\;.
\end{align}
Scelto $\mathbf{h}=\mathbf{u}(t_0+k)-\mathbf{u}(t_0)$ si ha $k \to 0 \Longrightarrow |\mathbf{h}|_N \to 0$ e dunque:
\begin{equation}
\mathbf{x}_0+\mathbf{h}=\mathbf{u}(t_0+k)\;,
\end{equation}
da cui segue:
\begin{align}
f(\mathbf{u}(t_0+k))-f(\mathbf{u}(t_0))= &\langle\nabla f(\mathbf{u}(t_0)),\mathbf{u}(t_0+k)-\mathbf{u}(t_0)\rangle_N \notag \\
&+|\mathbf{u}(t_0+k)-\mathbf{u}(t_0)|_N\cdot\eta((\mathbf{u}(t_0+k)-\mathbf{u}(t_0))\;.
\end{align}
Il secondo addendo è infinitesimo di ordine superiore a $k$, dunque può essere trascurato. Si ottiene quindi:
\begin{equation}
f(\mathbf{u}(t_0+k))-f(\mathbf{u}(t_0)) =\langle \nabla f(\mathbf{u}(t_0)),\mathbf{u}'(t_0)\rangle_N\cdot k+\langle \nabla f(\mathbf{u}(t_0)),\boldsymbol{\omega}(k)\rangle_N\cdot k\;.
\end{equation}
In definitiva, si ha:
\begin{equation}
\lim_{k \to 0} \frac{f(\mathbf{u}(t_0+k))-f(\mathbf{u}(t_0))}{k}=\lim_{k \to 0}\left[\langle \nabla f(\mathbf{u}(t_0)),\mathbf{u}'(t_0)
\rangle_N+\langle \nabla f(\mathbf{u}(t_0)),\boldsymbol{\omega}(k)\rangle_N\right]\;,
\end{equation}
poiché $\mathbf{\omega}(k)$ è un infinitesimo di ordine superiore a $k$, segue:
\begin{equation}
\lim_{k\to 0}\frac{f(\mathbf{u}(t_0+k))-f(\mathbf{u}(t_0))}{k}=\langle\nabla f(\mathbf{u}(t_0)),\mathbf{u}'(t_0)\rangle_N\;.
\end{equation}
\endproof
\begin{thm} Siano $A \subseteq \mathbb{R}^N$ e $B \subseteq \mathbb{R}^P$ aperti, $\mathbf{g}:B \to A$ e $f:A \to \mathbb{R}$ tali che $\mathbf{g}(x)=(g^1(x),\ldots,g^N(x))$ sia differenziabile in un punto $\mathbf{x}_0 \in B$ e $f$ sia differenziabile in $\mathbf{g}(\mathbf{x_0})=\mathbf{y}_0 \in A$, allora $f \circ \mathbf{g}: B \to \mathbb{R}$ è differenziabile in $\mathbf{x}_0$ e si ha:
\begin{equation}
D_i (f \circ \mathbf{g})(\mathbf{x}_0)=\sum_{j=1}^N D_j f(\mathbf{y}_0) D_j \mathbf{g}(\mathbf{x}_0)= \langle \nabla f(\mathbf{y}_0),
D_i \mathbf{g}(\mathbf{x}_0)\rangle_N\;.
\end{equation}
\end{thm}
\begin{thm}[Lagrange N-dimensionale] Sia $A \subseteq \mathbb{R}^N$ aperto, $f:A \to \mathbb{R}$ differenziabile in $A$. Siano $\mathbf{x},\mathbf{y} \in A$ e:
\begin{equation}
I=\{ (1-t)\mathbf{x}+t\mathbf{y}, t \in [0,1]\} \subset A\;.
\end{equation}
Allora $\exists \mathbf{v} \in I$ tale che:
\begin{equation}
f(\mathbf{y})-f(\mathbf{x})=\langle \nabla f(\mathbf{v}),\mathbf{y}-\mathbf{x}\rangle_N\;.
\end{equation}
\end{thm}
\proof $\forall t \in [0,1]$ definisco $F(t)=f((1-t)\mathbf{x}+t\mathbf{y})$. Si ha dunque:
\begin{equation}
F'(t)=\langle \nabla f((1-t)\mathbf{x}+t\mathbf{y}),\mathbf{y}-\mathbf{x}\rangle_N\;.
\end{equation}
Per il teorema di Lagrange unidimensionale, $\exists \xi \in [0,1]$ tale che:
\begin{equation}
F'(\xi)=\frac{F(1)-F(0)}{1-0}\;,
\end{equation}
cioè:
\begin{equation}
f(\mathbf{y})-f(\mathbf{y})=\langle \nabla f((1-\xi)\mathbf{x}-\xi \mathbf{y}),\mathbf{y}-\mathbf{x}\rangle_N\;.
\end{equation}
Posto $\mathbf{v}=(1-\xi)\mathbf{x}+\xi \mathbf{y}$, otteniamo la tesi.
\endproof
\section{Derivate successive}
Se una funzione $f: A \to \mathbb{R}$ è differenziabile nell'aperto $A$, allora $\exists D_if:A \to \mathbb{R}, i=1,\ldots,N$. Se le 
derivate parziali sono a loro volta differenziabili in $A$, allora $\exists D_iD_jf(\mathbf{x}), \forall i,j=1,\ldots,N$.
\begin{defn} Una funzione $f$ si dice di \textit{classe k} in A e si denota $f \in C^k(A)$, se esistono continue tutte le derivate parziali di ordine $k$.
\end{defn}
\begin{oss} $C^0(A) = f$ continue su $A$.
\end{oss}
\begin{oss}
\begin{equation}
C^{\infty}(A)=\bigcap_{k \in \mathbb{N}} C^k(A)\;.
\end{equation}
\end{oss}
\begin{thm}[Schwartz bidimensionale] Sia $f \in C^2(A)$, allora $D_iD_jf=D_jD_if \quad \forall i,j=1,2$.
\end{thm}
\proof Sia $(x_0,y_0) \in A$ e $(h,k)$ un incremento sufficientemente piccolo. Definiamo la quantità:
\begin{equation}
\Delta(h,k)=f(x_0+h,y_0+k)-f(x_0+h,y_0)-f(x_0,y_0+k)+f(x_0,y_0)\;.
\end{equation}
Possiamo considerare $\Delta(h,k)$ come l'incremento della funzione:
\begin{equation}
x \longmapsto f(x,y_0+k)-f(x,y_0)\;.
\end{equation}
Allora per il teorema di Lagrange unidimensionale, $\exists \xi \in\; ]x_0,x_0+h[$ tale che:
\begin{equation}
\Delta(h,k)=h(f_x(\xi,y_0+k)-f_x(\xi,y_0)) \;. \label{ch4_1schwartz}
\end{equation}
Possiamo inoltre considerare $\Delta(h,k)$ come l'incremento delle funzione:
\begin{equation}
y \longmapsto f(x_0+h,y)-f(x_0,y)\;.
\end{equation}
Allora, sempre per il teorema di Lagrange unidimensionale, $\exists \eta \in\; ]y_0,y_0+k[$ tale che:
\begin{equation}
\Delta(h,k)=k(f_y(x_0+h,\eta)-f_y(x_0,\eta))\;. \label{ch4_2schwartz}
\end{equation}
La quantità espressa nella \eqref{ch4_1schwartz} è l'incremento della funzione $f_x$ fra $y_0$ e $y_0+k$. Per ipotesi, $f_x$ è continua e derivabile. Dunque, applicando nuovamente il teorema di Lagrange unidimensionale si ha che $\exists \omega \in\; ]y_0,y_0+k[$ tale che:
\begin{equation}
h(f_x(\xi,y_0+k)-f_x(\xi,y_0))=hkf_{xy}(\xi,\omega)\;.
\end{equation}
La quantità espressa nella \eqref{ch4_2schwartz} è invece l'incremento della funzione $f_y$ fra $x_0$ e $x_0+h$. Per ipotesi, $f_y$ è continua e derivabile. Dunque, applicando nuovamente il teorema di Lagrange unidimensionale, si ha che $\exists \tau \in\; ]x_0,x_0+h[$ tale che:
\begin{equation}
k(f_y(x_0+h,\eta)-f_y(x_0,\eta))=kh f_{yx}(\tau,\eta)\;.
\end{equation}
Pertanto, per la continuità delle derivate parziali di ordine due si ha:
\begin{equation}
\lim_{(h,k)\to(0,0)} \frac{\Delta(h,k)}{hk}=\begin{cases}
                                      f_{xy}(x_0,y_0) \\
                                      \\
f_{yx}(x_0,y_0)
                                     \end{cases}\;.
\end{equation}
Per l'unicità del limite, si conclude che:
\begin{equation}
f_{xy}(x_0,y_0)=f_{yx}(x_0,y_0)\;.
\end{equation}
\endproof
\begin{defn} Si definisce \textit{matrice Hessiana} di una funzione $f$ la matrice:
\begin{equation}
[H_f(\mathbf{x})]_{ij}=D_iD_jf(\mathbf{x})\;.
\end{equation}
\end{defn}
\begin{lem} $\forall n,k \in \mathbb{N}$ tali che $n \ge k$ si ha:
\begin{equation}
\sum_{i=k}^n \binom{i}{k}=\binom{n+1}{k+1}\;.
\end{equation}
\end{lem}
\proof (per induzione su $n$) \\

Per $n=k$ è banalmente vero. Dimostriamo che $n \Longrightarrow n+1$:
\begin{equation}
\sum_{i=k}^{n+1} \binom{i}{k}= \binom{n+1}{k}+\sum_{i=k}{n} \binom{i}{k}\;.
\end{equation}
Per l'ipotesi induttiva, si ottiene:
\begin{equation}
\sum_{i=k}^{n+1}\binom{i}{k}=\binom{n+1}{k}+\binom{n+1}{k+1}=\binom{n+2}{k+1}\;,
\end{equation}
dove abbiamo usato la formula di Stiefel.
\endproof
\begin{prop} Il numero di elementi distinti della matrice Hessiana di ordine $k$ è:
\begin{equation}
\binom{N+k-1}{k} \qquad \forall k \in \mathbb{N}\;.
\end{equation}
\end{prop}
\proof $\forall k \ge 2$, il numero di derivate distinte di ordine $k$ è dato da:
\begin{equation}
\sum_{i_k=1}^N \sum_{i_{k-1}=1}^{i_k} \ldots \sum_{i_2=1}^{i_3}\sum_{i_1=1}^{i_2} 1 = \sum_{i_k=1}^N \sum_{i_{k-1}=1}^{i_k} \ldots
 \sum_{i_2=1}^{i_3} \binom{i_2}{1}\;.
\end{equation}
La sommatoria più interna, per il lemma, diventa:
\begin{equation}
\sum_{i_k=1}^N\sum_{i_{k-1}=1}^{i_k}\ldots\sum_{i_2=1}^{i_3}\binom{i_2}{1}=\sum_{i_k=1}^N \sum_{i_{k-1}=1}^{i_k} \ldots \sum_{i_3=1}^{i_4} \binom{i_3+1}{2}\;.
\end{equation}
Iterando il procedimento, si ottiene:
\begin{equation}
\sum_{i_k=1}^N\sum_{i_{k-1}=1}^{i_k}\ldots\sum_{i_2=1}^{i_3}\sum_{i_1=1}^{i_2}1=\binom{N-k+1}{k}\;.
\end{equation}
\endproof
\begin{defn} Si definisce \textit{multi-indice} una $n$-upla $\mathbf{p}=(p_1,\ldots,p_N) \in \mathbb{N}^N$ con le seguenti proprietà:
\begin{itemize}
 \item $|\mathbf{p}|=\displaystyle{\sum_{i=1}^N p_i}$;
 \item $\mathbf{p}!=\displaystyle{\prod_{i_1}^N p_i!}$;
 \item $\mathbf{q}<\mathbf{p}\quad  \Longleftrightarrow \quad q_i<p_i \quad \forall i$;
 \item $D^{\mathbf{p}}f(\mathbf{x})=D_1^{p_1}\cdots D_N^{p_N}f(\mathbf{x})$;
 \item $\displaystyle{\binom{\mathbf{p}}{\mathbf{q}}=\prod_{i=1}^N \binom{p_i}{q_i}}$;
 \item $\mathbf{x}^{\mathbf{p}}=x_1^{p_1}\cdots x_N^{p_N}$.
\end{itemize}
\end{defn}
\begin{thm}[Formula di Taylor $N$-dimensionale] Sia $A \subseteq \mathbb{R}^N$ aperto, $f:A \to \mathbb{R}$, $f \in C^k(A)$ e $\mathbf{x}_0 \in A$. Allora esiste uno e un solo polinomio $P_k(x)$ di grado minore o uguale a $k$ tale che, per $\mathbf{x} \to \mathbf{x}_0$:
\begin{equation}
f(\mathbf{x})-P_k(\mathbf{x})=o\left(|\mathbf{x}-\mathbf{x}_0|_N^k\right)\;,
\end{equation}
con:
\begin{equation}
P_k(\mathbf{x})=\sum_{|\mathbf{p}|<k} \frac{D^{\mathbf{p}}f(\mathbf{x}_0)}{\mathbf{p}!}(\mathbf{x}-\mathbf{x}_0)^{\mathbf{p}}\;.
\end{equation}
\end{thm}
\proof (esistenza) $k \ge 1$ \\

Sia $\mathbf{v} \in \mathbb{R}^N$ di norma unitaria. Definiamo $F(t)=f(\mathbf{x}_0+t\mathbf{v})$, $t \in [-\delta,\delta]$ con 
$B(\mathbf{x}_0,\delta) \subset A$. Allora:
\begin{align}
F'(t) &= \langle \nabla f(\mathbf{x}_0+t\mathbf{v}),\mathbf{v}\rangle_N=\sum_{i=1}^N D_if(\mathbf{x}_0+t\mathbf{v})v_i\;, \\
F''(t) &= \sum_{i,j=1}^N D_jD_if(\mathbf{x}_0+t\mathbf{v})v_jv_i=\sum_{j=1}^N\left(\sum_{i=1}^N D_jD_if(\mathbf{x}_0+t\mathbf{v})v_i\right)v_j \notag \\
&= \sum_{|\mathbf{p}|=2}\frac{2!}{\mathbf{p}!}D^{\mathbf{p}}f(\mathbf{x}_0+t\mathbf{v})\mathbf{v}^{\mathbf{p}}\;, \\
F^{(k)}(t) &=\sum_{|\mathbf{p}|=k}\frac{k!}{\mathbf{p}!} D^{\mathbf{p}}f(\mathbf{x}_0+t\mathbf{v})\mathbf{v}^{\mathbf{p}}\;.
\end{align}
Applicando la formula di Taylor unidimensionale a $F(t)$ si ha, per $t \to 0$:
\begin{equation}
F(t)=\sum_{h=0}^k \frac{F^{(h)}(0)}{h!}t^h+o(|t|^k)\;,
\end{equation}
cioè, sostituendo:
\begin{align}
f(\mathbf{x}_0+t\mathbf{v}) &=\sum_{h=0}^k\frac{1}{h!}\sum_{|\mathbf{p}|=h} \frac{h!}{\mathbf{p}!}D^{\mathbf{p}}f(\mathbf{x}_0)t^h\mathbf{v}^{\mathbf{p}}+o(|t|^k) \notag \\
&=\sum_{|\mathbf{p}|\le k} \frac{1}{\mathbf{p}!}D^{\mathbf{p}}f(\mathbf{x}_0)(t\mathbf{v})^{\mathbf{p}}+o(|t\mathbf{v}|^k_N)\;. \label{ch4_1taylor}
\end{align}
Fissato $\mathbf{x}$, prendiamo $\mathbf{v}=(\mathbf{x}-\mathbf{x}_0)/|\mathbf{x}-\mathbf{x}_0|_N$ con $|\mathbf{x}-\mathbf{x}_0|_N< \delta$, da cui segue $t=|\mathbf{x}-\mathbf{x}_0|_N$. Sostituendo nella \eqref{ch4_1taylor} si ottiene:
\begin{equation}
f(\mathbf{x})=\sum_{|\mathbf{p}|\le k} \frac{1}{\mathbf{p}!}D^{\mathbf{p}}f(\mathbf{x}_0)(\mathbf{x}-\mathbf{x}_0)^{\mathbf{p}}+
o(|\mathbf{x}-\mathbf{x}_0|_N^k)\;.
\end{equation}
\endproof
\proof (unicità) \\

Supponiamo per assurdo che $\exists Q(\mathbf{x})$ di grado al più $k$ tale che per $\mathbf{x} \to \mathbf{x}_0$ si abbia:
\begin{equation}
f(\mathbf{x})-Q(\mathbf{x})=o(|\mathbf{x}-\mathbf{x}_0|_N^k)\;.
\end{equation}
Allora:
\begin{equation}
P_k(\mathbf{x})-Q(\mathbf{x})=\sum_{|\mathbf{p}|\le k} c_{\mathbf{p}}(\mathbf{x}-\mathbf{x}_0)^{\mathbf{p}}=o(|\mathbf{x}-
\mathbf{x}_0|_N^k)\;.
\end{equation}
Dunque si ha:
\begin{equation}
\frac{P_k(\mathbf{x}_0+t\mathbf{v})-Q(\mathbf{x}_0+t\mathbf{v})}{t^k} = \sum_{|\mathbf{p}|\le k} c_{\mathbf{p}} t^{|\mathbf{p}|-k}
\mathbf{v}^{\mathbf{p}} \to 0 \qquad \mbox{per}\quad t \to 0\;.
\end{equation}
Ma la somma può essere riscritta nella forma:
\begin{equation}
\sum_{|\mathbf{p}|\le k} c_{\mathbf{p}} t^{|\mathbf{p}|-k}\mathbf{v}^{\mathbf{p}}=\sum_{h=0}^k t^{h-k} \left(\sum_{|\mathbf{p}|=h} c_{\mathbf{p}} \mathbf{v}^{\mathbf{p}}\right)\;.
\end{equation}
Questa deve tendere a zero, ciò di conseguenza implica che:
\begin{equation}
\sum_{|\mathbf{p}|=h} c_{\mathbf{p}} \mathbf{v}^{\mathbf{p}}=0 \qquad \forall h \le k\;.
\end{equation}
Moltiplicando per un'opportuna costante, si ottiene:
\begin{equation}
\sum_{|\mathbf{p}|=h} c_{\mathbf{p}}\mathbf{x}^{\mathbf{p}}=0 \qquad \forall \mathbf{x} \in \mathbb{R}^N\;.
\end{equation}
Se $|\mathbf{q}|=h$ allora:
\begin{equation}
D^{\mathbf{q}}\left(\sum_{|\mathbf{p}|=h} c_{\mathbf{p}} \mathbf{x}^{\mathbf{p}}\right)= \mathbf{q}!c_{\mathbf{q}}=0\;.
\end{equation}
Ma $\mathbf{q}! \ne 0$ per ipotesi, dunque si ha $c_{\mathbf{q}}=0$ e di conseguenza $Q(\mathbf{x})=P_k(\mathbf{x})$.
\endproof
\begin{thm}[Formula di Taylor con resto di Lagrange] Sia $A \subseteq \mathbb{R}^N$ aperto, $f:A \to \mathbb{R}$, $f \in C^{k+1}(A)$ e $\mathbf{x}_0 \in A$. Allora, per $\mathbf{x} \to \mathbf{x}_0$ si ha che $\exists \BF{\xi} \in I=\{\mathbf{x}_0+t(\mathbf{x}-\mathbf{x}_0),t\in [0,1]\}$ tale che:
\begin{equation}
f(\mathbf{x})-P_k(\mathbf{x})=\sum_{|\mathbf{p}|=k+1} \frac{D^{\mathbf{p}}f(\BF{\xi})}{\mathbf{p}!} (\mathbf{x}-\mathbf{x}_0)^
{\mathbf{p}}\;.
\end{equation}
\end{thm}
\proof [Basta scrivere il resto di Lagrange di $F(t)=f(\mathbf{x}_0+t\mathbf{v})$].
\endproof
\begin{defn} Un insieme $A$ si dice \textit{connesso} se $\forall \mathbf{x}_0,\mathbf{x}_1 \in A\; \exists \mathbf{f}:[0,1] \to A$ continua tale che:
\begin{equation}
\mathbf{f}(0)=\mathbf{x}_0 \qquad \mathbf{f}(1)=\mathbf{x}_1\;.
\end{equation}
\end{defn}
\begin{thm} Sia $A \subseteq \mathbb{R}^N$ un aperto connesso e $f:A \to \mathbb{R}$, $f \in C^1(A)$ tale che $\nabla f \equiv \mathbf{0}$ in $A$. Allora $f$ è costante in $A$.
\end{thm}
\proof Sia $\mathbf{x}_0 \in A$ e sia $C=\{\mathbf{x} \in A\;|\;f(\mathbf{x})=f(\mathbf{x}_0)\}$. $C$ è non vuoto e chiuso. Inoltre si ha, 
fissato $\mathbf{x} \in C$ e $\delta >0$ e $\mathbf{y} \in B(\mathbf{x},\delta)$:
\begin{equation}
f(\mathbf{x})-f(\mathbf{y})=\langle \nabla f(\BF{\xi}),\mathbf{x}-\mathbf{y}\rangle_N\;,
\end{equation}
dove si è usato il resto di Lagrange di ordine 1. Ricordando che $\nabla f \equiv \mathbf{0}$, segue che:
\begin{equation}
f(\mathbf{y})=f(\mathbf{x})=f(\mathbf{x}_0)\;,
\end{equation}
poiché $\mathbf{x} \in C$. Allora $f(\mathbf{y})=f(\mathbf{x}_0) \Longrightarrow \mathbf{y} \in C$, da cui segue che $C$ è aperto. Si ha 
perciò:
\begin{equation}
A=C \cup (A \cap C^C)\;.
\end{equation}
Quindi $A$ sarebbe non connesso, il che contraddice l'ipotesi e dunque costituisce un assurdo. Pertanto, uno tra $C$ e $A\cap C^C$ deve 
essere vuoto. Per costruzione, $C$ è non vuoto, dunque $A \cap C^C \equiv \emptyset$, ma ciò implica $C \equiv A$ e quindi $f$ è costante su tutto $A$.
\endproof
\begin{thm}[Formula di Leibniz] Siano $f,g \in C^k(A)$, $A \subseteq \mathbb{R}^N$ aperto e $|\mathbf{p}|\le k$. Allora:
\begin{equation}
D^{\mathbf{p}}(f \cdot g)=\sum_{\mathbf{h}\le\mathbf{p}} \binom{\mathbf{p}}{\mathbf{h}}D^{\mathbf{h}}f \cdot D^{\mathbf{p}-\mathbf{h}}g\;.
\end{equation}
\end{thm}
\proof 
\begin{equation}
D^{\mathbf{p}}(fg)=D_N^{p_N}D_{N-1}^{p_{N-1}} \ldots D_2^{p_2}D_1^{p_1}(fg)\;.
\end{equation}
Possiamo applicare alla derivata più interna la formula di Leibniz unidimensionale, ottenendo:
\begin{equation}
D^{\mathbf{p}}(fg)=D_N^{p_N}D_{N-1}^{p_{N-1}} \ldots D_2^{p_2}\left(\sum_{h_1=0}^{p_1} \binom{p_1}{h_1}D_1^{h_1}f\cdot D_1^{p_1-h_1}g\right)\;.
\end{equation}
Iterando il procedimento per tutte le $N$ variabili, otteniamo:
\begin{align}
D^{\mathbf{p}}(fg) &= \sum_{h_1=0}^{p_1}\cdots\sum_{h_N=0}^{p_N}\binom{p_1}{h_1}\cdots\binom{p_N}{h_N}\left[(D_N^{h_N}\cdots D_1^{h_1}f)(D_N^{p_N-h_N}\cdots D_1^{p_1-h_1}g)\right] \notag \\
&=\sum_{|\mathbf{h}|\le|\mathbf{p}|}\binom{\mathbf{p}}{\mathbf{h}} D^{\mathbf{h}}fD^{\mathbf{p}-\mathbf{h}}g\;.
\end{align}
\endproof
\begin{defn} Una funzione $f$ si dice \textit{omogenea} di grado $\alpha \in \mathbb{R}$ se $\forall t>0$ si ha:
\begin{equation}
f(t\mathbf{x})=t^{\alpha}f(\mathbf{x}) \qquad \forall \mathbf{x} \in \mathbb{R}^N\;.
\end{equation}
\end{defn}
\begin{thm}[Eulero] Sia $A \subseteq \mathbb{R}^N$ aperto e $f:A \to \mathbb{R}$ omogenea di grado $\alpha$ e differenziabile in $A$. Allora le derivate parziali sono omogenee di grado $\alpha -1$ e si ha:
\begin{equation}
\sum_{i=1}^N \frac{\partial{f}}{\partial{x_i}}x_i= \langle \nabla f(\mathbf{x}),\mathbf{x}\rangle_N=\alpha f(\mathbf{x})\;.
\end{equation}
\end{thm}
\proof Fissato $t>0$ definisco $F(\mathbf{x})=f(t\mathbf{x})$. Allora:
\begin{equation}
D_iF(\mathbf{x})=\sum_{j=1}^N D_jf(t\mathbf{x})\cdot \delta_{ij}t=D_if(t\mathbf{x})t\;.
\end{equation}
Inoltre:
\begin{equation}
D_iF(\mathbf{x})=D_i(t^{\alpha}f(\mathbf{x}))=t^{\alpha}D_if(\mathbf{x})\;,
\end{equation}
da cui segue:
\begin{equation}
tD_if(t\mathbf{x})=t^{\alpha}f(\mathbf{x}) \Longleftrightarrow D_if(t\mathbf{x})=t^{\alpha-1}D_if(\mathbf{x})\;,
\end{equation}
che dimostra la prima parte del teorema. Consideriamo adesso l'identità:
\begin{equation}
\frac{\partial}{\partial{t}}\frac{f(t\mathbf{x})}{t^{\alpha}}=0\;.
\end{equation}
Svolgendo la derivata si ottiene:
\begin{equation}
\frac{\left(\sum_{i=1}^N D_if(t\mathbf{x})x_i\right)t^{\alpha}-f(t\mathbf{x})\alpha\cdot t^{\alpha-1}}{t^{2\alpha}}=0\;.
\end{equation}
Per ipotesi, $t>0$, quindi possiamo semplificare il denominatore e dividere il numeratore per $t^{\alpha-1}$, ottenendo:
\begin{equation}
t\sum_{i=1}^N D_if(t\mathbf{x})x_i-\alpha f(t\mathbf{x})=0 \Longleftrightarrow t\sum_{i=1}^N D_if(t\mathbf{x})x_i=\alpha f(t\mathbf{x})\;,
\end{equation}
da cui segue:
\begin{equation}
\langle \nabla f(t\mathbf{x}),\mathbf{x}\rangle_N=\alpha f(t\mathbf{x})\;,
\end{equation}
poiché l'identità vale $\forall t>0$, posto $t=1$ si ottiene:
\begin{equation}
\langle \nabla f(\mathbf{x}),\mathbf{x}\rangle_N=\alpha f(\mathbf{x})\;.
\end{equation}
\endproof
\section{Forme quadratiche}
\begin{defn} Sia $A \in M(n,\mathbb{R})$ simmetrica con $A=\{a_{ij}\}$. Si definisce \textit{forma quadratica associata} alla matrice $A$:
\begin{equation}
\phi(\mathbf{x})=\langle A\mathbf{x},\mathbf{x}\rangle_N = \sum_{i,j=1}^N a_{ij}x_ix_j\;.
\end{equation}
La forma quadratica associata ad una matrice è un polinomio omogeneo di grado 2.
\end{defn}
\begin{oss} $\phi \in C^{\infty}(\mathbb{R}^N)$.
\end{oss}
\begin{oss} $\nabla \phi(\mathbf{x})=2A\mathbf{x}$.
\end{oss}
\proof 
\begin{equation}
\sum_{k=1}^N D_k\phi(\mathbf{x})=\sum_{k=1}^N \left( \sum_{j=1}^N a_{kj}x_j+\sum_{i=1}^N a_{ik}x_i\right)=2\sum_{k,j=1}^Na_{kj}x_j=2A\mathbf{x}\;,
\end{equation}
poiché $A$ è simmetrica.
\endproof
\begin{defn} Una forma quadratica si dice:
\begin{itemize}
 \item \textit{definita positiva} se $\phi(\mathbf{x})>0 \quad \forall \mathbf{x} \in \mathbb{R}^N$;
 \item \textit{definita negativa} se $\phi(\mathbf{x})<0 \quad \forall \mathbf{x} \in \mathbb{R}^N$;
 \item \textit{semidefinita positiva} se $\phi(\mathbf{x})\ge0 \quad \forall \mathbf{x} \in \mathbb{R}^N$;
 \item \textit{semidefinita negativa} se $\phi(\mathbf{x})\le0 \quad \forall \mathbf{x} \in \mathbb{R}^N$;
 \item \textit{indefinita} se $\phi(\mathbf{x})$ assume valori positivi e negativi.
\end{itemize}
\end{defn}
\begin{oss} Sia $\Gamma=\{\mathbf{x}\in \mathbb{R}^N\;|\;|\mathbf{x}|_N=1\}$. Allora, poiché $\phi$ è continua, assumerà massimo $M_0$ e minimo $m_0$ su $\Gamma$. Supponiamo che $\phi(\mathbf{v}_0)=m_0$ e $\phi(\mathbf{w}_0)=M_0$ con $\mathbf{v}_0,\mathbf{w}_0 \in \Gamma$. Allora:
\begin{equation}
m_0 \le \phi(\mathbf{v}) \le M_0 \qquad \forall \mathbf{v} \in \Gamma\;.
\end{equation}
Per omogeneità, possiamo scrivere:
\begin{equation}
\phi(\mathbf{x})=|\mathbf{x}|_N^2 \cdot \phi\left(\frac{\mathbf{x}}{|\mathbf{x}|_N}\right) \quad \forall \mathbf{x} \in \mathbb{R}^N\;.
\end{equation}
Dunque si avrà:
\begin{equation}
m_0|\mathbf{x}|_N^2 \le \phi(\mathbf{x}) \le M_0|\mathbf{x}|_N^2 \qquad \forall \mathbf{x} \in \mathbb{R}^N\;.
\end{equation}
\end{oss}
\begin{prop} $m_0$ e $M_0$ sono rispettivamente il minimo ed il massimo autovalore di $A$.
\end{prop}
\proof Definiamo:
\begin{equation}
F(\mathbf{x})=\frac{\phi(\mathbf{x})}{|\mathbf{x}|_N^2} \qquad \forall \mathbf{x} \in \mathbb{R}^N \setminus \{\mathbf{0}\}\;.
\end{equation}
Si ha dunque:
\begin{equation}
F(\mathbf{v}_0)=m_0 \le F(\mathbf{x}) \le M_0=F(\mathbf{w}_0) \quad \forall \mathbf{x} \in \mathbb{R}^N \setminus \{\mathbf{0}\}\;.
\end{equation}
Nei punti $\mathbf{v}_0$ e $\mathbf{w}_0$ il gradiente di $F$ deve essere nullo; infatti, posto:
\begin{equation}
g(t)=F(\mathbf{v}_0+t\mathbf{x}) \qquad t \in [-\delta,\delta], \forall \mathbf{x} \in \mathbb{R}^N\;,
\end{equation}
si osserva che $g$ ha un minimo per $t=0$, dunque:
\begin{equation}
g'(0)=\langle \nabla F(\mathbf{v}_0),\mathbf{x}\rangle_N=0 \Longrightarrow \nabla F(\mathbf{v}_0)=0\;.
\end{equation}
Allo stesso modo si dimostra che il gradiente di $F$ è nullo in $\mathbf{w}_0$. Calcoliamo dunque il gradiente di $F$:
\begin{equation}
D_kF(\mathbf{x})=\frac{D_k\phi(\mathbf{x})|\mathbf{x}|_N^2-\phi(\mathbf{x})\cdot 2x_k}{|\mathbf{x}|_N^4}= \frac{D_k\phi(\mathbf{x})}{
|\mathbf{x}|_N^2}-\frac{2\phi(\mathbf{x})x_k}{|\mathbf{x}|_N^4}\;,
\end{equation}
da cui:
\begin{align}
\nabla F(\mathbf{x}) &= \sum_{k=1}^N D_kF(\mathbf{x})=\frac{\nabla \phi(\mathbf{x})}{|\mathbf{x}|_N^2}-\frac{2\phi(\mathbf{x})\mathbf{x}}{|\mathbf{x}|_N^4} \notag \\
&= \frac{2A\mathbf{x}}{|\mathbf{x}|_N^2}-\frac{2\phi(\mathbf{x})\mathbf{x}}{|\mathbf{x}|_N^4}=\frac{2}{|\mathbf{x}|_N^2}(A\mathbf{x}-F(\mathbf{x})\mathbf{x})\;.
\end{align}
Allora:
\begin{equation}
\nabla F(\mathbf{v}_0)=2(A\mathbf{v}_0-F(\mathbf{v}_0)\mathbf{v}_0)=0\;.
\end{equation}
Ma $F(\mathbf{v}_0)=m_0$, dunque si ha:
\begin{equation}
A\mathbf{v}_0=m_0\mathbf{v}_0\;.
\end{equation}
Ossia $\mathbf{v}_0$ è autovettore relativo all'autovalore $m_0$. Analogamente, si ha:
\begin{equation}
A\mathbf{w}_0 = M_0\mathbf{w}_0\;.
\end{equation}
Cioè $\mathbf{w}_0$ è autovettore relativo all'autovalore $M_0$. Se $\lambda$ è autovalore per $A$ con autovettore $\mathbf{v} \in 
\Gamma$ allora:
\begin{equation}
\phi(\mathbf{v})=\langle A\mathbf{v},\mathbf{v}\rangle_N = \lambda |\mathbf{v}|_N^2=\lambda\;.
\end{equation}
Ma, poiché $m_0$ e $M_0$ sono il minimo ed il massimo di $\phi$ su $\Gamma$, si avrà:
\begin{equation}
m_0 \le \lambda \le M_0\;,
\end{equation}
da cui segue che $m_0$ è il minimo autovalore e $M_0$ è il massimo autovalore.
\endproof
\begin{oss} Sia $A \in M(\mathbb{R},N)$ simmetrica.
\begin{equation}
\det(A-\lambda I)=\prod_{i=1}^N (\lambda-\lambda_i)=\lambda^N+a_1\lambda^{N-1}+\cdots+a_{N-1}\lambda+a_N\;.
\end{equation}
Per la regola di Cartesio, si ha che;
\begin{itemize}
 \item $\phi$ è definito negativo $\Longleftrightarrow$ l'equazione presenta $N$ permanenze di segno;
 \item $\phi$ è definito positivo $\Longleftrightarrow$ l'equazione presenta $N$ variazioni di segno;
 \item $\phi$ è semidefinito negativo $\Longleftrightarrow$ l'equazione presenta $N-r$ permanenze di segno e $r$ coefficienti nulli;
 \item $\phi$ è semidefinito positivo $\Longleftrightarrow$ l'equazione presenta $N-r$ variazioni di segno e $r$ coefficienti nulli;
 \item $\phi$ è indefinito negli altri casi.
\end{itemize}
\end{oss}
\begin{defn} Sia $f:A \to \mathbb{R}$ una funzione definita su $A \subseteq \mathbb{R}^N$ e $\mathbf{x}_0 \in A$.
\begin{itemize}
 \item Si dice che $\mathbf{x}_0$ è un punto di \textit{massimo locale} per $f$ se $\exists B(\mathbf{x}_0,\delta)\subseteq A$ tale che 
$f(\mathbf{x})\le f(\mathbf{x}_0) \quad \forall \mathbf{x} \in B$;
 \item si dice che $\mathbf{x}_0$ è un punto di \textit{minimo locale} per $f$ se $\exists B(\mathbf{x}_0,\delta)\subseteq A$ tale che 
$f(\mathbf{x})\ge f(\mathbf{x}_0) \quad \forall \mathbf{x} \in B$.
\end{itemize}
\end{defn}
\begin{thm} Sia $f \in C^2(A)$, $\mathbf{x}_0 \in A$ e $\phi(\mathbf{x})=\langle H_f(\mathbf{x}_0)\mathbf{x},\mathbf{x}\rangle_N$. Allora
\begin{enumerate}
 \item $\mathbf{x}_0$ è punto di massimo relativo per $f \Longrightarrow \nabla f(\mathbf{x}_0)=\mathbf{0}$ e $\phi$ è semidefinito 
negativo;
 \item $\mathbf{x}_0$ è punto di minimo relativo per $f \Longrightarrow \nabla f(\mathbf{x}_0)=\mathbf{0}$ e $\phi$ è semidefinito 
positivo;
 \item $\nabla f(\mathbf{x}_0)=\mathbf{0}$ e $\phi$ è definito negativo $\Longrightarrow \mathbf{x}_0$ è punto di massimo relativo;
 \item $\nabla f(\mathbf{x}_0)=\mathbf{0}$ e $\phi$ è definito positivo $\Longrightarrow \mathbf{x}_0$ è punto di minimo relativo;
 \item $\nabla f(\mathbf{x}_0)=\mathbf{0}$ e $\phi$ è indefinito $\Longrightarrow \mathbf{x}_0$ è punto di sella.
\end{enumerate}
\end{thm}

\textbf{Premessa} \\
(a) Se $\mathbf{x} \in B(\mathbf{x}_0,\delta)\subseteq A$, $\forall t \in [0,1]$ definiamo $F(t)=f(\mathbf{x}_0+t(\mathbf{x}-\mathbf{x}_0))$. 
Si ha dunque:
\begin{align}
F'(t) &=\langle \nabla f(\mathbf{x}_0+t(\mathbf{x}-\mathbf{x}_0))(\mathbf{x}-\mathbf{x}_0),\mathbf{x}-\mathbf{x}_0\rangle_N\;, \notag \\
F''(t) &= \langle H_f(\mathbf{x}_0+t(\mathbf{x}-\mathbf{x}_0))(\mathbf{x}-\mathbf{x}_0),\mathbf{x}-\mathbf{x}_0\rangle_N\;.
\end{align}
(b) $\forall \mathbf{x} \in B(\mathbf{x}_0,\delta)\subseteq A\; \exists \xi \in\; ]0,1[$ tale che:
\begin{equation}
f(\mathbf{x})=f(\mathbf{x}_0)+\langle \nabla f(\mathbf{x}_0),\mathbf{x}-\mathbf{x}_0\rangle_N+\frac{1}{2}\langle H_f(\mathbf{x}_0+\xi
(\mathbf{x}-\mathbf{x_0}))(\mathbf{x}-\mathbf{x}_0),\mathbf{x}-\mathbf{x}_0\rangle_N\;.
\end{equation}
\proof $(1)$ \\
Dalla dimostrazione precedente, sappiamo che se $\mathbf{x}_0$ è punto di massimo relativo per $f$, allora $\nabla f(\mathbf{x}_0)=
\mathbf{0}$. Inoltre, $F$ ha un massimo in corrispondenza di $t=0$, dunque dovrà essere $F''(0)\le 0$ cioè:
\begin{equation}
\langle H_f(\mathbf{x}_0)(\mathbf{x}-\mathbf{x}_0),\mathbf{x}-\mathbf{x}_0\rangle_N \le 0\;.
\end{equation}
Posto $\mathbf{x}-\mathbf{x}_0=\mathbf{v}$ segue:
\begin{equation}
\langle H_f(\mathbf{x}_0)\mathbf{v},\mathbf{v}\rangle_N=\phi(\mathbf{v})\le 0 \qquad \forall \mathbf{v} \in A\;.
\end{equation}
\endproof
$(2)$ si dimostra come $(1)$
\proof $(3)$ \\
Sia $\nabla f(\mathbf{x}_0)=\mathbf{0}$ e $\phi$ definito negativo. Allora gli autovalori di $H_f(\mathbf{x}_0)$ sono tutti negativi. Sia 
$-\delta$ il massimo autovalore. Ne segue:
\begin{equation}
\langle H_f(\mathbf{x}_0)\mathbf{v},\mathbf{v}\rangle_N \le -\delta|\mathbf{v}|_N^2 \quad \forall \mathbf{v} \in \mathbb{R}^N \;. \label{ch4_hnegativo}
\end{equation}
Dimostriamo che:
\begin{equation}
\langle H_f(\mathbf{x})\mathbf{v},\mathbf{v}\rangle_N \le -\frac{\delta}{2}|\mathbf{v}|_N^2\;.
\end{equation}
$\forall \mathbf{v} \in \mathbb{R}^N$ e $\forall \mathbf{x} \in B(\mathbf{x}_0,r)$ con $r$ sufficientemente piccolo. Abbiamo allora:
\begin{align}
\langle H_f(\mathbf{x})\mathbf{v},\mathbf{v}\rangle_N &= \langle H_f(\mathbf{x})\mathbf{v},\mathbf{v}\rangle_N-\langle H_f(\mathbf{x}_0)\mathbf{v},\mathbf{v}\rangle_N+\langle H_f(\mathbf{x}_0)\mathbf{v},\mathbf{v}\rangle_N \notag \\
&=\langle [H_f(\mathbf{x})-H_f(\mathbf{x}_0)]\mathbf{v},\mathbf{v}\rangle_N+\langle H_f(\mathbf{x}_0)\mathbf{v},\mathbf{v}\rangle_N\;.
\end{align}
Maggioriamo il primo addendo usando la diseguaglianza di Cauchy-Schwarz e il secondo addendo usando l'ipotesi \eqref{ch4_hnegativo}:
\begin{align}
\langle H_f(\mathbf{x})\mathbf{v},\mathbf{v}\rangle_N &\le \left|H_f(\mathbf{x})-H_f(\mathbf{x}_0)]\mathbf{v}\right|_N\cdot |\mathbf{v}_N-\delta|\mathbf{v}|_N^2 \notag \\
&= \left|[H_f(\mathbf{x})-H_f(\mathbf{x}_0)\right|_{M(N)}\cdot |\mathbf{v}|_N^2-\delta|\mathbf{v}|_N^2\;.
\end{align}
Per la continuità delle derivate seconde, $\forall \mathbf{x} \in B(\mathbf{x}_0,r)$ si ha:
\begin{equation}
\frac{\partial^2}{\partial x_i\partial x_j} f(\mathbf{x})-\frac{\partial^2}{\partial x_i\partial x_j}f(\mathbf{x}_0) < \frac{\delta}{2}
\quad \forall i,j\;,
\end{equation}
e dunque:
\begin{equation}
\langle H_f(\mathbf{x})\mathbf{v},\mathbf{v}\rangle_N\le \frac{\delta}{2}|\mathbf{v}|_N^2 - \delta |\mathbf{v}|_N^2=-\frac{\delta}{2}|\mathbf{v}|_N^2\;.
\end{equation}
Quindi $\phi$ è definito negativo. Sviluppiamo $f$ in serie di Taylor $\forall \mathbf{x} \in \mathbb{R}^N$ tale che $|\mathbf{x}-\mathbf{x}_0|_N<r_{\delta}$, con $r_{\delta}$ opportunamente piccolo:
\begin{equation}
f(\mathbf{x})-f(\mathbf{x}_0)=\langle \nabla f(\mathbf{x}_0),\mathbf{x}-\mathbf{x}_0\rangle_N+\frac{1}{2}\langle H_f(\mathbf{x}_0)(\mathbf{x}-\mathbf{x}_0),\mathbf{x}-\mathbf{x}_0\rangle_N\;.
\end{equation}
Ma per ipotesi $\nabla f\equiv \mathbf{0}$ e $\langle H_f(\mathbf{x}_0)(\mathbf{x}-\mathbf{x}_0),\mathbf{x}-\mathbf{x}_0\rangle_N \le 
-\delta |\mathbf{x}-\mathbf{x}_0|_N^2$, dunque:
\begin{equation}
f(\mathbf{x})-f(\mathbf{x}_0) \le -\frac{\delta}{2}|\mathbf{x}-\mathbf{x}_0|_N^2 \le 0\;.
\end{equation}
Da questo segue che $f(\mathbf{x})\le f(\mathbf{x}_0) \quad \forall \mathbf{x} \in B(\mathbf{x}_0,r_{\delta})$, cioè $\mathbf{x}_0$ è un 
punto di massimo relativo.
\endproof
$(4)$ si dimostra come $(3)$ con stime invertite.
\section{Operatore di Laplace in due dimensioni}
\begin{defn} Sia $u(x,y) \in C^2(I)$. Il \textit{laplaciano} di $u$ è definito:
\begin{equation}
\nabla^2 u(x,y)=u_{xx}(x,y)+u_{yy}(x,y)\;.
\end{equation}
In coordinate polari, posti $x=\rho \cos \theta$ e $y=\rho \sin \theta$ si ha:
\begin{equation}
v(\rho,\theta)=u(\rho \cos \theta, \rho\sin\theta)\;.
\end{equation}
Le derivate parziali di ordine 1 sono:
\begin{align}
&v_{\rho}=u_x\cos\theta +u_y\sin\theta, & v_{\theta}=-u_x\rho\sin\theta+u_y\rho\cos\theta\;.
\end{align}
Le derivate parziali di ordine 2 non miste sono invece:
\begin{align}
v_{\rho\rho} &= u_{xx}\cos^2\theta+2u_{xy}\sin\theta\cos\theta+u_{yy}\sin^2\theta\;, \\
v_{\theta\theta} &= u_{xx}\rho^2\sin^2\theta-2u_{xy}\rho^2\sin\theta\cos\theta+u_{yy}\rho^2\cos^2\theta-u_x\rho\cos\theta-u_y\rho\sin\theta\;.
\end{align}
Si ha quindi:
\begin{equation}
v_{\rho\rho}+\frac{1}{\rho^2}v_{\theta\theta}=u_{xx}+u_{yy}-\frac{1}{\rho}v_{\rho}=\nabla^2u(x,y)-\frac{1}{\rho}v_{\rho}\;,
\end{equation}
da cui:
\begin{equation}
\nabla^2v(\rho,\theta)=\frac{1}{\rho}\frac{\partial}{\partial \rho}\left(\rho\frac{\partial}{\partial\rho}v(\rho,\theta)\right)+
\frac{1}{\rho^2}\frac{\partial^2}{\partial\theta^2}v(\rho,\theta)\;.
\end{equation}
\end{defn}
\chapter{Funzioni implicite}
\section{Caso bidimensionale}
\begin{thm}[Funzioni implicite o del Dini]  Sia $F \in C^1(A)$, con $A$ aperto di $\mathbb{R}^{2}$. Sia $Z=\{(x,y)\in A\;|\;F(x,y)=0\}$. Sia $(x_0,y_0) \in Z$. Se in $(x_0,y_0)$ si ha $\nabla F(x_0,y_0) \ne \mathbf{0}$ allore esiste un intorno $U \times V$ del punto $(x_0,y_0)$ tale che $Z \cap (U \times V)$ è grafico di una funzione di classe $C^1$. In particolare, se $F_y(x_0,y_0)\ne 0$, allora $\exists g:U \to V$ di classe $C^1$, tale che:
\begin{equation}
(x,y) \in U \times V, \quad F(x,y)=0 \qquad \Longleftrightarrow \qquad y=g(x)\;.
\end{equation}
Inoltre $g(x_0)=y_0$ e:
\begin{equation}
g'(x)=-\frac{F_x(x,g(x))}{F_y(x,g(x))} \qquad \forall x \in U\;.
\end{equation}
Se inoltre $F \in C^k$ allora la funzione implicita $g$ è di classe $C^k$.
\end{thm}
\proof Sia $U_0 \times V$ un rettangolo contenuto in $A$ e centrato nel punto $(x_0,y_0)$ tale che $F_x(x,y)>0\quad \forall (x,y)\in U_0\times V$. Siano:
\begin{align}
&U_0=[x_0-h_0,x_0+h_0], &V=[y_0-k,y_0+k]\;.
\end{align}
Avremo $F(x_0,y_0+k)>0$ e $F(x_0,y_0-k)<0$. Allora, per la continuità di $F$:
\begin{equation}
\exists U \subseteq U_0\;\mbox{tale che}\; \begin{cases}
                                            F(x,y_0+k)>0 \\
                                            \\
F(x,y_0-k)<0
                                           \end{cases}
\forall x \in U
\end{equation}
Fissato $x \in U$, consideriamo la funzione $y \mapsto F(x,y)$. Per il teorema di esistenza degli zeri delle funzioni continue e per la 
monotonia della derivata prima, $\exists! y \in V$ tale che $F(x,y)=0$. Definiamo $y\equiv g(x)$. Si avrà allora:
\begin{equation}
(x,y) \in U \times V,\quad F(x,y)=0\qquad  \Longleftrightarrow\qquad  y=g(x)\;.
\end{equation}
In particolare, $F(x,g(x))=0 \quad \forall x \in U$. Siano $x,x' \in U$, allora si avrà:
\begin{equation}
0=F(x,g(x))-F(x',g(x'))\;,
\end{equation}
$\forall t \in [0,1]$ definiamo:
\begin{equation}
G(t)\equiv F(x+t(x'-x), g(x)+t(g(x')-g(x)))\;.
\end{equation}
Si ha evidentemente:
\begin{equation}
F(x,g(x))-F(x',g(x')) = G(1)-G(0)\;.
\end{equation}
Per il teorema di Lagrange, $\exists \xi \in\; ]0,1[$ tale che:
\begin{equation}
G(1)-G(0)=G'(\xi)(1-0)=G'(\xi)\;.
\end{equation}
Esplicitando la derivata prima di $G$ in $\xi$:
\begin{equation}
G'(\xi)= F_x(x_{\xi},y_{\xi})(x'-x)+F_y(x_{\xi},y_{\xi})(g(x')-g(x))=0\;,
\end{equation}
da cui segue:
\begin{equation}
g(x')-g(x)=-\frac{F_x(x_{\xi},y_{\xi})}{F_y(x_{\xi},y_{\xi})}(x'-x)\;.
\end{equation}
Per il teorema di Weierstrass:
\begin{equation}
\exists m=\min_{U_0 \times V} F_y(x,y)>0, \qquad\qquad \exists M=\max_{U_0 \times V} |F_x(x,y)|\;,
\end{equation}
allora:
\begin{equation}
|g(x')-g(x)|=-\left|\frac{F_x(x_{\xi},y_{\xi})}{F_y(x_{\xi},y_{\xi})}\right| |x'-x| \le \frac{M}{m}|x'-x|\;,
\end{equation}
dunque $g$ è localmente Lipschitziana e quindi continua. Inoltre, per $x' \to x$ si ha:
\begin{equation}
x_{\xi} \to x, \qquad\qquad y_{\xi} \to g(x)\;,
\end{equation}
da cui segue:
\begin{equation}
\lim_{x' \to x} \frac{g(x')-g(x)}{x'-x} = -\frac{F_x(x,g(x))}{F_y(x,g(x))}\;,
\end{equation}
per cui $g$ è derivabile e in più la derivata prima è continua in quanto composizione di funzioni continue per ipotesi. Quindi
$g \in C^1(U \times V)$. Inoltre:
\begin{equation}
F \in C^k\quad  \Longrightarrow\quad  F_x,F_y \in C^{k-1}\quad \Longrightarrow\quad g' \in C^{k-1} \Longrightarrow g \in C^k\;.
\end{equation}
\endproof
\section{Contrazioni}
\begin{defn} Sia $(X,d)$ uno spazio metrico. Una \textit{contrazione} su $X$ è un'applicazione $F:X \to X$ per la quale $\exists \lambda \in [0,1[$ tale che:
\begin{equation}
d(F(x),F(x')) \le \lambda d(x,x') \qquad \forall x,x' \in X\;.
\end{equation}
\end{defn}
\begin{thm}[delle contrazioni] Sia $(X,d)$ uno spazio metrico completo e sia $F:X \to X$ una contrazione. Allora $F$ ha un unico punto fisso, cioè $\exists! \overline{x} \in X$ tale che $F(\overline{x})=\overline{x}$.
\end{thm}
\proof (esistenza) \\

Per ipotesi, $\exists \lambda \in [0,1[$ tale che:
\begin{equation}
d(F(x),F(x')) \le \lambda d(x,x') \qquad \forall x,x' \in X\;.
\end{equation}
Sia $x^* \in X$. Definiamo per ricorrenza la successione:
\begin{equation}
\begin{cases}
 x_0=x^* \\
 \\
x_{n+1}=F(x_n) \quad n \in \mathbb{N}
\end{cases}\;.
\end{equation}
Osserviamo che:
\begin{equation}
d(x_{n+1},x_n)=d(F(x_n),F(x_{n-1})) \le \lambda d(x_n,x_{n-1})\;,
\end{equation}
e quindi:
\begin{equation}
d(x_{n+1},x_n) \le \lambda d(x_n,x_{n-1}) \le \ldots \le \lambda^n d(x_1,x_0)\;.
\end{equation}
Applicando la diseguaglianza triangolare, se $m>n$ si ha:
\begin{equation}
d(x_m,x_n) \le \sum_{h=n}^{m-1} d(x_{h+1},x_h) \le \sum_{h=n}^{m-1} \lambda^h d(x_1,x^*)\;.
\end{equation}
Poiché la serie $\sum \lambda^h$ è convergente, la successione $\{x_n\}$ è di Cauchy in $X$. Dato che $X$ è completo, essa converge ad un elemento $\overline{x} \in X$. Proviamo che $\overline{x}$ è un punto fisso per $F$:
\begin{align}
d(\overline{x},F(\overline{x})) &\le d(\overline{x},x_{n+1})+d(x_{n+1},F(\overline{x}))=d(\overline{x},x_{n+1})+d(F(x_n),F(\overline{x})) \notag \\
&\le d(\overline{x},x_{n+1})+\lambda d(x_n,F(\overline{x}))\;,
\end{align}
da cui, per $n \to \infty$, otteniamo $d(\overline{x},F(\overline{x}))=0$ cioè $F(\overline{x})=\overline{x}$.
\endproof
\proof (unicità) \\

Se $\underline{x}\in X$ è un altro punto fisso per $F$, si ha:
\begin{equation}
d(\overline{x},\underline{x})=d(F(\overline{x}),F(\underline{x})) \le \lambda d(\overline{x},\underline{x})\;,
\end{equation}
ma, essendo $\lambda<1$, ciò risulta impossibile se $\overline{x}\ne\underline{x}$. Si conclude dunque che $\overline{x}=\underline{x}$.
\endproof
\begin{thm}[delle contrazioni dipendenti da parametro] Siano $(B,\delta)$ uno spazio metrico, $(X,d)$ uno spazio metrico completo e $T:B\times X \to X$ un'applicazione continua. Supponiamo che $\exists \lambda \in [0,1[$ tale che:
\begin{equation}
d(T(b,x),T(b,x')) \le \lambda d(x,x') \qquad \forall x,x' \in X, \quad \forall b \in B\;.
\end{equation}
Allora, $\forall b \in B\; \exists! x_b \in X$ tale che $T(b,x_b)=x_b$ e inoltre la funzione:
\begin{equation}
\begin{matrix}
&\beta: & B & \to &X \\
&{} &b &\mapsto & x_b
\end{matrix}
\end{equation}
è continua.
\end{thm}
\proof $\forall b \in B$ il punto fisso esiste unico per il teorema precedente. Inoltre posso scrivere $\forall a,b \in B$:
\begin{align}
d(x_a,x_b) &= d(T(a,x_a),T(b,x_b)) \le d(T(a,x_a),T(b,x_a))+d(T(b,x_a),T(b,x_b)) \notag \\
&\le d(T(a,x_a),T(b,x_a))+\lambda d(x_a,x_b)\;,
\end{align}
da cui segue:
\begin{equation}
d(x_a,x_b) \le \frac{1}{1-\lambda} d(T(a,x_a),T(b,x_b)) \qquad \forall a,b \in B\;.
\end{equation}
Tenuto fisso $a \in B$, fissiamo $\epsilon>0$. Per la continuità di $T$ nel punto $(a,x_a)$, $\exists \eta>0$ tale che $\forall b \in B$ 
soddisfacenti $\delta(a,b)<\eta$ si ha:
\begin{equation}
d(T(a,x_a),T(b,x_b))<(1-\lambda)\epsilon\;.
\end{equation}
Da ciò segue che, se $\delta(a,b)<\eta$:
\begin{equation}
d(x_a,x_b)< \epsilon\;.
\end{equation}
\endproof
\section{Caso generale}
\begin{defn} Sia $\mathbf{F}:\mathbb{R}^N \to \mathbb{R}^M$ una funzione data da $\mathbf{y}=\mathbf{F}(\mathbf{x})$. Le $M$ componenti del vettore $\mathbf{F}(\mathbf{x})$ sono funzioni reali delle $N$ variabili $x_1,\ldots,x_N$. Le derivate parziali (se esistono) possono essere organizzate in una matrice $M \times N$, detta \textit{Jacobiana} di $\mathbf{F}$ nel modo seguente:
\begin{equation}
D\mathbf{F}(\mathbf{x})=\begin{pmatrix}
                         \dfrac{\partial F_1}{\partial x_1} & \cdots & \dfrac{\partial F_1}{\partial x_N} \\
\vdots & \vdots & \vdots \\
\dfrac{\partial F_M}{\partial x_1} & \cdots & \dfrac{\partial F_M}{\partial x_N}
                        \end{pmatrix}\;.
\end{equation}
Inoltre, se $M=N$ la Jacobiana di $\mathbf{F}$ è una matrice quadrata, lo \textit{Jacobiano} di $\mathbf{F}$ è definito come il 
determinante della matrice Jacobiana.
\end{defn}
\begin{thm}[del Dini, caso generale] Sia $\mathbf{F}:A \subseteq \mathbb{R}^N \to \mathbb{R}^K$ di classe $C^1$, con $N=r+K>K$ e $Z=\{ \mathbf{x}\in A\;|\; \mathbf{F}(\mathbf{x})=\mathbf{0}\}$. Se $\mathbf{z}_0 \in Z$ è tale che $D\mathbf{F}(\mathbf{z}_0)$ abbia rango massimo $K$, allora esiste un intorno $U\subseteq A$ di $\mathbf{z}_0$ tale che $Z \cap U$ è grafico di una funzione $\mathbf{f}$ definita su un aperto di $\mathbb{R}^r$ in $\mathbb{R}^K$ di classe $C^1$. Più precisamente, posti:
\begin{align}
&\mathbf{z}=(\mathbf{x},\mathbf{y}),\qquad \mathbf{x}\in\mathbb{R}^r,\mathbf{y}\in\mathbb{R}^k\;, \\
& D\mathbf{F}(\mathbf{x},\mathbf{y})=(D_x\mathbf{F}(\mathbf{x},\mathbf{y})|D_y\mathbf{F}(\mathbf{x},\mathbf{y}))\;,
\end{align}
supponendo $\mathbf{F}(\mathbf{x}_0,\mathbf{y}_0)=0$ e $\det D\mathbf{F}_y(\mathbf{x}_0,\mathbf{y}_0)\ne 0$, allora esistono $V$ intorno 
di $\mathbf{x}_0$ e $W$ intorno di $\mathbf{y}_0$ chiusi, $V\times W \subseteq A$ ed esiste $\mathbf{f}:V \to W, \mathbf{f} \in C^1$ tali
 che:
\begin{equation}
\mathbf{F}(\mathbf{x},\mathbf{y})=0 \Longleftrightarrow \mathbf{y}=\mathbf{f}(\mathbf{x}), \quad (\mathbf{x},\mathbf{y})\in Z \cap 
(V \times W)\;.
\end{equation}
Inoltre:
\begin{equation}
D\mathbf{f}(\mathbf{x})=-[D_y\mathbf{F}(\mathbf{x},\mathbf{f}(\mathbf{x}))]^{-1}[D_x\mathbf{F}(\mathbf{x},\mathbf{f}(\mathbf{x}))] \quad \forall \mathbf{x} \in V\;.
\end{equation}
\end{thm}
\proof Essendo $\mathbf{F}$ per ipotesi differenziabile in $(\mathbf{x}_0,\mathbf{y}_0)$ e $\mathbf{F}(\mathbf{x}_0,\mathbf{y}_0)=0$, possiamo scrivere:
\begin{equation}
\mathbf{F}(\mathbf{x},\mathbf{y})=D_x\mathbf{F}(\mathbf{x}_0,\mathbf{y}_0)(\mathbf{x}-\mathbf{x}_0)+D_y\mathbf{F}(\mathbf{x}_0,\mathbf{y_0
})(\mathbf{y}-\mathbf{y}_0)+\mathbf{v}(\mathbf{x},\mathbf{y})\;, \label{ch5_dinigen}
\end{equation}
dove $\mathbf{v}$ è una funzione di classe $C^1(A,\mathbb{R}^k)$ tale che:
\begin{equation}
\frac{\mathbf{v}(\mathbf{x},\mathbf{y})}{\sqrt{|\mathbf{x}-\mathbf{x_0}|_r^2+|\mathbf{y}-\mathbf{y}_0|_k^2}} \to \mathbf{0}
\quad\mbox{per}\quad \sqrt{|\mathbf{x}-\mathbf{x_0}|_r^2+|\mathbf{y}-\mathbf{y}_0|_k^2} \to 0\;.
\end{equation}
Dato che, per ipotesi, la matrice $D_y\mathbf{F}(\mathbf{x}_0,\mathbf{y}_0)$ è invertibile, dalla relazione \eqref{ch5_dinigen} ricaviamo:
\begin{equation}
\mathbf{y}=\mathbf{y}_0+B\mathbf{F}(\mathbf{x},\mathbf{y})-Q(\mathbf{x}-\mathbf{x}_0)-B\mathbf{v}(\mathbf{x},\mathbf{y}) \quad 
\forall (\mathbf{x},\mathbf{y}) \in A\;,
\end{equation}
dove:
\begin{align}
&B=[D_y\mathbf{F}(\mathbf{x}_0,\mathbf{y}_0)]^{-1}, & Q=[D_y\mathbf{F}(\mathbf{x}_0,\mathbf{y}_0)]^{-1}[D_x\mathbf{F}(\mathbf{x}_0,\mathbf{y}_0)]\;.
\end{align}
Posti:
\begin{align}
&\mathbf{g}(\mathbf{x})=\mathbf{y}_0-Q(\mathbf{x}-\mathbf{x}_0)\qquad \forall \mathbf{x}\in\mathbb{R}^r\;, \\
&\mathbf{G}(\mathbf{x},\mathbf{y})=B\mathbf{v}(\mathbf{x},\mathbf{y})\qquad \forall (\mathbf{x},\mathbf{y})\in A\;,
\end{align}
si ha che $\mathbf{g}$ è un'applicazione affine di $\mathbb{R}^r$ in $\mathbb{R}^k$, mentre $\mathbf{G} \in C^1(A,\mathbb{R}^k)$ con:
\begin{equation}
|\mathbf{G}(\mathbf{x},\mathbf{y})|_k \le ||B||_{M_k} |\mathbf{v}(\mathbf{x},\mathbf{y})|_k\;,
\end{equation}
ed in particolare $\mathbf{G}$ è nulla in $(\mathbf{x}_0,\mathbf{y}_0)$ con differenziale nullo. Per $(\mathbf{x},\mathbf{y})\in A$ si ha:
\begin{equation}
\mathbf{F}(\mathbf{x},\mathbf{y})=\mathbf{0} \Longleftrightarrow \mathbf{y}=\mathbf{g}(\mathbf{x})-\mathbf{G}(\mathbf{x},\mathbf{y})\;.
\end{equation}
Bisogna trovare un intorno $U \subseteq \mathbb{R}^r$ di $\mathbf{x}_0$ ed un intorno compatto $V \subseteq \mathbb{R}^k$ di $\mathbf{y}_0$ tali che $\forall \mathbf{x} \in U$ l'applicazione:
\begin{equation}
\mathbf{T}_x(\mathbf{y})=\mathbf{g}(\mathbf{x})-\mathbf{G}(\mathbf{x},\mathbf{y})\;,
\end{equation}
trasformi $V$ in $V$ e sia una contrazione. Per il teorema delle contrazioni, seguirà allora che $\forall \mathbf{x} \in U\; \exists!
\mathbf{y}=\mathbf{f}(\mathbf{x}) \in V$ tale che $\mathbf{T}_x(\mathbf{y})=\mathbf{y}$, cioè $\mathbf{F}(\mathbf{x},\mathbf{y})=0$. Per $\rho>0$ siano $V_{\rho}$ la palla di centro $\mathbf{x}_0$ in $\mathbb{R}^r$ e $W_{\rho}$ la palla di centro $\mathbf{y}_0$ in $\mathbb{R}^k$ di raggio $\rho$ ed osserviamo che, essendo $\nabla \mathbf{G}(\mathbf{x}_0,\mathbf{y}_0)=\mathbf{0}$, si ha, posto $(\BF{\xi}_t,\BF{\eta}_t)=((1-t)\mathbf{x}+t\mathbf{x}',(1-t)\mathbf{y}+t\mathbf{y}')$:
\begin{align}
&|\mathbf{G}(\mathbf{x},\mathbf{y})-\mathbf{G}(\mathbf{x}',\mathbf{y}')|_k = \left|\int_0^1 \frac{\diff}{\diff{t}}\mathbf{G}(\BF{\xi}_t,\BF{\eta}_t)\diff{t}\right|_k \notag \\
&\le \left|\int_0^1[(\mathbf{G}_x(\BF{\xi}_t,\BF{\eta}_t-\mathbf{G}_x(\mathbf{x}_0,\mathbf{y}_0))(\mathbf{x}-\mathbf{x}')+(\mathbf{G}_y(\BF{\xi}_t,\BF{\eta}_t)-\mathbf{G}_y(\mathbf{x}_0,\mathbf{y}_0))(\mathbf{y}-\mathbf{y}')]\diff{t}\right|_k\;.
\end{align}
Esiste dunque $\rho_0>0$ tale che :
\begin{equation}
|\mathbf{G}(\mathbf{x},\mathbf{y})-\mathbf{G}(\mathbf{x}',\mathbf{y}')|_k \le \frac{1}{2}(|\mathbf{x}-\mathbf{x}'|_r+|\mathbf{y}-\mathbf{y}'|_k) \qquad \forall \mathbf{x},\mathbf{x}' \in V_{\rho_0},\forall \mathbf{y},\mathbf{y}' \in W_{\rho_0}\;,
\end{equation}
ed in particolare:
\begin{equation}
|\mathbf{G}(\mathbf{x},\mathbf{y})|_k \le \frac{1}{2}(|\mathbf{x}-\mathbf{x}_0|_r+|\mathbf{y}-\mathbf{y}_0|_k)
 \qquad \forall \mathbf{x} \in V_{\rho_0},\forall \mathbf{y} \in W_{\rho_0}\;.
\end{equation}
Fissiamo $\rho_1 \in\; ]0,\rho_0[$. Si osserva che per $\mathbf{x} \in V_{\rho_1}$ l'applicazione $\mathbf{T}_x$ manda $W_{\rho_0}$ in se
stesso, a patto che $\rho_1$ sia sufficientemente piccolo; infatti:
\begin{align}
|\mathbf{T}_x(\mathbf{y}-\mathbf{y}_0|_k &= |\mathbf{g}(\mathbf{x})-\mathbf{G}(\mathbf{x},\mathbf{y})-\mathbf{y}_0|_k \notag \\
&\le ||Q||_{M_{k,r}}||\mathbf{x}-\mathbf{x}_0|_r+\frac{1}{2}(|\mathbf{x}-\mathbf{x}_0|_r+|\mathbf{y}-\mathbf{y}_0|_k) \notag \\
&\le \left(||Q||_{M_{k,r}}+\frac{1}{2}\right)\rho_1+\frac{\rho_0}{2}\le \rho_0\;,
\end{align}
pur di scegliere:
\begin{equation}
\rho_1 \le \frac{\rho_0}{2||Q||_{M_{k,r}}+1}\;.
\end{equation}
Inoltre, per $\mathbf{x} \in V_{\rho_1}$ la $\mathbf{T}_x$ è una contrazione in $W_{\rho_0}$; infatti:
\begin{equation}
|\mathbf{T}_x(\mathbf{y})-\mathbf{T}_x(\mathbf{y}')|_k = |\mathbf{G}(\mathbf{x},\mathbf{y})|_k \le \frac{1}{2}|\mathbf{y}-\mathbf{y}'|_k\qquad \forall \mathbf{y},\mathbf{y}' \in W_{\rho_0}\;.
\end{equation}
Essendo $W_{\rho_0}$ uno spazio metrico con la distanza indotta dalla norma euclidea di $\mathbb{R}^k$, si conclude che $\forall \mathbf{x} \in V_{\rho_1}\; \exists!\;\mathbf{f}(\mathbf{x}) \in W_{\rho_0}$ tale che $\mathbf{T}_x(\mathbf{f}(\mathbf{x}))=\mathbf{f}(\mathbf{x})$, il che significa, per quanto detto, $\mathbf{F}(\mathbf{x},\mathbf{f}(\mathbf{x}))=\mathbf{0}$. Si ha, in particolare, $\mathbf{f}(\mathbf{x}_0)=\mathbf{y}_0$. Abbiamo così costruito la funzione implicita:
\begin{equation}
\mathbf{f}:V_{\rho_1} \to W_{\rho_0}\;,
\end{equation}
che, per il teorema delle contrazioni dipendenti da parametro, è continua. Quindi anche la funzione:
\begin{equation}
\mathbf{x} \mapsto \det[D_y\mathbf{F}(\mathbf{x},\mathbf{f}(\mathbf{x}))]\;,
\end{equation}
è continua in $V_{\rho_1}$; allora, essendo $\det[D_y\mathbf{F}(\mathbf{x}_0,\mathbf{y}_0)]\ne 0$, avremo:
\begin{equation}
\det[D_y\mathbf{F}(\mathbf{x},\mathbf{f}(\mathbf{x}))] \ne 0 \qquad \forall \mathbf{x} \in V_{\rho_1}\;.
\end{equation}
Proviamo che $\mathbf{f}$ è differenziabile in $V_{\rho_1}$. Sia $\mathbf{x}' \in V_{\rho_1}$. Poiché $\mathbf{F}$ è differenziabile in 
$(\mathbf{x}',\mathbf{f}(\mathbf{x}')) \in V_{\rho_1} \times W_{\rho_0}$ si ha:
\begin{align}
\mathbf{F}(\mathbf{x},\mathbf{f}(\mathbf{x}))-\mathbf{F}(\mathbf{x}',\mathbf{f}(\mathbf{x}')) &= D_x\mathbf{F}(\mathbf{x}',\mathbf{f}(\mathbf{x}'))(\mathbf{x}-\mathbf{x}')\notag \\
&+D_y\mathbf{F}(\mathbf{x}',\mathbf{f}(\mathbf{x}'))(\mathbf{f}(\mathbf{x})-\mathbf{f}(\mathbf{x}')+o(\mathbf{x},\mathbf{f}(\mathbf{x})) \notag \\
&=\mathbf{0}\;,
\end{align}
con:
\begin{equation}
\frac{\mathbf{o}(\mathbf{x},\mathbf{y})}{\sqrt{|\mathbf{x}-\mathbf{x}'|_r^2+|\mathbf{y}-\mathbf{f}(\mathbf{x}')|_k^2}} \to \mathbf{0}
\qquad\mbox{per}\qquad \sqrt{|\mathbf{x}-\mathbf{x}'|_r^2+|\mathbf{y}-\mathbf{f}(\mathbf{x}')|_k^2} \to 0\;.
\end{equation}
Dunque si ha, ricavando $\mathbf{f}(\mathbf{x})-\mathbf{f}(\mathbf{x}')$, dividendo per $|\mathbf{x}-\mathbf{x}'|_r$ e facendo il limite
 per $\mathbf{x} \to \mathbf{x}'$:
\begin{equation}
D\mathbf{f}(\mathbf{x}')=-[D_y\mathbf{F}(\mathbf{x}',\mathbf{f}(\mathbf{x}'))]^{-1}\cdot [D_x\mathbf{F}(\mathbf{x}',\mathbf{f}(\mathbf{x}'))] \qquad \forall \mathbf{x}' \in V_{\rho_1}\;.
\end{equation}
\endproof
\section{Funzioni invertibili}
\begin{defn} Una funzione $\mathbf{F}:A\subseteq\mathbb{R}^N\to\mathbb{R}^N$ si dice \textit{localmente invertibile} in $\mathbf{x}_0\in A$ se $\exists U\subseteq A$ intorno di $\mathbf{x}_0$ e $\exists V$ intorno di $\mathbf{F}(\mathbf{x}_0)$ tali che $\mathbf{F}:U\to V$ sia bigettiva.
\end{defn}
\begin{thm}[Invertibilità locale] Siano $\mathbf{F}:A\subseteq\mathbb{R}^N\to\mathbb{R}^N$ di classe $C^1(A)$ e $\mathbf{x}_0\in A$ tale che $\det[D\mathbf{F}(\mathbf{x}_0)]\ne 0$. Allora $\mathbf{F}$ è localmente invertibile in $\mathbf{x}_0$ e la funzione inversa $\mathbf{F}^{-1}$ è di classe $C^1$ in un intorno $V$ di $\mathbf{y}_0=\mathbf{F}(\mathbf{x}_0)$. Si ha inoltre:
\begin{equation}
D\mathbf{F}^{-1}(\mathbf{y})=[D\mathbf{F}(\mathbf{F}^{-1}(\mathbf{y}))]^{-1}\quad \forall \mathbf{y}\in V\;.
\end{equation}
\end{thm}
\proof $\forall (\mathbf{x},\mathbf{y})\in A\times\mathbb{R}^N$ definisco la funzione $\mathbf{G}:A\times\mathbb{R}^N\to\mathbb{R}^N$ data da:
\begin{equation}
\mathbf{G}(\mathbf{x},\mathbf{y})=\mathbf{y}-\mathbf{F}(\mathbf{x})\;.
\end{equation}
Si vede immediatamente che $\mathbf{G}$ è composizione di funzioni di classe $C^1$ e dunque anch'essa sarà di classe $C^1$, e la sua 
matrice Jacobiana sarà data da:
\begin{equation}
D\mathbf{G}(\mathbf{x},\mathbf{y})=(-D\mathbf{F}(\mathbf{x})\;|\;I_N)\;.
\end{equation}
Si osserva che per ipotesi $\mathbf{G}(\mathbf{x}_0,\mathbf{y}_0)=\mathbf{0}$ e inoltre si ha $\det[\mathbf{G}_x(\mathbf{x}_0,
\mathbf{y}_0)]=(-1)^N\det[D\mathbf{F}(\mathbf{x}_0)]\ne 0$ sempre per ipotesi. Allora la funzione $\mathbf{G}$ soddisfa le ipotesi del 
teorema del Dini, pertanto esisteranno $U$ intorno di $\mathbf{x}_0$, $V$ intorno di $\mathbf{y}_0$ e una funzione $\mathbf{g}:U\to V$ di classe $C^1$ tali che:
\begin{equation}
\mathbf{G}(\mathbf{x},\mathbf{y})=\mathbf{0}\qquad \Longleftrightarrow \qquad \mathbf{x}=\mathbf{g}(\mathbf{y})\;.
\end{equation}
che, per definizione di $\mathbf{G}$, equivale a dire:
\begin{equation}
\mathbf{y}=\mathbf{F}(\mathbf{x})\qquad \Longleftrightarrow\qquad \mathbf{x}=\mathbf{g}(\mathbf{y})\;.
\end{equation}
Da questa relazione otteniamo l'identità $\mathbf{y}=\mathbf{F}(\mathbf{g}(\mathbf{y}))$, da cui si deduce che $\mathbf{g}=\mathbf{F}^{-1}$. Abbiamo dunque dimostrato la prima parte del teorema, trovando appunto la funzione inversa $\mathbf{g}$ per la quale, sempre per il teorema del Dini, vale:
\begin{equation}
D\mathbf{g}(\mathbf{y})=-[\mathbf{G}_x(\mathbf{g}(\mathbf{y}),\mathbf{y})]^{-1}\cdot [\mathbf{G}_y(\mathbf{g}(\mathbf{y}),\mathbf{y})]=-[-D\mathbf{F}(\mathbf{g}(\mathbf{y}))]^{-1}\cdot I_N=[D\mathbf{F}(\mathbf{g}(\mathbf{y}))]^{-1}\;.
\end{equation}
Sostituendo $\mathbf{g}=\mathbf{F}^{-1}$ infine, otteniamo:
\begin{equation}
D\mathbf{F}^{-1}(\mathbf{y})=[D\mathbf{F}(\mathbf{F}^{-1}(\mathbf{y}))]^{-1}\;.
\end{equation}
\endproof
\begin{thm}[Rango] Siano $\mathbf{F}:A\subseteq\mathbb{R}^N\to\mathbb{R}^N$, con $N=k+r>k$ e $\mathbf{x}_0\in A$. Se la matrice $D\mathbf{F}(\mathbf{x}_0)$ ha rango massimo $k$, ad esempio:
\begin{equation}
\det \left\{\frac{\partial F_i}{\partial x^j}(\mathbf{x}_0)\right\}_{i,j=1,\ldots,k}\ne 0\;,
\end{equation}
allora $\exists U\subseteq A$ intorno di $\mathbf{x}_0$ ed $\exists V$ intorno di $(F_1(\mathbf{x}_0),\ldots,F_k(\mathbf{x}_0))$ tale che
 $\mathbf{F}(U)$ è grafico di una funzione $\mathbf{h}(y^1,\ldots,y^k):V\to\mathbb{R}^r$ di classe $C^1$. Il piano $k$-dimensionale 
tangente a $\mathbf{F}(U)$ nel punto $(\mathbf{x}_0,\mathbf{h}(\mathbf{y}_0))=\mathbf{F}(\mathbf{x}_0)$ è il piano passante per 
$\mathbf{F}(\mathbf{x}_0)$ generato dai vettori $\partial F_i/\partial x_i$, $i=1,\ldots,k$ di equazione parametrica:
\begin{equation}
\mathbf{u}=\mathbf{F}(\mathbf{x}_0)+D\mathbf{F}(\mathbf{x}_0)\cdot \mathbf{t},\quad \mathbf{t}\in\mathbb{R}^k\;,
\end{equation}
cioè:
\begin{equation}
\mathbf{u}=\mathbf{F}(\mathbf{x}_0)+\sum_{i=1}^k\frac{\partial \mathbf{F}}{\partial x^i}(\mathbf{x}_0)t^i\;.
\end{equation}
\end{thm}
\proof Scriviamo $\mathbf{u}\in\mathbb{R}^N$ come $\mathbf{u}\equiv(\mathbf{y},\mathbf{z}), \mathbf{y}\in\mathbb{R}^k,\mathbf{z}\in\mathbb{R}^r$ e $\mathbf{F}\equiv(\mathbf{f},\mathbf{g}), \mathbf{f}=(F_1,\ldots,F_k), \mathbf{g}=(F_{k+1},\ldots,F_N)$. I punti di $\mathbf{F}(A)$ sono $\mathbf{u}=(\mathbf{y},\mathbf{z})$ con:
\begin{equation}
\begin{cases}
 y^i=f_i(x^1,\ldots,x^k) \\
\\
z^j=g_j(x^{k+1},\ldots,x^N)
\end{cases}\;.
\end{equation}
Per ipotesi, si ha $\det[D\mathbf{f}(\mathbf{x}_0)]\ne 0$, dunque per il teorema di invertibilità locale esisteranno $U$ intorno di $
\mathbf{x}_0$, $V$ intorno di $\mathbf{y}_0$ e $\mathbf{f}^{-1}:V\to U$ di classe $C^1$. Si può dunque scrivere:
\begin{equation}
\begin{cases}
 x^i=(f^{-1})_i(y^1,\ldots,y^k) \\
\\
z^j=g_j(\mathbf{f}^{-1}(y^1,\ldots,y^k))
\end{cases}\;.
\end{equation}
Allora:
\begin{equation}
\mathbf{F}(U)=\left\{(\mathbf{y},\mathbf{z})\in\mathbb{R}^N\;|\;\mathbf{y}\in V, \mathbf{z}=\mathbf{g}(\mathbf{f}^{-1}(\mathbf{y}))\right\}\;.
\end{equation}
Posto $\mathbf{g}\circ\mathbf{f}^{-1}\equiv \mathbf{h}(y^1,\ldots,y^k):V\to\mathbb{R}^r$, l'equazione del piano tangente a $\mathbf{F}(U)$ sarà:
\begin{equation}
\begin{cases}
 \mathbf{y}=\mathbf{y} \\
\\
\mathbf{z}=\mathbf{h}(\mathbf{y}_0)+D\mathbf{h}(\mathbf{y}_0)(\mathbf{y}-\mathbf{y}_0)
\end{cases}\;,
\end{equation}
ovvero:
\begin{equation}
\begin{cases}
\mathbf{y}=\mathbf{y} \\
\\
 \mathbf{z}=\mathbf{z}_0+D\mathbf{g}(\mathbf{f}^{-1}(\mathbf{y}_0))(\mathbf{y}-\mathbf{y}_0)=\mathbf{z}_0+D\mathbf{g}(\mathbf{x}_0)\cdot[D\mathbf{f}(\mathbf{x}_0)]^{-1}(\mathbf{y}-\mathbf{y}_0)
\end{cases}\;.
\end{equation}
In forma vettoriale:
\begin{equation}
\left(\begin{matrix}
       \mathbf{y}-\mathbf{y}_0 \\
\\
\mathbf{z}-\mathbf{z}_0
      \end{matrix}\right)=\left(\begin{matrix}
D\mathbf{f}(\mathbf{x}_0) \\
\\
D\mathbf{g}(\mathbf{x}_0)\end{matrix}\right)\left[D\mathbf{f}^{-1}(\mathbf{x}_0)(\mathbf{y}-\mathbf{y}_0)\right]\;.
\end{equation}
Posti:
\begin{align}
\begin{pmatrix}
\mathbf{y}-\mathbf{y}_0 \\
\\
\mathbf{z}-\mathbf{z}_0
\end{pmatrix}&= \mathbf{u}-\mathbf{F}(\mathbf{x}_0)\;, \notag \\
\begin{pmatrix}
D\mathbf{f}(\mathbf{x}_0) \\
\\
D\mathbf{g}(\mathbf{x}_0
\end{pmatrix}&=D\mathbf{F}(\mathbf{x}_0)\;, \notag \\
D\mathbf{f}^{-1}(\mathbf{x}_0)(\mathbf{y}-\mathbf{y}_0)&=\mathbf{t}\in\mathbb{R}^k\;,
\end{align}
si ottiene:
\begin{equation}
\mathbf{u}=\mathbf{F}(\mathbf{x}_0)+D\mathbf{F}(\mathbf{x}_0)\cdot\mathbf{t},\quad \mathbf{t}\in\mathbb{R}^k\;.
\end{equation}
\endproof

\section{Massimi e minimi vincolati}
\begin{defn} Sia $A \subseteq \mathbb{R}^N$ aperto, sia $f \in C^1(A)$ e sia $K$ una varietà $r$-dimensionale ($r<N$) di classe $C^1$ contenuta in $A$. Un punto $\mathbf{x}_0 \in A$ si dice \textit{punto stazionario} per $f$ su $K$ se $\mathbf{x}_0 \in K$ e il vettore $\nabla f(\mathbf{x}_0)$ è ortogonale all'iperpiano $r$-dimensionale tangente a $K$ in $\mathbf{x}_0$.
\end{defn}
\begin{thm} Sia $A\subseteq \mathbb{R}^N$ aperto, sia $f \in C^1(A)$ e sia $K$ una varietà $r$-dimensionale ($r<N$) di classe $C^1$ contenuta in $A$. Se $\mathbf{x}_0 \in K$ è punto di massimo o di minimo relativo per $f|_K$, allora $\mathbf{x}_0$ è un punto stazionario vincolato per $f$ su $K$.
\end{thm}
\proof Supponiamo che $K$ sia della forma $K=\mathbf{g}(U)$, con $U \subseteq \mathbb{R}^r$ aperto e $\mathbf{g}$ di classe $C^1$ con matrice Jacobiana di rango $r$ per ogni punto di $U$. Sarà in particolare $\mathbf{x}_0=\mathbf{g}(\mathbf{y}_0), \mathbf{y}_0\in U$. Per ipotesi, si ha che $\mathbf{y}_0$ è un punto di massimo o di minimo locale per la funzione composta $F(\mathbf{y})=f(\mathbf{g}(\mathbf{y})), \mathbf{y} \in U$. Quindi deve essere:
\begin{equation}
D_iF(\mathbf{y}_0)=\sum_{j=1}^N D_jf(\mathbf{g}(\mathbf{y}_0))D_ig_j(\mathbf{y}_0)=0 \qquad i=1,\ldots,r\;,
\end{equation}
ossia:
\begin{equation}
\langle \nabla f(\mathbf{x}_0),D_i\mathbf{g}(\mathbf{y}_0)\rangle_N = 0 \qquad i=1,\ldots,r\;.
\end{equation}
Ciò significa che $\nabla f(\mathbf{x}_0)$ è ortogonale ai vettori $D_1\mathbf{g}(\mathbf{y}_0),\ldots,D_r\mathbf{g}(\mathbf{y}_0)$, i 
quali, per il teorema del rango, sono i generatori del piano $r$-dimensionale tangente a $K$ nel punto $\mathbf{x}_0$. Ciò prova che in 
$\mathbf{x}_0$ il vettore $\nabla f(\mathbf{x}_0)$ è ortogonale a $K$, ottenendo così la tesi.
\endproof
\section{Metodo dei moltiplicatori di Lagrange}
\begin{thm} Sia $A \subseteq \mathbb{R}^N$ aperto, sia $f \in C^1(A)$ e sia:
\begin{equation}
K=\{\mathbf{x} \in A\;|\; \mathbf{G}(\mathbf{x})=\mathbf{0}\}\;,
\end{equation}
dove $\mathbf{G}:A \to \mathbb{R}^k$ ($k<N$) è una funzione di classe $C^1$ con matrice Jacobiana $D\mathbf{G}(\mathbf{x})$ di rango 
massimo $k$ $\forall \mathbf{x} \in K$. Allora $\mathbf{x}_0 \in A$ è un punto stazionario vincolato per $f$ su $K$ se e solo se esiste 
$\mathbf{m}_0 \in \mathbb{R}^k$ tale che $(\mathbf{x}_0,\mathbf{m}_0)$ è punto stazionario libero in $A \times \mathbb{R}^k$ per la 
funzione Lagrangiana:
\begin{equation}
L(\mathbf{x},\mathbf{m})=f(\mathbf{x})-\langle \mathbf{m},\mathbf{G}(\mathbf{x})\rangle_k\;.
\end{equation}
\end{thm}
\proof $(\Longrightarrow)$ \\

Nelle ipotesi fatte, posto $r=N-k$, $K$ è una varietà $r$-dimensionale di classe $C^1$, in virtù del teorema del Dini. Sia $\mathbf{x}_0 
\in K$ un punto stazionario vincolato per $f$: allora si ha $\mathbf{G}(\mathbf{x}_0)=\mathbf{0}$ e, per il teorema precedente, il vettore
 $\nabla f(\mathbf{x}_0)$ deve essere ortogonale al piano $r$-dimensionale tangente a $K$ in $\mathbf{x}_0$. Ma, essendo $K$ una curva di livello della funzione $\mathbf{G}$, i vettori normali a $K$ in $\mathbf{x}_0$ sono le righe della matrice Jacobiana $D\mathbf{G}(\mathbf{x}_0)$, ossia i vettori $\nabla G_1(\mathbf{x}_0),\ldots,\nabla G_k(\mathbf{x}_0)$. Quindi $\nabla f(\mathbf{x}_0)$ è combinazione lineare di tali vettori, e dunque esistono $m_1,\ldots,m_k$ (detti \textit{moltiplicatori}) tali che:
\begin{equation}
\nabla f(\mathbf{x}_0)-\sum_{i=1}^k m_i\nabla G_i(\mathbf{x}_0)=\mathbf{0}\;.
\end{equation}
In altre parole, il punto $\mathbf{x}_0$ verifica le condizioni:
\begin{equation}
\begin{cases}
 D_jf(\mathbf{x}_0)-\displaystyle{\sum_{i=1}^k m_iD_jG_i(\mathbf{x}_0)}=0 \quad j=1,\ldots,N \\
 \\
-G_i(\mathbf{x}_0)=0 \quad i=1,\ldots,k
\end{cases}\;,
\end{equation}
le quali equivalgono, per definizione della Lagrangiana $L$ e ponendo $\mathbf{m}_0 \equiv (m_1,\ldots,m_k)$ all'annullarsi del gradiente 
di $L$ in $(\mathbf{x}_0,\mathbf{m}_0)$ rispetto alle coordinate $x_j$ e $m_i$.
\endproof
\proof $(\Longleftarrow)$ \\
Se un punto $(\mathbf{x}_0,\mathbf{m}_0) \in A \times \mathbb{R}^k$ è stazionario per la Lagrangiana, ossia soddisfa il sistema sopra 
scritto, allora il secondo gruppo di equazioni ci dice che $\mathbf{x}_0 \in K$, mentre il primo gruppo esprime la lineare dipendenza di 
$\nabla f(\mathbf{x}_0)$ dai vettori normali a $K$ in $\mathbf{x}_0$. Ciò prova che $\mathbf{x}_0$ è punto stazionario vincolato per $f$ 
su $K$.
\endproof
\chapter{Integrale di Lebesgue}
\section{Compattezza in spazi metrici}
\begin{defn} Sia $(X,d)$ uno spazio metrico. $K \subseteq X$ si dice \textit{compatto} se ogni ricoprimento aperto di $K$, cioè ogni famiglia di aperti $\{U_i\}_{i\in I}$ tale che $\bigcup_{i \in I} U_i \supseteq K$, contiene un sottoricoprimento finito, cioè $\exists i_1,\ldots,
i_m \in I$ tali che:
\begin{equation}
\bigcup_{k=1}^m U_{i_k} \supseteq K\;.
\end{equation}
\end{defn}
\begin{defn} Sia $(X,d)$ uno spazio metrico. $K \subseteq X$ si dice \textit{compatto per successioni} se ogni successione $\{x_n\}_{n \in \mathbb{N}}\subseteq K$ contiene una sottosuccessione che converge ad un elemento $x \in K$.
\end{defn}
\begin{prop} Sia $(X,d)$ uno spazio metrico e $K \subseteq X$. Allora $K$ è compatto se e solo se $K$ è compatto per successioni.
\end{prop}
\proof $(\Longrightarrow)$ \\

Sia $\{x_n\} \subseteq K$ compatto. Sia $S$ l'insieme dei valori assunti da $\{x_n\}$. Si ha $S \subseteq K$. Se $S$ è finito, esisterà 
sicuramente una sottosuccessione costante che dunque converge ad un elemento di $S \subseteq K$. Se invece $S$ è infinito, dico che 
$\exists \overline{x} \in K$ tale che ogni intorno di $\overline{x}$ contiene infiniti punti di $S$. Se per assurdo così non fosse, allora
 $\forall y \in K\; \exists B_y$ tale che $B_y \cap S$ è un insieme finito. Consideriamo $\{B_y\}_{y \in K} \supseteq K$. Per ipotesi di 
compattezza, $\exists y_1,\ldots,y_m \in K$ tali che:
\begin{equation}
\bigcup_{i=1}^m B_{y_i} \supseteq K\;.
\end{equation}
Allora si avrebbe:
\begin{equation}
S \subseteq \bigcup_{i=1}^m (B_{y_i} \cap S)\;.
\end{equation}
Ma $S$ è infinito, mentre $\bigcup_{i=1}^m (B_{y_i}\cap S)$ è finito, quindi l'inclusione è assurda. Allora avremo che:
\begin{equation}
\forall k\; \exists x_{n_k} \in B\left(\overline{x},\frac{1}{k}\right)\;,
\end{equation}
cioè:
\begin{equation}
\lim_{k \to \infty} x_{n_k}=\overline{x} \in K\;.
\end{equation}
\endproof
Per dimostrare la seconda implicazione, ci avvarremo della seguente definizione e del seguente lemma.
\begin{defn} Sia $(X,d)$ uno spazio metrico, $K\subseteq X$ e $\{U_i\}_{i \in I}$ un ricoprimento aperto di $K$. Allora si definisce \textit{numero di Lebesgue} del ricoprimento $u=\{U_i\}_{i \in I}$ la quantità:
\begin{equation}
\epsilon(u)\equiv \begin{cases}
              \sup\{\delta>0\;|\;\forall x \in K\;\exists i \in I:\overline{B(x,\delta)}\subseteq U_i\} \\
              \\
0\quad \mbox{se l'insieme sopra è vuoto}
             \end{cases}\;.
\end{equation}
Si ha evidentemente $\epsilon(u)\ge 0$.
\end{defn}
\begin{oss} Si ha $\epsilon(u)>0$ se e solo se $\exists \delta>0$ tale che $\forall x \in K\; \exists i \in I$ tale che $\overline{B(x,\delta)}
\subseteq U_i$.
\end{oss}
\begin{lem} Sia $K$ un insieme compatto per successioni. Allora per ogni ricoprimento $u=\{U_i\}_{i \in I}$ di $K$ si ha $\epsilon(u)>0$.
\end{lem}
\proof $\forall x \in K$ consideriamo la quantità $\epsilon(x)=\sup\{\delta>0\;|\;\exists i \in I:\overline{B(x,\delta)}\subseteq U_i\}$. Si ha 
$\epsilon(x)>0 \quad \forall x \in K$ e inoltre $\epsilon(u)=\inf_{x \in K} \epsilon(x)$. Infatti, in generale, si ha $\epsilon(u)\le 
\epsilon(x) \quad \forall x \in K$ e in particolare $\epsilon(u)\le \inf_{x\in K} \epsilon(x)$. Inoltre se $\delta$ è un numero reale tale
 che $0<\delta<\inf_{x\in K}\epsilon(x)$, allora si verifica che $\delta \le \epsilon(u)$. Pertanto, non vi è alcun numero reale compreso 
tra $\epsilon(u)$ e $\epsilon(x)$ e dunque $\epsilon(u)=\inf_{x \in K} \epsilon(x)$. Allora bisogna dimostrare che $\epsilon_0=\inf_{x \in
 K} \epsilon(x)>0$. Sappiamo che $\exists \{x_n\} \subseteq K$ tale che $\epsilon(x_n)\to \epsilon_0$. Per ipotesi di compattezza per 
successioni, $\exists \{x_{n_k}\}_{k \in \mathbb{N}} \subseteq \{x_n\}_{n \in \mathbb{N}}$ che converge ad un punto $x^* \in K$. Si ha 
evidentemente $\epsilon(x^*)>0$. Per definizione di limite, $\exists k_0$ tale che $\forall k\ge k_0$ si ha definitivamente:
\begin{equation}
d(x_{n_k},x^*)<\frac{1}{4}\epsilon(x^*)\;.
\end{equation}
Si verifica che:
\begin{equation}
B\left(x_{n_k},\frac{1}{4}\epsilon(x^*)\right)\subseteq B\left(x^*,\frac{1}{2}\epsilon(x^*)\right)\;.
\end{equation}
Infatti, sia $z \in B\left(x_{n_k},\frac{1}{4}\epsilon(x^*)\right)$. Allora:
\begin{equation}
d(z,x^*) \le d(z,x_{n_k})+d(x_{n_k},x^*) < \frac{1}{4}\epsilon(x^*)+\frac{1}{4}\epsilon(x^*)=\frac{1}{2}\epsilon(x^*) \Longrightarrow 
z \in B\left(x^*,\frac{1}{2}\epsilon(x^*)\right)\;.
\end{equation}
Pertanto, $\forall k \ge k_0$ e per un certo $i_0$ segue, per definizione di $\epsilon(x)$:
\begin{equation}
B\left(x^*,\frac{1}{2}\epsilon(x^*)\right) \subseteq U_{i_0}\;,
\end{equation}
e quindi:
\begin{equation}
\frac{1}{4}\epsilon(x^*) \le \epsilon(x_{n_k}) \quad \forall k\ge k_0\;.
\end{equation}
Passando al limite per $k \to \infty$ si ottiene:
\begin{equation}
0 \le \frac{1}{4}\epsilon(x^*)\le \epsilon_0=\inf_{x \in K} \epsilon(x)\;.
\end{equation}
\endproof
\proof $(\Longleftarrow)$ \\

Sia $K$ compatto per successioni. Bisogna dimostrare che $K$ è compatto. Sia $u=\{U_i\}_{i\in I}$ un ricoprimento di $K$ e $\epsilon(u)>0$. Supponiamo per assurdo che $u$ non abbia sottoricoprimenti finiti. Allora $\forall \epsilon \in\;]0,\epsilon(u)[$ il ricoprimento $\{B(x,\epsilon)\}_{x \in K}$ non ha sottoricoprimenti finiti. Fissato $x_1 \in K$, si ha che $B(x_1,\epsilon)$ non ricopre $K$, dunque $\exists x_2 \in K\setminus B(x_1,\epsilon)$. $\{B(x_1,\epsilon),B(x_2,\epsilon)\}$ non ricopre $K$ e dunque $\exists x_3 \in K \setminus\bigcup_{i=1}^2 B(x_i,\epsilon)$. Induttivamente, si costruisce:
\begin{equation}
x_n \in K\setminus \bigcup_{i=1}^{n-1} B(x_i,\epsilon)\;.
\end{equation}
$\{x_n\} \subseteq K$ è una successione che non ha sottosuccessioni convergenti, poiché $d(x_n,x_m)>\epsilon$ se $n>m$. Ma ciò costituisce un assurdo in quanto per ipotesi $K$ è compatto per successioni. Quindi risulta che $\{U_i\}_{i\in I}$ ha un sottoricoprimento finito di $K$ e dunque $K$ è compatto.
\endproof
\section{Misura di Lebesgue}
Sia $\mathfrak{M}_N$ la classe degli insiemi misurabili di $\mathbb{R}^N$, $\mathfrak{M}_N \subset \mathcal{P}(\mathbb{R}^N)$. Definiamo i parallelepipedi $N$-dimensionali $P$ come:
\begin{equation}
P\equiv\prod_{i=1}^N\;]a_i,b_i[\;,
\end{equation}
e denotiamo con $\mathcal{P}_N$ l'insieme dei parallelepipedi $N$-dimensionali.
\begin{defn} Si definisce \textit{volume N-dimensionale} di un parallelepipedo $P \in \mathcal{P}_N$ la quantità:
\begin{equation}
V_N(P)\equiv \prod_{i=1}^N (b_i-a_i)\;,
\end{equation}
con la convenzione che $0\cdot \infty=0$.
\end{defn}
\begin{defn} $\forall E \subseteq \mathbb{R}^N$ si definisce \textit{misura esterna} la quantità:
\begin{equation}
m_N^*(E)=\inf\left\{\sum_{k \in \mathbb{N}} V_N(P_k), P_k \in \mathcal{P}_N\;\mbox{aperti}\;|\;\bigcup_{k \in \mathbb{N}} P_k \supset E
\right\}\;.
\end{equation}
\end{defn}
\begin{defn} Sia $A \subseteq \mathbb{R}^N$. Si definisce \textit{chiusura} di $A$ l'insieme:
\begin{equation}
\overline{A}=A \cup \partial A\;,
\end{equation}
dove $\partial A$ indica la frontiera di $A$.
\end{defn}
\begin{defn} Sia $A \subseteq \mathbb{R}^N$. Si definisce \textit{parte interna} di $A$ l'insieme:
\begin{equation}
\stackrel{\circ}{A}\;=A \setminus \partial A\;.
\end{equation}
\end{defn}
\begin{prop}[Proprietà della misura esterna]
 \begin{enumerate}
  \item $\forall E \subseteq \mathbb{R}^N,\quad m_N^*(E)\ge 0$;
 \item $m_N^*(\emptyset)=0, \quad m_N^*(\{\mathbf{x}\})=0$;
 \item $m_N^*$ è monotona crescente rispetto all'inclusione;
 \item $m_N^*$ è invariante per traslazione;
 \item $m_N^*=V_N$ sulla classe $\mathcal{P}_N$ dei parallelepipedi;
 \item $m_N^*$ è numerabilmente subadditiva, cioè se $\{E_n\}_{n \in \mathbb{N}}\subseteq \mathcal{P}(\mathbb{R}^N)$, allora
\begin{equation}
m_N^*\left(\bigcup_{n=1}^{\infty} E_n\right) \le \sum_{n=1}^{\infty} m_N^*(E_n)\;.
\end{equation}
 \end{enumerate}
\end{prop}
\begin{lem} Sia $P \in \mathcal{P}_N$. Se $j \in \{1,\ldots,N\}$ e $c \in \mathbb{R}$, posti:
\begin{align}
&P_1= P\cap\{x\;|\; x_j\le c\}, &P_2=\{x\;|\;x_j\ge c\}\;,
\end{align}
allora:
\begin{equation}
V_N(P)=V_N(P_1)+V_N(P_2)\;.
\end{equation}
\end{lem}
\proof Sia $P=\prod_{i=1}^N\;]a_i,b_i[$. Sezioniamo $P$ con un piano della forma $x_j=c$, con $a_j<c<b_j$. Si ha $P=P_1 \cup P_2$ dove:
\begin{align}
P_1 &=\;]a_1,b_1[\;\times \cdots \times \;]a_j,c[\;\times \cdots \times \;]a_N,b_N[ \;,\notag \\
P_2 &= \;]a_1,b_1[\;\times \cdots\times \;]c,b_j[\; \times \cdots \times \;]a_N,b_N[\;.
\end{align}
Allora:
\begin{align}
V_N(P) &= \prod_{i=1}^N(b_i-a_i)=(b_j-a_j)\prod_{i\ne j} (b_i-a_i)=(b_j-c+c-a_j)\prod_{i\ne j}(b_i-a_i) \notag \\
&=(c-a_j)\prod_{i\ne j}(b_i-a_i)+(b_j-c)\prod_{i\ne j}(b_i-a_i)=V_N(P_1)+V_N(P_2)\;.
\end{align}
\endproof
\begin{lem} Se $P \in \mathcal{P}_N$, $P=\bigcup_{i=1}^k P_i$, con $P_i \in \mathcal{P}_N$ e $\stackrel{\circ}{P_i} \cap \stackrel{\circ}{P_j}=\{0\}$ allora:
\begin{equation}
V_N=\sum_{i=1}^k V_N(P_i)\;.
\end{equation}
\end{lem}
\proof (proprietà 5) Sia $P \in \mathcal{P}_N$. Distinguiamo tre casi:
\begin{equation}
V_N(P)=\begin{cases}
        0 \\
\in \mathbb{R}^+ \\
\infty
       \end{cases}\;.
\end{equation}
Se $V_N=0$, l'uguaglianza è ovvia. Supponiamo dunque $0<V_N<\infty$ e $P$ aperto. Allora $P$ è un ricoprimento di se stesso e quindi, per definizione di misura esterna si ha $m_N^*(P) \le V_N(P)$. Se $P$ non è aperto, si ha:
\begin{equation}
\prod_{i=1}^N\;]a_i,b_i[\;\subseteq P \subseteq \prod_{i=1}^N [a_i,b_i]\;,
\end{equation}
e inoltre, $\forall \epsilon>0$ si ha:
\begin{equation}
P \subseteq \prod_{i=1}^N \;]a_i-\epsilon,b_i+\epsilon[\;.
\end{equation}
Si tratta di un ricoprimento di $P$, quindi segue:
\begin{equation}
m_N^*(P) \le V_N(P_{\epsilon})=\prod_{i=1}^N (b_i-a_i+2\epsilon) \le V_N(P)+\epsilon C\;,
\end{equation}
dunque anche in questo caso $m_N^*(P)$ è un minorante. Dimostriamo adesso la diseguaglianza inversa. Sia $\{P_k\}$ un ricoprimento di $P$, con $P_k \in \mathcal{P}_N\quad \forall k$. Supponiamo $P$ chiuso e limitato, dunque compatto. Per definizione di compattezza, è 
possibile estrarre un sottoricoprimento finito di $P$ da $\{P_k\}$, cioè $\exists P_{k_1},\ldots,P_{k_m}$ tali che:
\begin{equation}
P \subseteq \bigcup_{i=1}^m P_{k_i} \subseteq \bigcup_{k \in \mathbb{N}} P_k\;.
\end{equation}
Allora:
\begin{equation}
\sum_{i=1}^m V_N(P_{k_i}) \le \sum_{k \in \mathbb{N}} V_N(P_k)\;.
\end{equation}
Consideriamo gli insiemi $Q_{k_i}=P_{k_i} \cap P \in \mathcal{P}_N$. Si ha ovviamente che $\bigcup_{i=1}^m Q_{k_i} = P$ ed inoltre:
\begin{equation}
\sum_{i=1}^m V_N(Q_{k_i})\le \sum_{i=1}^m V_N(P_{k_i})\;.
\end{equation}
Consideriamo le sovrapposizioni dei $Q_{k_i}$ come parallelepipedi a sé stanti. Allora, avendo contato le intersezioni una sola volta, 
l'unione di tutte le divisioni di $V_N(P)$ così create verifica, per il lemma:
\begin{equation}
V_N(P) \le \sum_{i=1}^m V_N(Q_{k_i}) \le \sum_{i=1}^m V_N(P_{k_i}) \le \sum_{k \in \mathbb{N}} V_N(P_k)\;,
\end{equation}
e, in particolare:
\begin{equation}
V_N(P) \le \inf\left\{\sum_{k \in \mathbb{N}} V_N(P_k)\;|\; \bigcup_{k \in \mathbb{N}} P_k \supseteq P\right\}=m_N^*(P)\;.
\end{equation}
Se invece $0<V_N(P)<\infty$ e $P$ non è compatto, allora $\overline{P}$ è compatto e dunque, per quanto visto:
\begin{equation}
V_N(P)=V_N(\overline{P})=m_N^*(\overline{P})=m_N^*(P\cup \partial P) \le m_N^*(P)+m_N^*(\partial P)\;.
\end{equation}
Ma per definizione, $m_N^*(\partial P)=0$, dunque si ottiene:
\begin{equation}
V_N(P) \le m_N^*(P)\;.
\end{equation}
Se $V_N(P)=\infty$, sia $Q_n=[-n,n]^{N}$. Allora:
\begin{equation}
V_N(P \cap Q_n) \le V_N(P)=\infty\;.
\end{equation}
Facendo tendere $n \to \infty$ si ha:
\begin{equation}
\lim_{n \to \infty} V_N(P \cap Q_n)=\infty=V_N(P)\;.
\end{equation}
Consideriamo $P \cap Q_n \in \mathcal{P}_N$. Per ipotesi e per monotonia della misura esterna ($P\cap Q_n \subseteq P$) si ha:
\begin{equation}
V_N(P \cap Q_n) \le m_N^*(P \cap Q_n) \le m_N^*(P)\;.
\end{equation}
Facendo tendere $n \to \infty$, per il teorema del confronto, si ha $m_N^*(P) \to \infty$ e dunque $m_N^*(P)=V_N(P)$.
\endproof
\proof (Proprietà 6) \\

$\forall n$ sia $E_n \subseteq \mathbb{R}^N$, con $E=\bigcup_{n \in \mathbb{N}} E_n$. Sappiamo che $\forall \epsilon$ e $\forall n,\;
\exists\{P_{kn}\}$ ricoprimento aperto di $E_n$ tale che:
\begin{equation}
\sum_{n=1}^{\infty} V_N(P_{kn}) < m_N^*(E_n)+\frac{\epsilon}{2^n}\;.
\end{equation}
Poiché $E \subseteq \bigcup_{k,n \in \mathbb{N}} P_{kn}$, segue che:
\begin{equation}
m_N^*(E) \le \sum_{k,n=1}^{\infty} V_N(P_{kn})=\sum_{n=1}^{\infty}\left(\sum_{k=1}^{\infty} V_N(P_{kn})\right) \le \sum_{n=1}^{\infty} 
m_N^*(E_n)+\epsilon\sum_{n=1}^{\infty} \frac{1}{2^n}\;.
\end{equation}
La seconda serie converge a $1$, dunque si ottiene:
\begin{equation}
m_N^*(E) \le \sum_{n=1}^{\infty} m_N^*(E_n)+\epsilon\;.
\end{equation}
\endproof
\subsection{Classe dei misurabili}
\begin{defn} Si definisce \emph{classe dei misurabili} l'insieme:
\begin{equation}
\mathfrak{M}_N\equiv\left\{E \subseteq \mathbb{R}^N\;|\; m_N^*(A)=m_N^*(A\cap E)+m_N^*(A\cap E^c), \forall A \subseteq
 \mathbb{R}^N\right\}\;.
\end{equation}
\end{defn}
\begin{oss} $\mathfrak{M}_N \ne \{0\}$.
\end{oss}
\begin{oss} Per dimostrare che $E \in \mathfrak{M}_N$, è sufficiente dimostrale la diseguaglianza $\ge$, in quanto la diseguaglianza $\le$ discende immediatamente dalla proprietà $(6)$ della misura esterna.
\end{oss}
\begin{oss} Se $m_N^*(E)=0$, allora $E \in \mathfrak{M}_N$.
\end{oss}
\proof Sia $A \in \mathbb{R}^N$. Poiché $A \cap E^c \subseteq A$, per monotonia della misura esterna segue che $m_N^*(A\cap E^c)\le m_N^*(A)$. Inoltre, poiché $A\cap E \subseteq E$, si ha $0 \le m_N^*(A \cap E) \le m_N^*(E)=0$ da cui segue $m_N^*(A \cap E)=0$. Allora si ottiene:
\begin{equation}
m_N*(A \cap E^c)+m_N^*(A \cap E) \le m_N^*(A)+m_N^*(E)= m_N^*(A)\;.
\end{equation}
\endproof
\begin{prop} $\mathfrak{M}_N$ è una $\sigma$-algebra, cioè è chiusa per unione e passaggio al complementare.
\end{prop}
\begin{lem} Siano $E,F \in \mathfrak{M}_N$. Allora $E \cup F \in \mathfrak{M}_N$.
\end{lem}
\proof Sia $A \subseteq \mathbb{R}^N$. Poiché $E \in \mathfrak{M}_N$, si ha:
\begin{equation}
m_N^*(A)=m_N^*(A \cap E) + m_N^*(A \cap E^c)\;.
\end{equation}
Usiamo l'insieme $A \cap E^c$ come test per $F$:
\begin{equation}
m_N^*(A)=m_N^*(A \cap E)+m_N^*((A\cap E^c)\cap F)+m_N^*((A\cap E^c)\cap F^c)\;.
\end{equation}
Valgono le relazioni $A\cap E^c \cap F=A \cap (F\setminus E)$, $A\cap E^c \cap F^c=A\cap(E\cup F)^c$ e $E\cup F\setminus E=E\cup F$, da 
cui $A\cap(E\cup(F\setminus E))=A \cap (E\cup F)$. Dunque, per subadditività:
\begin{align}
m_N^*(A) &\ge m_N^*((A\cap E)\cup(A\cap F\setminus E))+m_N^*(A\cap(E\cup F)^c) \notag \\
&=m_N^*(A\cap(E\cup F\setminus E))+m_N^*(A\cap(E\cup F)^c) \notag \\
&=m_N^*(A\cap(E\cup F))+m_N^*(A\cap(E\cup F)^c)\;,
\end{align}
da cui segue che $E\cup F \in \mathfrak{M}_N$.
\endproof
\begin{cor} Siano $E,F \in \mathfrak{M}_N$. Allora $E\setminus F, E\cap F \in \mathfrak{M}_N$.
\end{cor}
\proof Se $E,F \in \mathfrak{M}_N$, allora per definizione $E^c,F^c \in \mathfrak{M}_N$. Poiché $\mathfrak{M}_N$ è chiusa per unione e passaggio al complementare, si ha:
\begin{equation}
\left(E^c\cup F^c\right)^{c}=E\cap F \in \mathfrak{M}_N\;.
\end{equation}
Poiché $E \setminus F=E \cap F^c$, per quanto visto si ha immediatamente che $E \setminus F \in \mathfrak{M}_N$.
\endproof
\begin{lem}[Finita additività dei disgiunti] Siano $E_1,\ldots,E_n \in \mathfrak{M}_N$ disgiunti. Allora:
\begin{equation}
m_N^*\left(A\cap \bigcup_{i=1}^n E_i\right)=\sum_{i=1}^n m_N^*(A\cap E_i)\;.
\end{equation}
\end{lem}
\proof (per induzione su $n$) \\

Se $n=1$, è banalmente vero. Dimostriamo che $n \Longrightarrow n+1$. Consideriamo $E_1,\ldots,E_{n+1} \in \mathfrak{M}_N$ disgiunti e applichiamo la definizione dei misurabili a $E_{n+1}$ prendendo come test $A \cap \bigcup_{i=1}^{n+1} E_i$:
\begin{equation}
m_N^*\left(A\cap \bigcup_{i=1}^{n+1} E_i\right)=m_N^*\left(A\cap \bigcup_{i=1}^{n+1} E_i \cap E_{n+1}\right)+m_N^*\left(A\cap \bigcup_{i=1}^{n+1} E_i \cap E_{n+1}^c\right)\;.
\end{equation}
Si ha $\bigcup_{i=1}^{n+1} E_i \cap E_{n+1}=E_{n+1}$ e $\bigcup_{i=1}^{n+1} E_i \cap E_{n+1}^c=\bigcup_{i=1}^n E_i$, dunque si ottiene:
\begin{equation}
m_N^*\left(A\cap\bigcup_{i=1}^{n+1} E_i\right)= m_N^*(A\cap E_{n+1})+m_N^*\left(A\cap \bigcup_{i=1}^n E_i\right)\;.
\end{equation}
Applicando l'ipotesi induttiva al secondo addendo, segue che:
\begin{equation}
m_N^*\left(A\cap\bigcup_{i=1}^{n+1} E_i\right)=m_N^*(A\cap E_{n+1})+\sum_{i=1}^n m_N^*(A\cap E_i)=\sum_{i=1}^{n+1} m_N^*(A\cap E_i)\;.
\end{equation}
\endproof
\begin{prop} Sia $\{E_n\} \subseteq \mathfrak{M}_N$ e $E=\bigcup_{n \in \mathbb{N}} E_n$. Allora $E \in \mathfrak{M}_N$.
\end{prop}
\proof $\forall n$ definisco la successione $\{F_n\}$ nel seguente modo:
\begin{equation}
\begin{cases}
 F_0=E_0 \\
 \\
F_{n+1}=E_{n+1}\setminus \bigcup_{k=0}^n F_k
\end{cases}\;.
\end{equation}
$\forall n$ si ha $\bigcup_{k=0}^n F_k=\bigcup_{k=0}^n E_k$. Inoltre gli $F_k$ sono disgiunti e numerabili. Posto $E=\bigcup_{k=0}^{\infty
} F_k$ si ha:
\begin{equation}
m_N^*(A)=m_N^*\left(A\cap \bigcup_{k=0}^n F_k\right)+m_N^*\left(A\cap \left(\bigcup_{k=0}^n F_k\right)^{c}\right)\;.
\end{equation}
Usando il lemma precedente, il secondo membro diventa:
\begin{align}
m_N^*(A) &= \sum_{k=0}^n m_N^*(A\cap F_k)+m_N^*\left(A\cap\bigcap_{k=0}^n F_k^c\right) \notag \\
&\ge \sum_{k=0}^{\infty} m_N^*(A\cap F_k)+m_N^*\left(A\cap\bigcap_{k=0}^{\infty}F_k^c\right) \notag \\
&=\sum_{k=0}^{\infty} m_N^*(A\cap F_k)+m_N^*(A\cap E^c)\ge m_N^*(A\cap E)+m_N^*(A\cap E^c)\;.
\end{align}
Dove l'ultima minorazione segue dalla proprietà di subadditività della misura esterna.
\endproof
\begin{prop}[Numerabile additività su disgiunti misurabili] Se $\{E_n\} \in \mathfrak{M}_N$ sono disgiunti, allora:
\begin{equation}
m_N^*\left(\bigcup_{n \in \mathbb{N}} E_n\right) = \sum_{n \in \mathbb{N}} m_N^*(E_n)\;.
\end{equation}
\end{prop}
\proof La diseguaglianza $\le$ discende immediatamente dalla proprietà di subadditività della misura esterna. Sappiamo, dalla proposizione precedente che $\forall n$ si ha:
\begin{equation}
m_N^*\left(\bigcup_{i=1}^n E_i\right)=\sum_{i=1}^n m_N^*(E_i)\;.
\end{equation}
Per la monotonia della misura esterna, si ha:
\begin{equation}
m_N^*\left(\bigcup_{i=1}^{\infty} E_i \right) \ge m_N^*\left(\bigcup_{i=1}^n E_i\right)=\sum_{i=1}^n m_N^*(E_i)\;,
\end{equation}
per $n \to \infty$, si ottiene:
\begin{equation}
m_N^*\left(\bigcup_{i=1}^{\infty} E_i \right) \ge \sum_{i=1}^{\infty} m_N^*(E_i)\;.
\end{equation}
\endproof
\begin{prop} Se $P \in \mathcal{P}_N$, allora $P \in \mathfrak{M}_N$.
\end{prop}
\proof Possiamo supporre $P$ aperto. Sia $A \subseteq \mathbb{R}^N$, dimostriamo che $m_N^*(A)\ge m_N^*(A\cap P)+m_N^*(A\cap P^c)$. Fissato $\epsilon>0$, $\exists\{P_n\} \subseteq \mathcal{P}_N, P_n$ aperti, che ricopre $A$ tale che:
\begin{equation}
\sum_{n \in \mathbb{N}} V_N(P_n) \le m_N^*(A)+\epsilon\quad\mbox{cioè}\quad m_N^*(A)\ge -\epsilon+\sum_{n \in \mathbb{N}} V_N(P_n)\;.
\end{equation}
Osserviamo che $\forall n, P_n \cap P$ è un parallelepipedo aperto, $P_n \setminus \overline{P}$ è unione finita aperta di parallelepipedi.
Sia $P_n\setminus \overline{P}=\bigcup_{j=1}^{h_n} R_{jn}$, con $R_{jn} \in \mathcal{P}_N$ privi di punti interni comuni. Allora:
\begin{equation}
V_N(P)=V_N(P_n\cap P)+\sum_{j=1}^{h_n} V_N(R_{jn})\;,
\end{equation}
e dunque:
\begin{equation}
m_N^*(A) \ge -\epsilon+\sum_{n \in \mathbb{N}}\left[V_N(P_n\cap P)+\sum_{j=1}^{h_n}V_N(R_{jn})\right]\;.
\end{equation}
Per definizione di misura esterna, $m_N^*(A\cap P) \le \sum_{n \in \mathbb{N}} V_N(P_n \cap P)$, pertanto:
\begin{equation}
m_N^*(A)\ge -\epsilon +m_N*(A\cap P)+\sum_{n \in \mathbb{N}}\sum_{j=1}^{h_n} V_N(R_{jn})\;.
\end{equation}
Si ha inoltre $A \setminus \overline{P} \subseteq \bigcup_{n \in \mathbb{N}} (P_n\setminus \overline{P})=\bigcup_{n \in \mathbb{N}}
\bigcup_{j=1}^{h_n} R_{jn}$, dunque per monotonia, $m_N^*(A \setminus \overline{P}) \le m_N^*\left(\bigcup_{n \in \mathbb{N}}(P_n\setminus\overline{P}\right)=m_N^*\left(\bigcup_{n\in \mathbb{N}}\bigcup_{j=1}^{h_n} R_{jn}\right)\le$ (subadditività) $\le \sum_{n \in \mathbb{N}}m_N^*\left(\bigcup_{j=1}^{h_n} R_{jn}\right)=$ (finita additività dei disgiunti) $=\sum_{n \in \mathbb{N}}\sum_{j=1}^{h_n} m_N^*(R_{jn})=$ (proprietà 5 della misura esterna) $=\sum_{n \in\mathbb{N}}\sum_{j=1}^{h_n} V_N(R_{jn})$. Dunque si ottiene:
\begin{equation}
m_N^*(A) \ge -\epsilon +m_N^*(A\cap P)+m_N^*(A\setminus \overline{P})\;.
\end{equation}
Ma $A\setminus \overline{P} \subseteq A\setminus P=(A\setminus \overline{P})\cup (A\cap \partial P)$, da cui: $m_N^*(A\setminus\overline{P})\le m_N^*(A\setminus P) \le m_N^*(A\setminus \overline{P})+m_N^*(A\cap \partial P)$. Poiché $m_N^*(A\cap \partial P)=0$, si ottiene $m_N^*(A\setminus P)=m_N^*(A \setminus \overline{P})$ e quindi:
\begin{equation}
m_N^*(A) \ge -\epsilon+m_N^*(A\cap P)+m_N^*(A\setminus P)=-\epsilon+m_N^*(A\cap P)+m_N^*(A\cap P^c)\;.
\end{equation}
\endproof
\begin{defn} Sia $E \subseteq \mathbb{R}^N$. Si definisce \textit{distanza} di un punto $\mathbf{x}\in \mathbb{R}^N$ da $E$ la quantità:
\begin{equation}
d(\mathbf{x},E):=\inf\{|\mathbf{x}-\mathbf{y}|_N\;|\;\mathbf{y} \in E\}\;.
\end{equation}
Vale la proprietà:
\begin{equation}
|d(\mathbf{x},E)-d(\mathbf{x}',E)|\le |\mathbf{x}-\mathbf{x}'|_N\;.
\end{equation}
Ciò implica che $d$ è continua.
\end{defn}
\begin{prop}[Misurabilità degli aperti] $\forall A \subseteq \mathbb{R}^N$ aperto $\exists\{P_n\} \subseteq \mathcal{P}_N$ tale che $A=\bigcup_{n \in \mathbb{N}} P_n$ e quindi $A \in \mathfrak{M}_N$.
\end{prop}
\proof $\forall n \in \mathbb{N}$ definiamo l'insieme $K_n$ nel seguente modo:
\begin{equation}
K_n \equiv\begin{cases}
      \left\{\mathbf{x} \in A\;|\; d(\mathbf{x},\partial A)\ge \dfrac{1}{n}\right\}\quad A\;\mbox{limitato} \\
\\
K_n \cap \overline{B(\mathbf{0},n)}\quad A\;\mbox{non limitato}
     \end{cases}\;.
\end{equation}
Si ha $\forall n, K_n\subseteq K_{n+1}\subseteq A$ e $\bigcup_{n \in \mathbb{N}} K_n=A$. Evidentemente, $\forall n\; \exists Q\left(
\mathbf{x},1/n\right)$ cubo aperto inscritto in $B\left(\mathbf{x},1/n\right)$ tale che:
\begin{equation}
K_n \subseteq \bigcup_{\mathbf{x}\in K_n} Q\left(\mathbf{x},\frac{1}{n}\right)\;,
\end{equation}
dunque abbiamo un ricoprimento del compatto $K_n$. Per definizione di compattezza, $\exists Q_1^{(n)},\ldots,Q_{h_n}^{(n)} \subseteq 
\left\{B\left(\mathbf{x},\frac{1}{n}\right)\right\}$ tali che:
\begin{equation}
K_n \subseteq \bigcup_{i=1}^{h_n} Q_i^{(n)}\;.
\end{equation}
Da ciò segue che:
\begin{equation}
A \subseteq \bigcup_{n=1}^{\infty}\bigcup_{i=1}^{h_n} Q_i^{(n)} \subseteq A \Longrightarrow A=\bigcup_{n=1}^{\infty}\bigcup_{i=1}^{h_n} 
Q_i^{(n)}\;.
\end{equation}
\endproof
\begin{defn} Si definisce $\mathfrak{B}_N$ la $\sigma$-algebra dei boreliani, cioè la minima $\sigma$-algebra che contiene gli aperti, o 
equivalentemente la $\sigma$-algebra generata dagli aperti.
\end{defn}
\begin{defn} Sia $X$ un insieme e $\mathfrak{M}\subseteq \mathcal{P}(X)$. $\mathfrak{M}$ si definisce $\sigma$-algebra se:
\begin{enumerate}
 \item $\emptyset,X \in \mathfrak{M}$;
 \item $E \in \mathfrak{M} \Longrightarrow E^c \in \mathfrak{M}$;
 \item $\{E_n\}_{n \in \mathbb{N}} \subseteq \mathfrak{M} \Longrightarrow \bigcup_{n \in \mathbb{N}}E_n \in \mathfrak{M}$.
\end{enumerate}
\end{defn}
\begin{prop} Se $\mathfrak{M}_i$ è una $\sigma$-algebra, $\mathfrak{M}_i\subseteq \mathcal{P}(X), i \in I$, allora $\bigcap_{i \in I} \mathfrak{M}_i$ è ancora una $\sigma$-algebra.
\end{prop}
\begin{oss} Sia $\{\mathcal{C}\subseteq\mathcal{P}(\mathbb{R}^N)\;|\; \mathcal{C}\;\mbox{è una}\;\sigma\mbox{-algebra che contiene gli aperti}\}$. Allora:
\begin{equation}
\mathfrak{M}_N\cap \mathcal{C}=\mathfrak{B}_N\;,
\end{equation}
e dunque $\mathfrak{B}_N \subseteq \mathfrak{M}_N$ (in particolare, $\mathfrak{B}_N \subset \mathfrak{M}_N$).
\end{oss}
\begin{defn}[Insieme ternario di Cantor]
\begin{equation}
C_3:=[0,1]\setminus \bigcup_{n=1}^{\infty}\bigcup_{j=1}^{2^{n-1}} I_{jn}\;,
\end{equation}
dove $l(I_{jn})=\frac{1}{3^n}$.
\end{defn}
\begin{oss} Poiché $[0,1]\setminus C_3$ è l'unione di intervalli aperti e disgiunti, si ha:
\begin{equation}
m_1^*([0,1]\setminus C_3)=\sum_{n=1}^{\infty}\sum_{j=1}^{2^{n-1}} \frac{1}{3^n}=\sum_{n=1}^{\infty}\frac{2^{n-1}}{3^n}=\frac{1}{2}
\sum_{n=1}^{\infty}\left(\frac{2}{3}\right)^n=\frac{1}{2}\cdot\frac{2/3}{1-2/3}=1\;,
\end{equation}
da cui segue $m_1^*(C_3)=0$. In generale, $\forall \lambda \in\;]0,1[$:
\begin{equation}
C_{1/\lambda}=[0,1]\setminus \bigcup_{n=1}^{\infty}\bigcup_{j=1}^{2^{n-1}} I_{jn}\;,
\end{equation}
con $l(I_{jn})=\lambda^n$. Quindi:
\begin{align}
m_1^*([0,1]\setminus C_{1/\lambda}) &= \frac{\lambda}{1-2\lambda}< 1\;, \notag \\
m_1^*(C_{1/\lambda})&=1-\frac{\lambda}{1-2\lambda}\;.
\end{align}
\end{oss}
\begin{oss} $C_3$ può essere definito anche nel seguente modo:
\begin{equation}
C_3:=\{x \in [0,1]\;|\;\mbox{esiste uno sviluppo ternario in cui non figura la cifra}\;1\}\;.
\end{equation}
\end{oss}
\begin{oss} Sia $f \in \mathcal{R}(a,b)$ (Riemann-integrabile), $f>0$. Fissato $n$, definiamo:
\begin{align}
I_i &=\left[a+i\frac{b-a}{n},a+(i+1)\frac{b-a}{n}\right],\qquad i=1,\ldots, n-1\;, \notag \\
P_i^n &= \left[a+i\frac{b-a}{n},a+(i+1)\frac{b-a}{n}\right]\times \left[0,\inf_{x\in I_i}f(x)\right]\;.
\end{align}
Allora:
\begin{equation}
\int_a^b f(x)\;\diff{x}=m_2^*\left(\bigcup_{n=1}^{\infty}\bigcup_{i=0}^{n-1} P_i^n\right)=m_2^*(\{(x,y)\;|\;x \in [a,b], 0\le y\le f(x)\})\;.
\end{equation}
Il grafico $\Gamma$ di $f$ dato da $\Gamma_f=\{(x,y)\;|\;x\in [a,b],y=f(x)\}$ è misurabile, e si ha $m_2^*(\Gamma_f)=0$.
\end{oss}
\begin{oss} Sia $E\subseteq \mathbb{R}^N$ aperto e $t\in \mathbb{R}$. Definiamo $tE:=\{t\mathbf{x}\;|\;\mathbf{x}\in E\}$. Allora si ha:
$m_N^*(tE)=|t|^Nm_N^*(E)$.
\end{oss}
\begin{defn}[Misura di Lebesgue] Si definisce \textit{misura di Lebesgue} la quantità:
\begin{equation}
m_N\equiv m_N^*|_{\mathfrak{M}_N}\;,
\end{equation}
cioè la misura esterna ristretta alla classe dei misurabili.
\end{defn}
\begin{prop} Siano $E,F \in \mathfrak{M}_N$. Allora:
\begin{equation}
m_N(E\cup F)+m_N(E\cap F)=m_N(E)+m_N(F)\;.
\end{equation}
\end{prop}
\proof $E\cup F=(E\cap F)\cup(E\setminus F)\cup(F\setminus E)$. Per l'additività sui disgiunti misurabili:
\begin{align}
m_N(E\cup F) &= m_N(E\cap F)+m_N(E\setminus F)+m_N(F\setminus E) \notag \\
&= m_N(E\cap F)+m_N(E\setminus(E\cap F))+m_N(F\setminus(E\cap F))\;.
\end{align}
Per l'additività, $m_N(E\cap F)+m_N(E\setminus(E\cap F))=m_N((E\cap F)\cup(E\setminus(E\cap F)))=m_N(E)$. Allora:
\begin{equation}
m_N(E\cup F)=m_N(E)+m_N(F\setminus(E\cap F))\;.
\end{equation}
Aggiungendo ad ambo i membri la quantità $m_N(E\cap F)$ si ha:
\begin{equation}
m_N(E\cup F)+m_N(E\cap F)=m_N(E)+m_N(F\setminus(E\cap F))+m_N(E\cap F)\;.
\end{equation}
Applicando il medesimo ragionamento precedente agli ultimi due addendi del secondo membro, si ottiene:
\begin{equation}
m_N(E\cup F)+m_N(E\cap F)=m_N(E)+m_N(F)\;.
\end{equation}
\endproof
\begin{prop} Sia $\{E_n\}_{n \in \mathbb{N}} \subseteq \mathfrak{M}_N$. Allora:
\begin{enumerate}
 \item $E_n \subseteq E_{n+1} \quad \forall n \Longrightarrow \lim_{n\to \infty} m_N(E_n)=m_N\left(\bigcup_{n \in \mathbb{N}} E_n\right)$;
 \item $E_n \supseteq E_{n+1} \quad \forall n$ e $\exists n_0$ tale che $m_N(E_{n_0})<\infty$ allora $\lim_{n\to \infty} m_N(E_n)=
m_N\left(\bigcap_{n\in \mathbb{N}} E_n\right)$.
\end{enumerate}
\end{prop}
\proof $(1)$ \\

Costruiamo una successione $\{F_n\}$ data da:
\begin{equation}
\begin{cases}
 F_0=E_0 \\
\\
F_n=E_n\setminus E_{n-1}
\end{cases}\;.
\end{equation}
Si ha che $E_n=\bigcup_{k=0}^n F_k$, $\bigcup_{n \in \mathbb{N}} E_n=\bigcup_{n \in \mathbb{N}} F_n$ e gli $F_k$ sono disgiunti. Allora, 
per la numerabile additività dei disgiunti:
\begin{equation}
m_N(E_n)=\sum_{k=0}^n m_N(F_k)\;,
\end{equation}
che per $n \to \infty$ diventa:
\begin{equation}
m_N(E_n)=\sum_{n=0}^{\infty} m_N(F_n)=m_N\left(\bigcup_{n\in \mathbb{N}} F_n \right)=m_N\left(\bigcup_{n\in\mathbb{N}} E_n\right)\;.
\end{equation}
\endproof
\proof $(2)$ \\

Evidentemente, si ha:
\begin{equation}
\bigcap_{n\in \mathbb{N}} E_n=\bigcap_{n=n_0}^{\infty} E_n\;.
\end{equation}
Consideriamo la successione $\{E_{n_0}\setminus E_n\}_{n \in \mathbb{N}}$. Essa è monotona crescente per inclusione, quindi per la 
dimostrazione precedente, si ha:
\begin{equation}
\lim_{n \to \infty} m_N(E_{n_0}\setminus E_n)=m_N\left(\bigcup_{n=n_0}^{\infty} (E_{n_0}\setminus E_n)\right)=m_N\left(E_{n_0}\setminus
\bigcap_{n=n_0}^{\infty} E_n \right)\;.
\end{equation}
Ricordando che, se $A\supseteq B$, allora $m_N(A\setminus B)=m_N(A)-m_N(B)$, si ottiene:
\begin{equation}
m_N(E_{n_0})-\lim_{n\to \infty} m_N(E_n)=m_N(E_{n_0})-m_N\left(\bigcap_{n=n_0}^{\infty} E_n\right)\;,
\end{equation}
da cui segue:
\begin{equation}
\lim_{n\to\infty}m_N(E_n)=m_N\left(\bigcap_{n=n_0}^{\infty} E_n\right)=m_N\left(\bigcap_{n\in\mathbb{N}} E_n\right)\;.
\end{equation}
\endproof
\section{Funzioni misurabili}
\begin{prop} Sia $f:D\to \overline{\mathbb{R}}, D\in \mathfrak{M}_N$. Sono fatti equivalenti:
\begin{enumerate}
 \item $\{x \in D\;|\; f(x)>\alpha\}\in \mathfrak{M}_N \qquad \forall \alpha \in \mathbb{R}$;
 \item $\{x \in D\;|\; f(x)\ge\alpha\}\in \mathfrak{M}_N \qquad \forall \alpha \in \mathbb{R}$;
 \item $\{x \in D\;|\; f(x)<\alpha\}\in \mathfrak{M}_N \qquad \forall \alpha \in \mathbb{R}$;
 \item $\{x \in D\;|\; f(x)\le\alpha\}\in \mathfrak{M}_N \qquad \forall \alpha \in \mathbb{R}$.
\end{enumerate}
\end{prop}
\proof $(1) \Longrightarrow (2)$ \\

Si ha:
\begin{equation}
\{f \ge \alpha\}=\bigcap_{n\in \mathbb{N}} \left\{f>\alpha-\frac{1}{n}\right\}\;.
\end{equation}
Dato che per ipotesi $\{f>\alpha-1/n\}\in \mathfrak{M}_N \;\forall n$, per le proprietà di $\sigma$-algebra, si ha che $\bigcap_{n\in
\mathbb{N}}\{f>\alpha-1/n\}\in\mathfrak{M}_N$, e quindi si ottiene la tesi.
\endproof
\proof $(2) \Longrightarrow (3)$ \\

Si ha che:
\begin{equation}
\{f<\alpha\}=\{f\ge \alpha\}^c\;.
\end{equation}
Poiché $\{f\ge \alpha\} \in \mathfrak{M}_N$ per ipotesi, e $\mathfrak{M}_N$ è chiusa per passaggio al complementare, si conclude che $\{f<\alpha\}\in\mathfrak{M}_N$.
\endproof
\proof $(3) \Longrightarrow (4)$ \\

Si dimostra come $(1) \Longrightarrow (2)$, ponendo $\{f\le\alpha\}=\bigcap_{n\in\mathbb{N}}\{f<\alpha+1/n\}$.
\endproof
\proof $(4) \Longrightarrow (1)$ \\

Si dimostra come $(2) \Longrightarrow (3)$, osservando che $\{f>\alpha\}=\{f\le \alpha\}^c$.
\endproof
\begin{defn} Una funzione $f$ si dice \textit{misurabile} se vale una delle suddette proprietà (e quindi tutte).
\end{defn}
\begin{prop} Se $f$ è una funzione continua, allora è misurabile.
\end{prop}
\proof Si ha $\{f>\alpha\}=f^{-1}(]\alpha,+\infty[)$ che è un aperto, e quindi è misurabile.
\endproof
\begin{prop} La funzione indicatrice di un insieme $D$ è misurabile se e solo se $D$ è misurabile.
\end{prop}
\proof La funzione indicatrice di un insieme $D$ è definita da:
\begin{equation}
I_D(x)=\begin{cases}
        1,\qquad x \in D\\
\\
0,\qquad x \not\in D
       \end{cases}\;.
\end{equation}
Quindi:
\begin{equation}
\{I_D(x)>\alpha\}= \left\{ \begin{matrix}
\emptyset &\alpha\ge 1 \\
\\
D & 0\le \alpha<1 \\
\\
\mathbb{R}^N & \alpha<0
\end{matrix}
\right.\;.
\end{equation}
$\emptyset$ e $\mathbb{R}^N$ sono sempre misurabili, mentre nel secondo caso $\{I_D(x)>\alpha\}$ è misurabile se e solo se $D$ è 
misurabile.
\endproof
\begin{prop} Le funzioni semplici sono misurabili.
\end{prop}
\proof Sia $\varphi$ una funzione semplice data da:
\begin{equation}
\varphi(x)=\sum_{i=1}^p \alpha_i I_{D_i}(x), \quad D_i \in \mathfrak{M}_N\; \forall i\;.
\end{equation}
Se $\varphi$ assume i valori distinti $\lambda_1,\ldots,\lambda_p$, posto, $\forall i \; B_i=\{\varphi=\lambda_i\}$, si ha che i $B_i$ sono
misurabili e disgiunti e la rappresentazione canonica di $\varphi$ è:
\begin{equation}
\varphi(x)=\sum_{i=1}^p \lambda_i I_{B_i}(x)\;.
\end{equation}
\endproof
\begin{prop} Siano $f,g:D\to\overline{R}$ due funzioni misurabili. Allora:
\begin{enumerate}
 \item $\lambda f$ è misurabile $\forall \lambda \in \mathbb{R}$;
 \item $f+g$ è misurabile purché sia ben definita;
 \item $f\cdot g$ è misurabile.
\end{enumerate}
\end{prop}
\proof $(1)$ ovvia. \\

\proof $(2)$ \\

\begin{equation}
\{f+g<\alpha\}=\{f=-\infty\}\cup\{g=-\infty\}\cup\{-\infty<f+g<\alpha\}\;.
\end{equation}
Si osserva che $\{f=-\infty\}=\bigcap_{n\in\mathbb{N}}\{f<-n\}$ e quindi è misurabile. Vale lo stesso discorso per $\{g=-\infty\}$. Resta 
da dimostrare che $\{-\infty<f+g<\alpha\}$ è misurabile:
\begin{align}
\{-\infty<f+g<\alpha\} &=\{-\infty<f<\alpha-g<+\infty\} \notag \\
&=\bigcup_{r\in\mathbb{Q}}\{-\infty<f<r<\alpha-g<+\infty\} \notag \\
&=\bigcup_{r\in\mathbb{Q}} [\{-\infty<f<r\}\cap\{-\infty<g<\alpha+r\}]\;.
\end{align}
Entrambi sono insiemi misurabili, dunque la loro intersezione sarà misurabile ed infine l'unione della loro intersezione sarà ancora 
misurabile.
\endproof
\proof $(3)$ \\

Si ha:
\begin{equation}
fg=\frac{1}{2}\left(f^2+g^2-(f-g)^{2}\right)\;.
\end{equation}
Dimostriamo che se $f$ è misurabile, anche $f^2$ lo è (e allo stesso modo per $g^2$):
\begin{equation}
\{f^2>\alpha\}=\begin{cases}
                \mathbb{R}^N, \quad \alpha<0 \\
\\
\{f<-\sqrt{\alpha}\}\cup\{f>\sqrt{\alpha}\}, \quad \alpha \ge 0
               \end{cases}\;.
\end{equation}
Poiché $\mathbb{R}^N$ è misurabile in quanto aperto e $f$ è misurabile per ipotesi, si ha che tutti gli insiemi che definiscono $f^2$ sono
 misurabili. Questo implica che $f^2$ è misurabile. Allo stesso modo, si deduce che $g^2$ è misurabile. Infine, per la dimostrazione $(2)$, si ha che $f-g$ è misurabile e, per quanto appena visto, anche $(f-g)^{2}$ sarà misurabile. Dunque si avrà che $fg$ è misurabile.
\endproof
\begin{prop} Se $f$ è misurabile, $f\ne 0$, allora $1/f$ è misurabile.
\end{prop}
\proof 
\begin{equation}
\left\{\frac{1}{f}>\alpha\right\}=\begin{cases}
                                   \left\{f<\frac{1}{\alpha}\right\}, \quad \alpha>0 \\
                                   \\
\{f>0\}, \quad \alpha=0 \\
\\
\{f>0\}\cup\{f<0\}\cup\left\{f<\frac{1}{\alpha}\right\}, \quad \alpha<0
                                  \end{cases}\;.
\end{equation}
Per quanto visto, sono tutti insiemi misurabili.
\endproof
\begin{prop} Se $\{f_n\}_{n\in\mathbb{N}}$ è una sottosuccessione di funzioni misurabili, allora le successioni:
\begin{align}
&\sup_{n\in\mathbb{N}} f_n(x), &\inf_{n\in\mathbb{N}} f_n(x)\;,
\end{align}
sono misurabili.
\end{prop}
\proof 
\begin{align}
\left\{\sup_{n\in\mathbb{N}} f_n(x)>\alpha\right\} &= \bigcup_{n\in\mathbb{N}} \{f_n>\alpha\}\;, \notag \\
\left\{\inf_{n\in\mathbb{N}} f_n(x)<\alpha\right\} &= \bigcup_{n\in\mathbb{N}} \{f_n<\alpha\}\;.
\end{align}
Entrambi sono unioni numerabili di insiemi misurabili, dunque saranno anch'essi misurabili.
\endproof
\begin{defn} Sia $\{f_n\}_{n\in\mathbb{N}}$ una successione di funzioni misurabili; si definisce \textit{massimo limite} della successione $\{f_n\}$, e si indica con $\max\lim_{n\to\infty}$ oppure $\limsup_{n\to\infty}$ la quantità:
\begin{equation}
\limsup_{n\to\infty} f_n(x)=\inf_{n\in\mathbb{N}}\sup_{m\ge n} f_m(x)\;.
\end{equation}
\end{defn}
\begin{defn} Si definisce \textit{minimo limite} della successione $\{f_n\}$, e si indica con $\min\lim_{n\to\infty}$ oppure $\liminf_{n\to\infty}$ la quantità:
\begin{equation}
\liminf_{n\to \infty} f_n(x)=\sup_{n\in\mathbb{N}}\inf_{m\ge n} f_m(x)\;.
\end{equation}
\end{defn}
\begin{prop} Sia $\{f_n\}_{n\in\mathbb{N}}$ una successione di funzioni misurabili tali che:
\begin{equation}
\exists \lim_{n\to\infty} f_n(x)=f(x)\;.
\end{equation}
Allora $f(x)$ è misurabile.
\end{prop}
\proof Poiché $\inf$ e $\sup$ sono misurabili, anche la loro unione, cioè il limite, sarà misurabile.
\endproof
\begin{prop} $f:D\to\overline{\mathbb{R}}$ è misurabile se e solo se esiste una successione di funzioni semplici $\{\varphi_n\}$ misurabili tale che $\varphi_n(x) \to f(x)$ per $n\to\infty, \forall x \in D$.
\end{prop}
\proof $(\Longleftarrow)$ già vista.
\proof $(\Longrightarrow)$ \\

Costruiamo la successione $\{\varphi_n\}$ nel seguente modo:
\begin{equation}
\varphi_n(x)=\begin{cases}
              n,\qquad f(x)>n \\
0,\qquad f(x)=0 \\
\dfrac{k-1}{2^n},\qquad \dfrac{k-1}{2^n}<f(x)\le \dfrac{k}{2^n},\quad k=1,\ldots,n2^n \\
\dfrac{k}{2^n},\qquad \dfrac{k-1}{2^n}\le f(x)<\dfrac{k}{2^n},\quad k=0,-1,\ldots,-n2^{n+1} \\
-n, \qquad f(x)<-n
             \end{cases}\;.
\end{equation}
$\varphi_n$ è evidentemente semplice. Se $f(x)=\pm \infty$, allora $\varphi_n(x)=\pm n$ che per $n\to \infty$ tende a $f(x)$. Se $f(x)\in
\mathbb{R}$, per $|f(x)|<n$ si ha:
\begin{equation}
|\varphi_n-f(x)|<\frac{1}{2^n}\;.
\end{equation}
Si ha inoltre $|\varphi_n(x)|\le|\varphi_{n+1}(x)|\le|f(x)|$, cioè $\varphi_n$ cresce comunque in modulo. Dunque la distanza $|\varphi_n-
f(x)|$ tende a zero e di conseguenza $\varphi_n(x)\to f(x)$.
\endproof
\section{Integrale di Lebesgue per funzioni semplici}
\begin{defn} Sia $\varphi=\sum_{i=1}^p \alpha_iI_{A_i}$ una funzione semplice espressa in forma canonica. Allora:
\begin{equation}
\int_{\mathbb{R}^N} \varphi\;\diff{x}:=\sum_{i=1}^p \alpha_i m_N(A_i)\;.
\end{equation}
\end{defn}
\begin{prop}[Monotonia dell'integrale] Siano $\varphi,\psi$ due funzioni semplici tali che $\varphi\le\psi$. Allora:
\begin{equation}
\int_{\mathbb{R}^N} \varphi\;\diff{x}\le \int_{\mathbb{R}^N} \psi\;\diff{x}\;.
\end{equation}
\end{prop}
\proof Siano:
\begin{align}
&\varphi=\sum_{i=1}^p\alpha_i I_{A_i}, & A_0=\mathbb{R}^N\setminus\bigcup_{i=1}^pA_i\;, \\
&\psi=\sum_{j=1}^q \beta_j I_{B_j}, & B_0=\mathbb{R}^N\setminus\bigcup_{j=1}^q B_j\;,
\end{align}
con $\alpha_0,\beta_0=0$. Sia $x \in A_i\cap B_j$, allora $\alpha_i\le\varphi\le\psi\le \beta_j$. Dunque:
\begin{align}
\int_{\mathbb{R}^N}\varphi\;\diff{x} &=\sum_{i=0}^p\alpha_i m_N(A_i)=\sum_{i=0}^p\alpha_i\sum_{j=0}^q m_N(A_i\cap B_j) \notag \\
&=\sum_{j=0}^q\sum_{i=0}^p\alpha_i m_N(A_i\cap B_j)\le \sum_{j=0}^q\sum_{i=0}^p\beta_jm_N(A_i\cap B_j) \notag \\
&=\sum_{j=0}^q\beta_j\sum_{i=0}^p m_N(A_i\cap B_j)=\sum_{j=0}^q\beta_jm_N(B_j)=\int_{\mathbb{R}^N}\psi\;\diff{x}\;.
\end{align}
\endproof
\begin{prop}[Linearità dell'integrale] Siano $\varphi,\psi$ due funzioni semplici. Allora, $\forall \lambda,\mu\in\mathbb{R}$:
\begin{equation}
\int_{\mathbb{R}^N}(\lambda\varphi+\mu\psi)\diff{x}=\lambda\int_{\mathbb{R}^N}\varphi\;dx+\mu\int_{\mathbb{R}^N}\psi\;\diff{x}\;.
\end{equation}
\end{prop}
\proof 
\begin{align}
\int_{\mathbb{R}^N}(\lambda\varphi+\mu\psi)\diff{x} &=\sum_{i,j}(\lambda\alpha_i+\mu\beta_j)m_N(A_i\cap B_j) \notag \\
&=\sum_{i,j}\lambda\alpha_im_N(A_i\cap B_j)+\sum_{i,j}\mu\beta_j m_N(A_i\cap B_j) \notag \\
&=\lambda\sum_{i=0}^p\alpha_i\sum_{j=0}^q m_N(A_i\cap B_j)+\mu\sum_{j=0}^q\beta_j\sum_{i=0}^p m_N(A_i\cap B_j) \notag \\
&=\lambda\sum_{i=0}^p\alpha_i m_N(A_i)+\mu\sum_{j=0}^q\beta_j m_N(B_j) \notag \\
&=\lambda\int_{\mathbb{R}^N}\varphi\;\diff{x}+\mu\int_{\mathbb{R}^N}\psi\;\diff{x}\;.
\end{align}
\endproof
\section{Integrale di Lebesgue per funzioni misurabili}
\begin{defn} Sia $f:\mathbb{R}^N\to\overline{\mathbb{R}}$ una funzione misurabile non negativa. Allora:
\begin{equation}
\int_{\mathbb{R}^N} f\;\diff{\mathbf{x}}\equiv \sup\left\{\int_{\mathbb{R}^N} \varphi\;\diff{\mathbf{x}}\;|\;\varphi\in S, 0\le\varphi\le f\right\}\;,
\end{equation}
dove $S$ è l'insieme delle funzioni semplici.
\end{defn}
\begin{defn}  Se $f$ cambia segno, consideriamo:
\begin{align}
&f^+=\max\{f(\mathbf{x}),0\}, &f^-=-\min\{f(\mathbf{x}),0\}\;.
\end{align}
$f^+,f^-$ sono entrambe positive e soddisfano le seguenti relazioni:
\begin{align}
&f(\mathbf{x})=f^+(\mathbf{x})-f^-(\mathbf{x}), & |f(\mathbf{x})|=f^+(\mathbf{x})+f^-(\mathbf{x})\;.
\end{align}
Allora $f$ si dice \textit{integrabile} se almeno uno tra:
\begin{align}
&\int_{\mathbb{R}^N} f^+\;\diff{\mathbf{x}}, &\int_{\mathbb{R}^N} f^-\;\diff{\mathbf{x}}\;,
\end{align}
esiste finito. In tal caso, si ha:
\begin{equation}
\int_{\mathbb{R}^N} f\;\diff{\mathbf{x}}=\int_{\mathbb{R}^N} f^+\;\diff{\mathbf{x}} -\int_{\mathbb{R}^N} f^-\;\diff{\mathbf{x}}\;.
\end{equation}
\end{defn}
\begin{defn} Una funzione $f$ si dice \textit{sommabile} su $\mathbb{R}^N$ se:
\begin{equation}
\int_{\mathbb{R}^N} f\;\diff{\mathbf{x}} \in \overline{\mathbb{R}}\;.
\end{equation}
\end{defn}
\begin{defn} Una funzione $f:\mathbb{R}^N\to\overline{\mathbb{R}}$ misurabile è integrabile su $E\subseteq\mathbb{R}^N$ se $f\cdot I_E$ è integrabile su $\mathbb{R}^N$. In tal caso:
\begin{equation}
\int_E f\;\diff{\mathbf{x}}=\int_{\mathbb{R}^N} fI_E\;\diff{\mathbf{x}}\;.
\end{equation}
\end{defn}
\begin{defn} Una funzione $f:D\to\overline{\mathbb{R}}$ misurabile, definita su $D\subseteq\mathbb{R}^N$ misurabile è integrabile su ogni sottoinsieme $E\subseteq D$ misurabile se la funzione $f^*$ data da:
\begin{equation}
f^*(\mathbf{x})=\begin{cases}
                 f(\mathbf{x}),\quad \mathbf{x}\in D \\
\\
0, \quad \mathbf{x}\not\in D
                \end{cases}\;,
\end{equation}
è integrabile su $E$ nel senso della definizione precedente.
\end{defn}
\begin{prop} Sia $f$ integrabile su $E\in \mathfrak{M}_N$. Sia $\{E_n\}_{n\in\mathbb{N}}\subseteq \mathfrak{M}_N$ una successione di insiemi disgiunti tali che $E=\bigcup_{n\in\mathbb{N}} E_n$. Allora:
\begin{equation}
\int_E f\;\diff{\mathbf{x}}=\sum_{n=0}^{\infty} \int_{E_n} f\;\diff{\mathbf{x}}\;.
\end{equation}
\end{prop}
\proof ($f=I_A$ con $A$ qualsiasi) \\

Si ha in questo caso:
\begin{equation}
\int_E I_A\;\diff{\mathbf{x}}=m_N(A\cap E)=\sum_{n=0}^{\infty} m_N(A\cap E_n)=\sum_{n=0}^{\infty}\int_{E_n} I_A\;\diff{\mathbf{x}}\;.
\end{equation}
\endproof
\proof ($f$ semplice) \\

In questo caso, $f=\sum_{i=1}^p \alpha_1 I_{A_i}$ e dunque:
\begin{equation}
\int_E f\;\diff{\mathbf{x}}=\sum_{i=1}^p \alpha_i m_N(A_i\cap E)=\sum_{n=0}^{\infty}\sum_{i=1}^p \alpha_i m_N(A_i\cap E_n)=\sum_{n=0}^{\infty}\int_{E_n} f\;\diff{\mathbf{x}}\;.
\end{equation}
\endproof
\proof ($f$ misurabile, non negativa) \\

$\forall n$ e $\forall \epsilon>0$ esiste $\varphi_n$ semplice, $0\le\varphi_n\le fI_{E_n}$ tale che:
\begin{equation}
\int_{E_n} \varphi_n\;\diff{\mathbf{x}}>\int_{E_n} f\;\diff{\mathbf{x}} - \frac{\epsilon}{2^n}\;.
\end{equation}
Sommando su $n$, si ottiene:
\begin{equation}
\sum_{n=1}^{\infty} \int_{E_n}\varphi_n\;\diff{\mathbf{x}}>\left(\sum_{n=1}^{\infty}\int_{E_n} f\;\diff{\mathbf{x}}\right)-\epsilon\;.
\end{equation}
Consideriamo adesso la successione crescente $\psi_m=\sum_{k=1}^m \varphi_k$. Si ha $0\le \psi_m\le fI_E$. Allora:
\begin{equation}
\int_E f\;\diff{\mathbf{x}}\ge \int_E \psi_m\;\diff{\mathbf{x}}=\sum_{n=1}^m \int_E \varphi_n\;\diff{\mathbf{x}}>\sum_{n=1}^m\int_{E_n} \varphi_n\;\diff{\mathbf{x}}>\sum_{n=1}^m\int_{E_n}f\;\diff{\mathbf{x}}-\epsilon\;.
\end{equation}
Per $m\to\infty$ si ottiene:
\begin{equation}
\int_E f\;\diff{\mathbf{x}}\ge \left(\sum_{n=1}^{\infty}\int_{E_n} f\;\diff{\mathbf{x}}\right)-\epsilon\;,
\end{equation}
che prova la diseguaglianza $\ge$. Per provare la diseguaglianza $\le$, osserviamo che $\forall \epsilon>0$ esiste $\varphi$ semplice, con
 $0\le \varphi\le fI_E$ tale che:
\begin{equation}
\int_{\mathbb{R}^N} \varphi\;\diff{\mathbf{x}}>\int_E f\;\diff{\mathbf{x}}-\epsilon\;.
\end{equation}
Allora:
\begin{equation}
\sum_{n=1}^{\infty}\int_{E_n} f\;\diff{\mathbf{x}}\ge \sum_{n=1}^{\infty} \int_{E_n}\varphi\;\diff{\mathbf{x}}=\int_E \varphi\;\diff{\mathbf{x}}=\int_{\mathbb{R}^N}\varphi\;\diff{\mathbf{x}}>\int_E f\;\diff{\mathbf{x}}-\epsilon\;.
\end{equation}
\endproof
\proof ($f$ cambia segno) \\

Consideriamo $f^+$ e $f^-$, entrambe positive, a cui applico il risultato della precedente dimostrazione, ottenendo due relazioni:
\begin{align}
\int_E f^+\;\diff{\mathbf{x}} &=\sum_{n=1}^{\infty}\int_{E_n} f^+\;\diff{\mathbf{x}}\;, \notag \\
\int_E f^-\;\diff{\mathbf{x}} &=\sum_{n=1}^{\infty}\int_{E_n} f^-\;\diff{\mathbf{x}}\;.
\end{align}
Sottraendole membro a membro si trova:
\begin{equation}
\int_{E} (f^+-f^-)\;\diff{\mathbf{x}}=\sum_{n=1}^{\infty}\int_{E_n}(f^+-f^-)\;\diff{\mathbf{x}}\;,
\end{equation}
cioè:
\begin{equation}
\int_E f\;\diff{\mathbf{x}}=\sum_{n=1}^{\infty} \int_{E_n} f\;\diff{\mathbf{x}}\;.
\end{equation}
\endproof
\begin{prop} Sia $f$ non negativa, definita su un insieme $D$ misurabile. Sia $\{D_n\}_{n\in\mathbb{N}}$ una successione di insiemi misurabili contenuta in $D$ tale che $\bigcup_{n=1}^{\infty} D_n=D$. Allora si ha:
\begin{equation}
\int_D f\;\diff{\mathbf{x}}\le \sum_{n\in\mathbb{N}} \int_{D_n}f\;\diff{\mathbf{x}}\;.
\end{equation}
\end{prop}
\proof Se i $D_n$ sono disgiunti, la tesi segue dalla proposizione precedente. Altrimenti, costruiamo la successione $\{F_n\}$ data da:
\begin{equation}
\begin{cases}
 F_0=D_0 \\
\\
F_n=D_n\setminus \bigcup_{k=0}^{n-1} D_k
\end{cases}\;.
\end{equation}
Gli $F_k$ sono misurabili e disgiunti e si ha ancora $\bigcup_{n\in\mathbb{N}} F_n=D$. Essendo $f\ge 0$, possiamo scrivere:
\begin{equation}
\int_D f\;\diff{\mathbf{x}}=\sum_{n=0}^{\infty}\int_{F_n}f\;\diff{\mathbf{x}}\le \sum_{n=0}^{\infty} \int_{D_n} f\;\diff{\mathbf{x}}\;.
\end{equation}
\endproof
\begin{prop} Siano $f,g$ integrabili definite su $D\subseteq \mathbb{R}^N$ misurabile. Allora:
\begin{enumerate}
 \item $f\le g\quad \Longrightarrow\quad \int_D f\;\diff{\mathbf{x}}\le\int_D g\;\diff{\mathbf{x}}$;
 \item $\int_D \alpha f\;\diff{\mathbf{x}}=\alpha\int_D f\;\diff{\mathbf{x}},\quad \forall \alpha \in \mathbb{R}$;
 \item se gli integrali non risultano infiniti di segno opposto, si ha $\int_D (f+g)\;\diff{\mathbf{x}}=\int_D f\;\diff{\mathbf{x}}+\int_D g\;\diff{\mathbf{x}}$.
\end{enumerate}
\end{prop}
\proof $(1)$ \\

$f\le g$ implica $f^+\le g^+$ e $f^-\ge g^-$, quindi:
\begin{align}
&\int_D f^+\;\diff{\mathbf{x}}\le \int_D g^+\;\diff{\mathbf{x}}\;, \notag \\
&\int_D f^-\;\diff{\mathbf{x}}\ge \int_D g^-\;\diff{\mathbf{x}}\quad \implies \quad -\int_D f^-\;\diff{\mathbf{x}}\le -\int_D g^-\;\diff{\mathbf{x}}\;.
\end{align}
Sommando le relazioni membro a membro si ottiene:
\begin{equation}
\int_D (f^+-f^-)\;\diff{\mathbf{x}}\le \int_D (g^+-g^-)\;\diff{\mathbf{x}}\quad \Longleftrightarrow\quad  \int_D f\;\diff{\mathbf{x}}\le\int_D g\;\diff{\mathbf{x}}\;.
\end{equation}
\endproof
\proof $(2)$ \\

Sia $\alpha\ge 0$, allora $(\alpha f)^+=\alpha f^+, (\alpha f)^-=\alpha f^-$, e quindi si ottiene la tesi per tutte le funzioni. Sia $
\alpha=-1$. Da $(-f)^+=f^-$ e $(-f)^-=f^+$ segue:
\begin{equation}
\int_D (-f)\;\diff{\mathbf{x}}=\int_D f^-\;\diff{\mathbf{x}}-\int_D f^+\;\diff{\mathbf{x}}=-\int_D (f^+-f^-)\;\diff{\mathbf{x}}=-\int_D f\;\diff{\mathbf{x}}\;.
\end{equation}
Se $\alpha\le 0$, combino i due casi precedenti.
\endproof
\proof $(3)$ \\

$(a)$ Se $f,g\ge 0$, si ha che $f+g$ è integrabile. Siano $\varphi,\psi\in S_0$ con $0\le\varphi\le fI_D$ e $0\le\psi\le gI_D$; allora 
$0\le \varphi+\psi\le (f+g)I_D$ e dunque:
\begin{equation}
\int_D \varphi\;\diff{\mathbf{x}}+\int_D \psi\;\diff{\mathbf{x}}=\int_D(\varphi+\psi)\;\diff{\mathbf{x}}\le \int_D(f+g)\;\diff{\mathbf{x}}\;.
\end{equation}
Per l'arbitrarietà di $\varphi,\psi$ si ottiene:
\begin{equation}
\int_D f\;\diff{\mathbf{x}}+\int_D g\;\diff{\mathbf{x}}\le \int_D(f+g)\;\diff{\mathbf{x}}\;.
\end{equation}
Per provare la diseguaglianza opposta, introduciamo gli insiemi misurabili:
\begin{align}
D^m &=\{\mathbf{x}\in D\;|\; m\le f(\mathbf{x})+g(\mathbf{x})\le m+1\},\quad m\in\mathbb{N}^+\;, \notag \\
D^{\infty} &= \{\mathbf{x}\in D\;|\; f(\mathbf{x})+g(\mathbf{x})=+\infty\}\;, \notag \\
D_m &=\left\{\mathbf{x}\in D\;|\; \frac{1}{m+1}\le f(\mathbf{x})+g(\mathbf{x})\le \frac{1}{m}\right\},\quad m\in\mathbb{N}^+\;, \notag \\
D_{\infty} &=\{\mathbf{x}\in D\;|\; f(\mathbf{x})+g(\mathbf{x})=0\}\;.
\end{align}
Fissato $\beta \in\;]0,1[$, scegliamo $\varphi\in S_0$ tale che $0\le \varphi\le \beta(f+g)I_D$. Siano poi $\{\varphi_n\}$ e $\{\psi_n\}$
 due successioni di funzioni semplici convergenti puntualmente a $fI_D$ e $gI_D$ rispettivamente. Poiché:
 \begin{align}
 &(f+g)-\varphi \ge (1-\beta)(f+g)\ge (1-\beta)m\quad\mbox{in}\; D^m\;, \notag \\
 &(f+g)-\varphi \ge (1-\beta)(f+g)\ge \frac{1-\beta}{m+1}\quad\mbox{in}\; D_m\;, \notag \\
&\varphi<f+g=+\infty\quad \mbox{in}\; D^{\infty}\;, \notag \\
&\varphi=f+g=0\quad \mbox{in}\; D_{\infty}\;, 
 \end{align}
allora $\forall m\in \mathbb{N}^+\cup\{\infty\}$ esiste $\nu_m\in \mathbb{N}$ tale che $\forall n>\nu_m$ si ha:
\begin{align}
&\varphi\le (f+g)-(1-\beta)m\le\varphi_n+\psi_n\le f+g\quad\mbox{in}\; D^m\;, \notag \\
&\varphi\le (f+g)-\frac{1-\beta}{m+1}\le \varphi_n+\psi_n\le f+g\quad \mbox{in}\; D_m\;, \notag \\
&\varphi\le \varphi_n+\psi_n\le +\infty=f+g\quad \mbox{in}\; D^{\infty}\;, \notag \\
&\varphi=\varphi_n+\psi_n=0=f+g\quad \mbox{in}\;D_{\infty}\;.
\end{align}
Posto $E=\{\varphi>0\}$, si ha $m_N(E)<\infty$. Inoltre, $\forall n\ge \nu_m$:
\begin{align}
&\varphi\le \varphi_nI_E+\psi_nI_E\le f+g\quad \mbox{in}\; D^m\;, \notag \\
&\varphi\le \varphi_nI_E+\psi_nI_E\le f+g\quad \mbox{in}\; D_m\;, \notag \\
&\varphi\le \varphi_nI_E+\psi_nI_E\le +\infty=f+g\quad \mbox{in}\; D^{\infty}\;, \notag \\
&\varphi=\varphi_nI_E+\psi_nI_E=0=f+g\quad \mbox{in}\; D_{\infty}\;.
\end{align}
Essendo $0\le \varphi_nI_E\le fI_D$ e $0\le \psi_nI_E\le gI_D$, $\forall n>\nu_m$ si ha:
\begin{align}
\int_{D^m}\varphi\;\diff{\mathbf{x}} &\le \int_{D^m}(\varphi_n+\psi_n)\diff{\mathbf{x}}=\int_{\mathbb{R}^N}(\varphi_n+\psi_n)I_{D^m\cap E}\;\diff{\mathbf{x}} \notag \\
&=\int_{\mathbb{R}^N}\varphi_nI_{D^m\cap E}\;\diff{\mathbf{x}}+\int_{\mathbb{R}^N}\psi_nI_{D^m\cap E}\le \int_{D^m}f\;\diff{\mathbf{x}}+\int_{D^m}g\;\diff{\mathbf{x}}\;,
\end{align}
e similmente:
\begin{equation}
\int_{D_m} \varphi\;\diff{\mathbf{x}}\le \int_{D_m}f\;\diff{\mathbf{x}}+\int_{D_m}g\;\diff{\mathbf{x}}\;.
\end{equation}
Dato che $D=\left(\bigcup_{m\in\mathbb{N}^+} D^m\right)\cup \left(\bigcup_{m\in\mathbb{N}^+}D_m\right)\cup D^{\infty}\cup D_{\infty}$, sommando su $m$ si trova:
\begin{equation}
\int_{\mathbb{R}^N} \varphi\;\diff{\mathbf{x}}=\int_D \varphi\;\diff{\mathbf{x}}\le \int_D f\;\diff{\mathbf{x}}+\int_D g\;\diff{\mathbf{x}}\;.
\end{equation}
Per l'arbitrarietà di $\varphi$ e $\psi$, si ottiene:
\begin{equation}
\beta\int_D (f+g)\;\diff{\mathbf{x}} \le \int_D f\;\diff{\mathbf{x}}+\int_D g\;\diff{\mathbf{x}}\;,
\end{equation}
per $\beta \to 1$, otteniamo la tesi. \\

$(b)$ Se $f,g \le 0$, la dimostrazione in tal caso è speculare a quella del caso $(a)$. \\

$(c)$ Se $f\ge0,g\le0$, definiamo gli insiemi misurabili:
\begin{align}
&S^+=\{(f+g)I_D>0\}, &S^-=\{(f+g)I_D<0\}\;,
\end{align}
gli integrali di $f+g$ in $S^+$ e $S^-$ sono ben definiti. Per quanto visto, si ha:
\begin{align}
\int_{S^+}f\;\diff{\mathbf{x}} &=\int_{\mathbb{R}^N}fI_{S^+}\;\diff{\mathbf{x}}=\int_{\mathbb{R}^N}[(f+g)I_{S^+}+(-g)I_{S^+}]\;\diff{\mathbf{x}} \notag \\
&=\int_{\mathbb{R}^N}(f+g)I_{S^+}\;\diff{\mathbf{x}}+\int_{\mathbb{R}^N}(-g)I_{S^+}\;\diff{\mathbf{x}}\notag \\
&=\int_{S^+}(f+g)\;\diff{\mathbf{x}}-\int_{S^+} g\;\diff{\mathbf{x}}\;,
\end{align}
e analogamente:
\begin{equation}
\int_{S^-}g\;\diff{\mathbf{x}}=\int_{S^-}(f+g)\;\diff{\mathbf{x}}-\int_{S^-}f\;\diff{\mathbf{x}}\;.
\end{equation}
Per ipotesi, non risulta mai $\int_D f\;\diff{\mathbf{x}}=-\int_D g\;\diff{\mathbf{x}}=+\infty$ e dunque almeno uno tra i due deve essere finito. Se ne deduce:
\begin{equation}
\int_{S^+}f\;\diff{\mathbf{x}}+\int_{S^+}g\;\diff{\mathbf{x}}=\int_{S^+}(f+g)\;\diff{\mathbf{x}}=\int_D(f+g)^+\;\diff{\mathbf{x}}\;,
\end{equation}
e analogamente:
\begin{equation}
\int_{S^-}f\;\diff{\mathbf{x}}+\int_{S^-}g\;\diff{\mathbf{x}}=\int_{S^-}(f+g)\;\diff{\mathbf{x}}=\int_D(f+g)^-\;\diff{\mathbf{x}}\;.
\end{equation}
Sommando le due relazioni, si ottiene la tesi. \\

$(d)$ Se $f,g$ sono di segno qualunque, poniamo:
\begin{align}
&F^+=\{fI_D\ge 0\}, &F^-=\{fI_D\le 0\}\;, \notag \\
&G^+=\{gI_D\ge 0\}, &G^-=\{gI_D\le 0\}\;.
\end{align}
Allora $F^+\cap G^+,F^+\cap G^-,F^-\cap G^+,F^-\cap G^-$ sono insiemi misurabili, la cui unione è $D$ e su ognuno di essi la tesi è 
vera in virtù dei tre punti precedenti. Sommando le quattro diseguaglianze si ottiene la tesi per $D$.
\endproof
\begin{defn} Sia $D$ un sottoinsieme misurabile di $\mathbb{R}^N$. Si dice che una proprietà $p(\mathbf{x})$ è vera \textit{quasi ovunque} in $D$ (abbreviato q.o. in $D$) se, posto $P=\{\mathbf{x}\in D\;|\;p(\mathbf{x})\}$, l'insieme $D\setminus P$ è misurabile, con $m_N(D\setminus P)=0$.
\end{defn}
\begin{thm}[Beppo Levi] Sia $D$ un sottoinsieme misurabile di $\mathbb{R}^N$ e sia $\{f_n\}_{n\in\mathbb{N}}$ una successione di funzioni misurabili definite su $D$, tali che $0\le f_n\le f_{n+1}$ q.o. in $D$. Allora il limite puntuale:
\begin{equation}
f(\mathbf{x})=\lim_{n\to\infty} f_n(\mathbf{x})\;,
\end{equation}
esiste q.o. in $D$ e si ha:
\begin{equation}
\lim_{n\to\infty}\int_D f_n\;\diff{\mathbf{x}}=\int_D f\;\diff{\mathbf{x}}\;.
\end{equation}
\end{thm}
\proof Posti $P_n=\{f_n<0\}$ e $Q_n=\{f_{n+1}<f_n\}$, si ha che gli insiemi $P_n,Q_n$ hanno misura nulla per ipotesi, quindi anche $P=\bigcup_{n\in\mathbb{N}} (P_n\cup Q_n)$ ha misura nulla e di conseguenza il limte puntuale $f$ è ben definito e non negativo in $D\setminus P$. Possiamo estendere $f$ a tutto $D$ ponendola uguale a zero in $P$, preservando così la misurabilità e non alternando il valore dell'integrale, cioè:
\begin{equation}
\int_D f\;\diff{\mathbf{x}}=\int_{D\setminus P}f\;\diff{\mathbf{x}}\;.
\end{equation}
Il limite degli integrali su $D$ di $f_n$ esiste certamente, poiché:
\begin{equation}
\int_D f_n\;\diff{\mathbf{x}}=\int_{D\setminus P} f_n\;\diff{\mathbf{x}}\le \int_{D\setminus P} f_{n+1}\;\diff{\mathbf{x}}=\int_D f_{n+1}\;\diff{\mathbf{x}}\;,
\end{equation}
ed anzi si ha:
\begin{equation}
\lim_{n\to \infty} \int_D f_n\;\diff{\mathbf{x}}\le \int_D f\;\diff{\mathbf{x}}\;.
\end{equation}
Proviamo ora la diseguaglianza opposta. Sia $\beta\in\;]0,1[$ e $\psi\in S_0$ tale che $0\le\psi\le f$. Posto $A_n=\{f_n>\beta\psi\}$, si
 ha che gli $A_n$ sono misurabili, definitivamente non vuoti, nonché crescenti rispetto all'inclusione; inoltre, dato che $f_n \to f$ per
$n\to \infty$, risulta $D=\bigcup_{n\in\mathbb{N}} A_n$. Costruiamo adesso la successione $B_n$, data da:
\begin{equation}
\begin{cases}
 B_0=A_0 \\
\\
B_n=A_n\setminus A_{n-1}, \quad n\ge 1
\end{cases}\;.
\end{equation}
Si ha allora:
\begin{equation}
\beta\int_{A_n}\psi\;\diff{\mathbf{x}}\le \int_{A_n}f_n\;\diff{\mathbf{x}}\le \lim_{n\to\infty}\int_D f_n\;\diff{\mathbf{x}}\;.
\end{equation}
Essendo $A_n$ l'unione disgiunta di $B_0,\ldots,B_n$, si ha:
\begin{equation}
\beta\sum_{k=0}^n\int_{B_k} \psi\;\diff{\mathbf{x}}\le \lim_{n\to \infty}\int_D f_n\;\diff{\mathbf{x}} \qquad \forall n\in \mathbb{N}\;.
\end{equation}
Poiché $D=\bigcup_{k\in\mathbb{N}}B_k$, ne segue:
\begin{equation}
\beta\int_D \psi\;\diff{\mathbf{x}}=\beta\sum_{k=0}^{\infty}\int_{B_k}\psi\;\diff{\mathbf{x}}\le \lim_{n\to\infty}\int_D f_n\;\diff{\mathbf{x}}\;.
\end{equation}
Per l'arbitrarietà di $\psi$, possiamo scrivere:
\begin{equation}
\beta\int_D f\;\diff{\mathbf{x}} \le \lim_{n\to\infty}\int_D f_n\;\diff{\mathbf{x}}\;,
\end{equation}
per $\beta\to 1$, si ottiene la tesi.
\endproof
\begin{lem}[Fatou] Sia $D\subseteq \mathbb{R}^N$ misurabile e sia $\{f_n\}_{n\in\mathbb{N}}$ una successione di funzioni misurabili definite su $D$ e q.o. non negative. Posto:
\begin{equation}
f(\mathbf{x})=\liminf_{n\to\infty} f_n(\mathbf{x}),\qquad \mathbf{x}\in D\;,
\end{equation}
si ha:
\begin{equation}
\int_D f\;\diff{\mathbf{x}}\le \liminf_{n\to \infty}\int_D f_n\;\diff{\mathbf{x}}\;.
\end{equation}
\end{lem}
\proof La successione $\{g_n\}_{n\in\mathbb{N}}$, data da:
\begin{equation}
g_n=\inf_{m>n} f_m\;,
\end{equation}
è crescente q.o. non negativa. Per definizione di minimo limite, segue:
\begin{equation}
f(\mathbf{x})=\lim_{n\to \infty}g_n(\mathbf{x})\;.
\end{equation}
Per il teorema di Beppo Levi, $\forall \mathbf{x}\in D$ si ha:
\begin{equation}
\int_D f\;\diff{\mathbf{x}}=\lim_{n\to\infty}\int_D g_n\;\diff{\mathbf{x}}\;,
\end{equation}
d'altra parte, essendo $g_n\le f_m$ in $D$ $\forall m\ge n$, integrando su $D$ troviamo:
\begin{equation}
\int_D g_n\;\diff{\mathbf{x}} \le \int_D f_m\;\diff{\mathbf{x}} \qquad \forall m\ge n\;,
\end{equation}
ovvero:
\begin{equation}
\int_D g_n\;\diff{\mathbf{x}} \le \inf_{m\ge n} \int_D f_m\;\diff{\mathbf{x}}\;.
\end{equation}
Pertanto, ancora per definizione di minimo limite:
\begin{equation}
\int_D f\;\diff{\mathbf{x}}=\lim_{n\to\infty}\int_D g_n\;\diff{\mathbf{x}}\le \liminf_{n\to\infty}\int_D f_n\;\diff{\mathbf{x}}\;.
\end{equation}
\endproof
\begin{thm}[Lebesgue] Sia $D$ un sottoinsieme misurabile di $\mathbb{R}^N$ e sia $\{f_n\}_{n\in\mathbb{N}}$ una successione di funzioni misurabili definite su $D$ tali che:
\begin{itemize}
 \item $f_n(\mathbf{x})\to f(\mathbf{x})$ q.o. in $D$;
 \item $|f_n(\mathbf{x})|\le g(\mathbf{x})$ q.o. in $D$ $\forall n \in \mathbb{N}$;
\end{itemize}
dove $g$ è una fissata funzione sommabile e non negativa su $D$. Allora si ha:
\begin{equation}
\lim_{n\to \infty}\int_D f_n\;\diff{\mathbf{x}}=\int_D f\;\diff{\mathbf{x}}\;,
\end{equation}
ed anzi:
\begin{equation}
\lim_{n\to \infty} \int_D|f_n-f|\;\diff{\mathbf{x}}=0\;.
\end{equation}
\end{thm}
\proof Consideriamo le successioni $\{g-f_n\}_{n\in\mathbb{N}}$ e $\{g+f_n\}_{n\in\mathbb{N}}$, entrambe costituite da funzioni q.o. non negative e convergenti puntualmente q.o. in $D$ rispettivamente a $g-f$ e $g+f$. Per il lemma di Fatou, si ha:
\begin{align}
&\int_D(g-f)\;\diff{\mathbf{x}}\le \liminf_{n\to\infty}\int_D(g-f_n)\;\diff{\mathbf{x}}=\int_Dg\;\diff{\mathbf{x}}-\limsup_{n\to\infty}\int_D f_n\;\diff{\mathbf{x}}\;, \notag \\
&\int_D(g+f)\;\diff{\mathbf{x}}\le\liminf_{n\to\infty}\int_D(g+f_n)\;\diff{\mathbf{x}}=\int_Dg\;\diff{\mathbf{x}}+\liminf_{n\to\infty}\int_Df_n\;\diff{\mathbf{x}}\;.
\end{align}
Essendo $g$ sommabile, possiamo semplificarne l'integrale, ottenendo:
\begin{equation}
\limsup_{n\to\infty}\int_Df_n\;\diff{\mathbf{x}}\le\int_D f\;\diff{\mathbf{x}}\le \lim\inf_{n\to \infty}\int_Df_n\;\diff{\mathbf{x}}\;,
\end{equation}
che prova la prima parte della tesi. La seconda parte segue applicando quanto appena dimostrato alla successione $\{|f_n-f|\}$, il che è 
lecito poiché:
\begin{align}
&|f_n(\mathbf{x})-f(\mathbf{x})|\to 0 &\mbox{q.o. in}\; D\;, \notag \\
&|f_n(\mathbf{x})-f(\mathbf{x})|\le 2g(\mathbf{x})\;, &\mbox{q.o. in}\; D,\quad \forall n\in\mathbb{N}\;.
\end{align}
\endproof
\begin{thm}[Assoluta continuità dell'integrale] Sia $D$ un sottoinsieme misurabile di $\mathbb{R}^N$. Se $f$ è una funzione sommabile su $D$, allora $\forall \epsilon>0$ esiste $\delta>0$ per il quale risulta:
\begin{equation}
\int_E |f|\;\diff{\mathbf{x}}<\epsilon, \quad \forall E\subseteq D, m_N(E)<\delta\;.
\end{equation}
\end{thm}
\proof Per assurdo supponiamo che $\exists \epsilon_0>0$ tale che $\forall n \in \mathbb{N}$, scelto $\delta=2^{-n}$, si può trovare un insieme misurabile $E_n\subseteq D$ tale che:
\begin{equation}
\int_{E_n} |f|\;\diff{\mathbf{x}}\ge \epsilon_0, \quad m_N(E_n)<2^{-n}\;.
\end{equation}
Ponendo allora:
\begin{align}
&F_n=\bigcup_{k=n}^{\infty} E_k, &F=\bigcup_{n=0}^{\infty}F_n\;,
\end{align}
abbiamo:
\begin{equation}
m_N(F_n)\le \sum_{k=n}^{\infty} 2^{-k}=2^{1-n} \quad \Longrightarrow\quad m_N(F)=0\;.
\end{equation}
Dunque $\int_F |f|\;\diff{\mathbf{x}}=0$. Osservando che la successione di funzioni sommabili $\{|f|I_{F_n}\}$ converge puntualmente in modo decrescente a $|f|I_F$, il teorema di Lebesgue ci assicura che:
\begin{equation}
0=\int_F |f|\;\diff{\mathbf{x}}=\lim_{n\to\infty}\int_{F_n}|f|\;\diff{\mathbf{x}}\ge \liminf_{n\to\infty}\int_{E_n}|f|\;\diff{\mathbf{x}}>\epsilon_0\;,
\end{equation}
il che è assurdo e quindi la tesi risulta vera.
\endproof
\section{Calcolo degli integrali multipli}
\begin{lem} Fissati $k,h\in \mathbb{N}^+$, siano $E\subseteq \mathbb{R}^k$ e $F\subseteq\mathbb{R}^h$ insiemi misurabili. Allora $E\times F$ è misurabile in $\mathbb{R}^{k+h}$ e si ha $m_{k+h}(E\times F)=m_k(E)m_h(F)$, con la convenzione $0\cdot\infty=0$.
\end{lem}
\proof La tesi è ovvia se $E,F$ sono rettangoli, dato che in tal caso $E\times F$ è ancora un rettangolo. Se $E,F$ sono plurirettangoli, sarà 
allora:
\begin{align}
&E=\bigcup_{j=1}^p R_j, &F=\bigcup_{i=1}^q S_i\;,
\end{align}
con $R_j,S_i$ rettangoli disgiunti. Dunque:
\begin{equation}
E\times F=\bigcup_{j=1}^p\bigcup_{i=1}^q (R_j\times S_i)\;,
\end{equation}
con $R_j\times S_i$ rettangoli disgiunti. Per l'additività:
\begin{align}
m_{k+h}(E\times F) &=\sum_{j=1}^p\sum_{i=1}^q m_{k+h}(R_j\times S_i)=\sum_{j=1}^p\sum_{i=1}^q m_k(R_j)m_h(S_i) \notag \\
&=m_k(E)m_h(F)\;.
\end{align}
Se $E,F$ sono aperti, esistono due successioni di plurirettangoli $\{R_n\}_{n\in\mathbb{N}}\subseteq \mathbb{R}^k$ e $\{S_n\}_{n\in
\mathbb{N}}\subseteq \mathbb{R}^h$ tali che $R_n\subseteq R_{n+1}\subseteq E$, $S_n\subseteq S_{n+1}\subseteq F$ e:
\begin{align}
&\lim_{n\to\infty} m_k(R_n)=m_k(E), &\lim_{n\to\infty} m_h(S_n)=m_h(F)\;.
\end{align}
Possiamo supporre che:
\begin{align}
&E=\bigcup_{n=0}^{\infty} R_n, &F=\bigcup_{n=0}^{\infty} S_n\;,
\end{align}
da cui:
\begin{equation}
R_n\times S_n\subseteq R_{n+1}\times S_{n+1}\subseteq E\times F=\bigcup_{n=0}^{\infty}(R_n\times S_n)\;,
\end{equation}
dunque:
\begin{equation}
m_{k+h}(E\times F)=\lim_{n\to\infty} m_{k+h}(R_n\times S_n)=\lim_{n\to\infty} m_k(R_n)m_h(S_n)=m_k(E)m_h(F)\;.
\end{equation}
Se $E,F$ sono compatti, esistono due successioni di plurirettangoli $\{R_n\}\subseteq \mathbb{R}^k$ e $\{S_n\}\subseteq \mathbb{R}^h$ tali che:
\begin{align}
&R_n\supseteq R_{n+1}\supseteq E=\bigcap_{n=0}^{\infty} R_n, &S_n\supset S_{n+1}\supset F=\bigcap_{n=0}^{\infty} S_n\;, \notag \\
&\lim_{n\to\infty}m_k(R_n)=m_k(E), &\lim_{n\to\infty}m_h(S_n)=m_h(F)\;.
\end{align}
Quindi:
\begin{equation}
R_n\times S_n\supseteq R_{n+1}\times S_{n+1}\supset E\times F=\bigcap_{n=0}^{\infty}(R_n\times S_n)\;,
\end{equation}
e ancora:
\begin{equation}
m_{k+h}(E\times F)=\lim_{n\to\infty}(R_n\times S_n)=\lim_{n\to\infty}m_k(R_n)m_h(S_n)=m_k(E)m_h(F)\;.
\end{equation}
Se $E,F$ sono misurabili e limitati, esistono due successioni di aperti $\{A_n\}_{n\in\mathbb{N}}\subseteq \mathbb{R}^k$ e $\{B_n\}_{n\in
\mathbb{N}}\subseteq \mathbb{R}^h$ e due successioni di compatti $\{K_n\}_{n\in\mathbb{N}}\subseteq \mathbb{R}^k$ e $\{H_n\}_{n\in\mathbb{N}}\subseteq \mathbb{R}^h$ tali che:
\begin{align}
&A_n\supseteq E\supseteq K_n, &B_n\supseteq F\supseteq H_n\;, \notag \\
&\lim_{n\to\infty}m_k(A_n)=\lim_{n\to\infty}m_k(K_n)=m_k(E)\;, &\lim_{n\to\infty}m_h(B_n)=m_k(E)m_h(F)\;.
\end{align}
Utilizzando gli $A_n$ e i $B_n$ si trova:
\begin{equation}
\overline{m}_{k+h}(E\times F)\le \lim_{n\to\infty}m_k(A_n)m_h(B_n)=m_k(E)m_h(F)\;.
\end{equation}
Utilizzando i $K_n$ e gli $H_n$ invece:
\begin{equation}
\underline{m}_{k+h}(E\times F)\ge \lim_{n\to\infty}m_k(K_n)m_h(H_n)=m_k(E)m_h(F)\;.
\end{equation}
Ne segue che $m_{k+h}(E\times F)=m_k(E)m_h(F)$. \\

Infine, se $E,F$ sono misurabili non necessariamente limitati, indicando con $Q_r^m$ il cubo di centro l'origine e lato $2r$ in $\mathbb{
R}^m$ si ha:
\begin{align}
&m_k(E)=\lim_{r\to\infty}m_k(E\cap Q_r^m), &m_h(F)=\lim_{r\to\infty}m_h(F\cap Q_r^m)\;.
\end{align}
Essendo $Q_r^{k+h}=Q_r^k\times Q_r^h$, si ottiene:
\begin{equation}
(E\times F)\cap Q_r^{k+h}=(E\cap Q_r^k)\times(F\cap Q_r^h) \quad \forall r>0\;.
\end{equation}
Dunque $E\times F$ è misurabile in $\mathbb{R}^{k+h}$ e si ha:
\begin{equation}
m_{k+h}(E\times F)=\lim_{r\to\infty}m_k(E\cap Q_r^k)m_h(F\cap Q_r^h)=m_k(E)m_h(F)\;.
\end{equation}
\endproof
\begin{prop} Siano $k,h\in \mathbb{N}^+$ e sia $E$ un sottoinsieme misurabile di $\mathbb{R}^{k+h}$. Per ogni $\mathbf{x}\in\mathbb{R}^k$ e per ogni $\mathbf{y}\in\mathbb{R}^h$, consideriamo le sezioni verticali ed orizzontali di $E$, rispettivamente date da:
\begin{align}
&E_{\mathbf{x}}=\{\mathbf{y}\in\mathbb{R}^h\;|\;(\mathbf{x},\mathbf{y})\in E\},  &E^{\mathbf{y}}=\{\mathbf{x}\in\mathbb{R}^k\;|\;(
\mathbf{x},\mathbf{y})\in E\}\;.
\end{align}
Allora valgono i seguenti fatti:
\begin{enumerate}
 \item $E_{\mathbf{x}}$ è misurabile in $\mathbb{R}^h$ per q.o. $\mathbf{x}\in\mathbb{R}^k$ e $E^{\mathbf{y}}$ è misurabile in $\mathbb{R}^k$ per q.o. $\mathbf{y}\in\mathbb{R}^h$;
 \item la funzione $\mathbf{x}\longmapsto m_h(E_{\mathbf{x}})$ è misurabile in $\mathbb{R}^k$ e la funzione $\mathbf{y}\longmapsto m_k(E^{\mathbf{y}})$ è misurabile in $\mathbb{R}^h$;
 \item risulta:
\begin{equation}
m_{k+h}(E)=\int_{\mathbb{R}^k} m_h(E_{\mathbf{x}})\;\diff{\mathbf{x}}=\int_{\mathbb{R}^h} m_k(E^{\mathbf{y}})\;\diff{\mathbf{y}}\;.
\end{equation}
\end{enumerate}
\end{prop}
\proof 
$(a)$ Se $E$ è un rettangolo, allora $E=P_1\times P_2$, con $P_1\subseteq \mathbb{R}^k$ e $P_2\subseteq \mathbb{R}^h$. Dunque:
\begin{equation}
E_{\mathbf{x}}=\begin{cases}
                P_2,\qquad \mathbf{x}\in P_1 \\
\\
\emptyset, \qquad \mathbf{x}\not\in P_1
               \end{cases}\;,
\end{equation}
cosicché $E_{\mathbf{x}}$ è misurabile per ogni $\mathbf{x}\in\mathbb{R}^k$ e $m_h(E_{\mathbf{x}})=m_h(P_2)I_{P_1}(\mathbf{x})$. In 
particolare, si evince che $\mathbf{x}\longmapsto m_h(E_{\mathbf{x}})$ è una funzione semplice e dunque misurabile. Inoltre, per il lemma:
\begin{equation}
\int_{\mathbb{R}^k} m_h(E_{\mathbf{x}})\;\diff{\mathbf{x}}=m_h(P_2)m_k(P_1)=m_{k+h}(E)\;.
\end{equation}
$(b)$ Se $E$ è aperto, esiste una successione $\{P_n\}\subseteq \mathcal{P}_{k+h}$ di disgiunti tale che:
\begin{align}
&P_n\subseteq P_{n+1}\subseteq E, &\bigcup_{n\in\mathbb{N}} P_n=E\;.
\end{align}
Allora $E_{\mathbf{x}}=\bigcup_{n=1}^{\infty}(P_n)_{\mathbf{x}}$ è misurabile e, per quanto già provato, $m_h(E_{\mathbf{x}})=\lim_{n\to
\infty} m_h((P_n)_{\mathbf{x}})$ è una funzione misurabile. Per il teorema di Beppo Levi, si ha pertanto:
\begin{equation}
m_{k+h}(E)=\lim_{n\to\infty}m_{k+h}(P_n)=\lim_{n\to\infty}\int_{\mathbb{R}^k} m_h((P_n)_{\mathbf{x}})\;\diff{\mathbf{x}}=\int_{\mathbb{R}^k}m_h(E_{\mathbf{x}})\;\diff{\mathbf{x}}\;.
\end{equation}
$(c)$ Se $E$ è chiuso, con $m_{k+h}(E)<\infty$, allora $\exists A\subseteq E$ aperto con $m_{k+h}(A)<\infty$. Scrivo $E=A\setminus(A
\setminus E)$, dunque $E_{\mathbf{x}}=A_{\mathbf{x}}\setminus(A\setminus E)_{\mathbf{x}}$. $A_{\mathbf{x}}$ e $(A\setminus E)_{\mathbf{x}
}$ sono entrambi aperti e pertanto sono misurabili. Di conseguenza, anche la loro differenza, cioè $E_{\mathbf{x}}$, è misurabile. 
Inoltre, $m_{h}(E_{\mathbf{x}})=m_h(A_{\mathbf{x}}\setminus(A\setminus E)_{\mathbf{x}})=m_h(A_{\mathbf{x}})-m_h((A\setminus E)_{\mathbf{x}})$, che sono tutte funzioni misurabuli, dunque anche $m_h(E_{\mathbf{x}})$ risulta misurabile. Infine:
\begin{align}
\int_{\mathbb{R}^k} m_h(E_{\mathbf{x}})\;\diff{\mathbf{x}} &=\int_{\mathbb{R}^k}m_h(A_{\mathbf{x}})\;\diff{\mathbf{x}}-\int_{\mathbb{R}^k}m_h((A\setminus E)_{\mathbf{x}})\;\diff{\mathbf{x}} \notag \\
&=m_{k+h}(A)-m_{k+h}(A\setminus E) \notag \\
&= m_{k+h}(A)-m_{k+h}(A)+m_{k+h}(E) \notag \\
&= m_{k+h}(E)\;.
\end{align}
Se $E$ è chiuso e $m_{k+h}(E)=\infty$, scriviamo $E$ come:
\begin{equation}
E=\bigcup_{n\in\mathbb{N}}(C\cap Q_n)\;,
\end{equation}
dove i $Q_n$ sono chiusi, misurabili e con misura finita. Per ogni $n$, si ha che $C\cap Q_n$ è chiuso e misurabile, con misura finita. 
Allora, in virtù del caso precedente si ha che:
\begin{equation}
E_{\mathbf{x}}=\bigcup_{n\in\mathbb{N}}(E\cap Q_n)_{\mathbf{x}}\;,
\end{equation}
è misurabile. Inoltre:
\begin{equation}
m_h(E_{\mathbf{x}})=m_h\left(\bigcup_{n\in\mathbb{N}}(E\cap Q_n)_{\mathbf{x}}\right)=\sum_{n=1}^{\infty}m_h((E\cap Q_n)_{\mathbf{x}})\;,
\end{equation}
che sono tutte funzioni misurabili. Infine:
\begin{align}
\int_{\mathbb{R}^k}m_h(E_{\mathbf{x}})\;\diff{\mathbf{x}} &=\int_{\mathbb{R}^k}\sum_{n=1}^{\infty} m_h((E\cap Q_n)_{\mathbf{x}})\;\diff{\mathbf{x}}=\sum_{n=1}^{\infty}\int_{\mathbb{R}^k} m_h((E\cap Q_n)_{\mathbf{x}})\;\diff{\mathbf{x}}\notag \\
&=\sum_{n=1}^{\infty}m_{k+h}(E\cap Q_n)=m_{k+h}\left(\bigcup_{n\in\mathbb{N}}(E\cap Q_n)\right) \notag \\
&=m_{k+h}(E)\;.
\end{align}
$(d)$ Ogni intersezione numerabile di aperti e ogni unione numerabile di chiusi verifica $(1),(2),(3)$. \\

$(e)$ Se $E\in\mathfrak{M}_N$, allora verifica $(1)$ per q.o. $\mathbf{x}\in\mathbb{R}^k$, $(2),(3)$. Sia $E$ misurabile con misura 
finita. Allora esistono $A,B\in\mathfrak{B}_N$ tali che $B\subseteq E\subseteq A$ e $m_{k+h}(A\setminus B)=0$. Dunque si ha anche $B_
{\mathbf{x}}\subseteq E_{\mathbf{x}}\subseteq A_{\mathbf{x}}$. Inoltre:
\begin{equation}
0=m_{k+h}(A\setminus B)=m_{k+h}(A)-m_{k+h}(B)=\int_{\mathbb{R}^k} (m_h(A_{\mathbf{x}})-m_h(B_{\mathbf{x}}))\;\diff{\mathbf{x}}\;,
\end{equation}
da cui segue che $m_h(A_{\mathbf{x}})=m_h(B_{\mathbf{x}})$ q.o. in $\mathbb{R}^k$ e quindi $m_h^*(E_{\mathbf{x}})=m_h(A_{\mathbf{x}})=m_h(B_{\mathbf{x}})$ q.o. in $\mathbb{R}^k$. Scrivendo $E_{\mathbf{x}}=A_{\mathbf{x}}\setminus (A_{\mathbf{x}}\setminus E_{\mathbf{x}})$, si ha che $A_{\mathbf{x}}\in\mathfrak{M}_h$ e $A_{\mathbf{x}}\setminus E_{\mathbf{x}}$ ha per costruzione misura nulla, perciò segue che $E_{\mathbf{x}}\in\mathfrak{M}_h$ per q.o. $\mathbf{x}\in\mathbb{R}^k$ e $m_h(E_{\mathbf{x}})=m_h(A_{\mathbf{x}})=m_h(B_{\mathbf{x}})$. Infine:
\begin{equation}
\int_{\mathbb{R}^k} m_h(E_{\mathbf{x}})\;\diff{\mathbf{x}}=\int_{\mathbb{R}^k}m_h(A_{\mathbf{x}})\;\diff{\mathbf{x}}=m_{k+h}(A)=m_{k+h}(E)\;.
\end{equation}
\endproof
\begin{thm}[Tonelli] Sia $f:\mathbb{R}^{k+h}\to\overline{\mathbb{R}}$ una funzione misurabile e non negativa. Allora si ha che:
\begin{enumerate}
 \item la funzione $f(\cdot,\mathbf{y})$ è misurabile in $\mathbb{R}^k$ per q.o. $\mathbf{y}\in\mathbb{R}^h$ e la funzione $f(\mathbf{x},
\cdot)$ è misurabile in $\mathbb{R}^h$ per q.o. $\mathbf{x}\in\mathbb{R}^k$;
\item la funzione $\int_{\mathbb{R}^h} f(\cdot,\mathbf{y})\;d\mathbf{y}$ è misurabile in $\mathbb{R}^k$ e la funzione $\int_{\mathbb{R}^k} f(\mathbf{x},\cdot)\;d\mathbf{x}$ è misurabile in $\mathbb{R}^h$;
\item risulta:
\begin{equation}
\int_{\mathbb{R}^{k+h}}f(\mathbf{x},\mathbf{y})\;\diff{\mathbf{x}}\diff{\mathbf{y}}=\int_{\mathbb{R}^k}\left[\int_{\mathbb{R}^h} f(\mathbf{x},\mathbf{y})\;\diff{\mathbf{y}}\right]\diff{\mathbf{x}}=\int_{\mathbb{R}^h}\left[\int_{\mathbb{R}^k} f(\mathbf{x},\mathbf{y})\;\diff{\mathbf{x}}\right]\diff{\mathbf{y}}\;.
\end{equation}
\end{enumerate}
\end{thm}
\proof Se $f=I_E$, con $E\subseteq\mathbb{R}^{k+h}$ misurabile, la tesi è fornita dalla proposizione precedente. Se $f$ è una funzione semplice, la tesi segue per linearità. Nel caso generale, grazie alla non negatività di $f$, esiste una successione di funzioni semplici $\{
\varphi_n\}_{n\in\mathbb{N}}$ che converge puntualmente a $f$ in modo crescente. La proprietà $(1)$ è vera perché $f(\cdot,\mathbf{y})$ e $f(\mathbf{x},\cdot)$ sono limiti puntuali di funzioni misurabili; $(2)$ e $(3)$ si ottengono appolicando il teorema di Beppo Levi. Se 
$f$ è di segno variabile e in più integrabile, allora basta scrivere il risultato per $f^+$ e $f^-$ e poi sottrarre, il che è lecito in quanto almeno uno tra gli integrali è finito.
\endproof
\begin{thm}[Fubini] Sia $f:\mathbb{R}^{k+h}\to\overline{\mathbb{R}}$ una funzione sommabile. Allora si ha che:
\begin{enumerate}
 \item la funzione $f(\cdot,\mathbf{y})$ è sommabile su $\mathbb{R}^k$ per q.o. $\mathbf{y}\in\mathbb{R}^h$ e la funzione $f(\mathbf{x},\cdot)$ è sommabile su $\mathbb{R}^h$ per q.o. $\mathbf{x}\in\mathbb{R}^k$;
 \item la funzione $\int_{\mathbb{R}^h} f(\cdot,\mathbf{y})\;d\mathbf{y}$ è sommabile su $\mathbb{R}^k$ e la funzione $\int_{\mathbb{R}^k} f(\mathbf{x},\cdot)\;d\mathbf{x}$ è sommabile su $\mathbb{R}^h$;
\item risulta:
\begin{equation}
\int_{\mathbb{R}^{k+h}}f(\mathbf{x},\mathbf{y})\;\diff{\mathbf{x}}\diff{\mathbf{y}}=\int_{\mathbb{R}^k}\left[\int_{\mathbb{R}^h}f(\mathbf{x},\mathbf{y})\;\diff{\mathbf{y}}\right]\diff{\mathbf{x}}=\int_{\mathbb{R}^h}\left[\int_{\mathbb{R}^k}f(\mathbf{x},\mathbf{y})\;\diff{\mathbf{x}}\right]\diff{\mathbf{y}}\;.
\end{equation}
\end{enumerate}
\end{thm}
\proof La tesi segue applicando il teorema di Tonelli alle funzioni non negative $f^+,f^-$; gli integrali risultanti sono entrambi finiti, in
 quanto $f^+,f^-\le |f|$ e $|f|$ è sommabile. Il risultato si ottiene per differenza.
\endproof
\section{Cambiamento di variabili}
\begin{defn} Siano $A,B\subseteq \mathbb{R}^N$ aperti. Una funzione $\mathbf{g}:A\to B$ si dice \textit{diffeomorfismo} se:
\begin{itemize}
 \item $\mathbf{g}$ è bigettiva;
 \item $\mathbf{g}\in C^1(A)$;
 \item $\mathbf{g}^{-1}\in C^1(B)$.
\end{itemize}
\end{defn}
\begin{oss} Se $\mathbf{g}$ è un diffeomorfismo, allora $J_{\mathbf{g}}(\mathbf{x})=\det\{D\mathbf{g}(\mathbf{x})\}\ne 0\;\forall \mathbf{x}$.
\end{oss}
\begin{thm}[Misurabilità] Siano $\mathbf{g}:A\to B$ un diffeomorfismo e $E\subseteq A$ misurabile. Allora $\mathbf{g}(E)$ è misurabile e si ha:
\begin{equation}
m_N(\mathbf{g}(E))=\int_E |J_{\mathbf{g}}(\mathbf{x})|\;\diff{\mathbf{x}}\;.
\end{equation}
\end{thm}
\begin{thm}[Formula del cambiamento di variabili] Sia $\mathbf{g}:A\to B$ un diffeomorfismo. Siano $F\subseteq B$ misurabile e $\mathbf{f}$ integrabile su $F$. Allora si ha:
\begin{equation}
\int_F \mathbf{f}(\mathbf{y})\;\diff{\mathbf{y}}=\int_{\mathbf{g}^{-1}(F)}\mathbf{f}(\mathbf{g}(\mathbf{x}))|J_{\mathbf{g}}(\mathbf{x})|\diff{\mathbf{x}}\;.
\end{equation}
\end{thm}
\begin{exm} Dato $F=\{(x,y)|x\le y\le 2x,1\le xy\le 3\}$, calcolare $m_2(F)$. \\

Osserviamo che i vincoli possono essere scritti nel seguente modo:
\begin{align}
&1\le \frac{y}{x}\le 2, &1\le xy\le 3\;.
\end{align}
Allora, poniamo:
\begin{equation}
\begin{cases}
 u=\dfrac{y}{x} \\
\\
v=xy
\end{cases}\;,
\end{equation}
da cui, invertendo le relazioni, si ottiene:
\begin{equation}
\begin{cases}
 x=v^{1/2}u^{-1/2} \\
\\
y=v^{1/2}u^{1/2}
\end{cases}\;,
\end{equation}
che definisce la funzione $g$. Allora si ha $F=g([1,2]\times[1,3])$. La matrice $Dg(u,v)$ sarà:
\begin{equation}
Dg(u,v)=\begin{pmatrix}
               -\dfrac{v^{1/2}u^{-3/2}}{2} & \dfrac{v^{-1/2}u^{-1/2}}{2} \\
\dfrac{v^{1/2}u^{-1/2}}{2} & \dfrac{v^{-1/2}u^{1/2}}{2}
              \end{pmatrix}\qquad \Longrightarrow\qquad  J_g(u,v)=-\frac{1}{2u}\;.
\end{equation}
Allora si ha:
\begin{equation}
m_2(F)=\int_1^2\int_1^3\frac{1}{2u}\;\diff{v}\diff{u}=\int_1^2\frac{1}{2u}[v]^3_1\;\diff{u}=\int_1^2 \frac{\diff{u}}{u}=[\ln u]^2_1=\ln 2\;.
\end{equation}
\end{exm}
\begin{exm} Sia $E=\{(x,y)\;|\;1\le x+y\le 3,x\le y\le 2x\}$. Calcolare:
\begin{equation}
\int_E x^2\;\diff{x}\diff{y}\;.
\end{equation}
Effettuiamo il cambiamento di variabili ponendo:
\begin{equation}
\begin{cases}
 u=x+y \\
\\
v=\dfrac{y}{x}
\end{cases}\;.
\end{equation}
Invertendo le relazioni, si ottiene:
\begin{equation}
\begin{cases}
 x=\dfrac{u}{1+v} \\
\\
y=\dfrac{uv}{1+v}
\end{cases}\;.
\end{equation}
La matrice delle derivate sarà:
\begin{equation}
Dg(u,v)=\begin{pmatrix}
               \dfrac{1}{1+v} & -\dfrac{u}{(1+v)^2} \\
\dfrac{v}{1+v} & \dfrac{u}{(1+v)^2}
              \end{pmatrix}\qquad  \Longrightarrow\qquad  J_g(u,v)=\frac{u}{(1+v)^2}\;.
\end{equation}
Allora:
\begin{align}
&(u,v)\in [1,3]\times [1,2], &E=g([1,3]\times[1,2])\;.
\end{align}
E perciò:
\begin{align}
\int_E x^2\;\diff{x}\diff{y} &=\int_1^3\frac{u^2}{(1+v)^2}\frac{u}{(1+v)^2}\;\diff{v}\diff{u}=\int_1^3u^3\;\diff{u}\int_1^2\frac{1}{(1+v)^4}\;\diff{v} \notag \\
&=\left[\frac{u^4}{4}\right]^3_1\cdot \left[-\frac{1}{3(1+v)^3}\right]^2_1=\frac{95}{162}\;.
\end{align}
\end{exm}
\section{Cambiamento di variabili in coordinate polari}
Le coordinate polari $(r,\theta)$ sono legate a quelle cartesiane da:
\begin{equation}
\begin{cases}
 x=r\cos\theta \\
\\
y=r\sin\theta
\end{cases}\;.
\end{equation}
Sia $g(r,\theta)$ la funzione data da:
\begin{equation}
g(r,\theta):\;]0,+\infty[\;\times\;]0,2\pi[\;\to\mathbb{R}^2\setminus\{(x,0),x>0\}\;.
\end{equation}
Così definita, $g$ è bigettiva. Inoltre l'insieme $\{(x,0),x>0\}$ che abbiamo tolto da $\mathbb{R}^2$ ha misura nulla, dunque non viene 
alterato niente ai fini del calcolo. Ricavando le coordinate polari in funzione di quelle cartesiane si ottiene:
\begin{equation}
\begin{cases}
r=\sqrt{x^2+y^2} \\
\\
\theta=\arctan\left(\dfrac{y}{x}\right)
\end{cases}\;.
\end{equation}
La matrice delle derivate sarà pertanto:
\begin{equation}
Dg(r,\theta)=\left(\begin{matrix}
                    \cos\theta & -r\sin\theta \\
\sin\theta & r\cos\theta
                   \end{matrix}\right)\qquad \Longrightarrow \qquad J_g(r,\theta)=r\;.
\end{equation}
\begin{prop} Se $E\subseteq \mathbb{R}^2$ è misurabile e $E'=\{(r,\theta)\;|\;(r\cos\theta,r\sin\theta)\in E\}$, allora $E'$ è misurabile e per ogni funzione $f$ integrabile su $E$ vale:
\begin{equation}
\int_E f(x,y)\;\diff{x}\diff{y}=\int_{E'} f(r\cos\theta,r\sin\theta)r\;\diff{r} \diff{\theta}\;.
\end{equation}
\end{prop}
\begin{exm} Sia $C=\{(x,y)\;|\;1\le x^2+y^2\le 4, x/\sqrt{3}\le y\le \sqrt{3}x\}$, calcolare:
\begin{equation}
\int_C x\;\diff{x}\diff{y}\;.
\end{equation}
In coordinate polari, si ha:
\begin{equation}
C'=\left\{(r,\theta)\;|\; 1\le r\le 2, \frac{\pi}{6}\le \theta\le \frac{\pi}{3}\right\}\;.
\end{equation}
Segue che:
\begin{align}
\int_C x\;\diff{x}\diff{y} &= \int_1^2\int_{\pi/6}^{\pi/3}r\cos\theta\; r\diff{\theta}\diff{r}=\int_1^2 r^2\;\diff{r}\int_{\pi/6}^{\pi/3}\cos\theta\;\diff{\theta} \notag \\
&=\left[\frac{r^3}{6}\right]^2_1[\sin\theta]^{\pi/3}_{\pi/6}=\frac{7}{6}\left(\frac{\sqrt{3}-1}{2}\right)\;.
\end{align}
\end{exm}
\begin{exm} Calcolare:
\begin{equation}
I=\int_0^{+\infty} e^{-x^2}\;\diff{x}\;.
\end{equation}
Facendo il quadrato dell'integrale e applicando il teorema di Tonelli si ottiene:
\begin{equation}
I^2=\int_0^{+\infty}e^{-x^2}\;\diff{x}\cdot\int_0^{+\infty}e^{-y^2}\;\diff{y}=\int_0^{+\infty}\int_0^{+\infty}e^{-(x^2+y^2)}\;\diff{x}\diff{y}\;.
\end{equation}
Passando in coordinate polari:
\begin{align}
&\sqrt{x^2+y^2}=r\in [0,+\infty[, &\arctan\left(\frac{y}{x}\right)=\theta\in \left[0, \frac{\pi}{2}\right]\;,
\end{align}
si ha:
\begin{equation}
I^2=\int_0^{+\infty}\int_0^{\pi/2}e^{-r^2}r\;\diff{\theta} \diff{r}=\int_0^{\pi/2}\diff{\theta}\int_0^{\infty}re^{-r^2}\;\diff{r}=\frac{\pi}{2}\left[\frac{e^{-r^2}}{2}\right]^{+\infty}_0=\frac{\pi}{4}\;.
\end{equation}
Dunque:
\begin{equation}
I=\int_0^{+\infty} e^{-x^2}\;\diff{x}=\sqrt{\frac{\pi}{4}}=\frac{\sqrt{\pi}}{2}\;.
\end{equation}
\end{exm}
\section{Cambiamento di variabili in coordinate cilindriche}
\begin{align}
&\begin{cases}
x=r\cos\theta \\
\\
y=r\sin\theta \\
\\
z=z
\end{cases} \;,\notag \\
g(r,\theta,z):[0,+\infty[\;\times [0,2\pi[\;\times\mathbb{R}&\to \mathbb{R}^3\setminus\{(x,y,z)\;|\;y=0, x\ge 0\}\;,
\end{align}
dove l'insieme che togliamo da $\mathbb{R}^3$ ha misura nulla. La matrice delle derivate sarà:
\begin{equation}
Dg(r,\theta,z)=\left(\begin{matrix}
                      \cos\theta & -r\sin\theta & 0 \\
\sin\theta & r\cos\theta & 0 \\
0 & 0 & 1
                     \end{matrix}\right)\quad  \Longrightarrow\quad  J_g(r,\theta,z)=r\;.
\end{equation}
\begin{prop} Se $E\subseteq \mathbb{R}^3$ è misurabile e $E'=\{(r,\theta,z)\;|\;(r\cos\theta,r\sin\theta,z)\in E\}$, allora si ha:
\begin{equation}
\int_E f(x,y,z)\;\diff{x}\diff{y}\diff{z}=\int_{E'} f(r\cos\theta,r\sin\theta,z)r\;\diff{r}\diff{\theta}\diff{z}\;.
\end{equation}
\end{prop}
\section{Volume del solido di rotazione}
Sia $D\in \mathbb{R}^2_{xz}$ e $E$ la rotazione di $D$ rispetto all'asse $z$ data da:
\begin{equation}
E=\{(x,y,z)\;|\; (\sqrt{x^2+y^2},z)\in D\}\;.
\end{equation}
Passando in coordinate cilindriche si ha:
\begin{equation}
E'=\{(r,\theta,z)\;|\; \theta\in[0,2\pi],(r,z)\in D\}\;.
\end{equation}
Allora il volume del solido di rotazione, equivalente alla sua misura tridimensionale, sarà:
\begin{equation}
m_3(E)=\int_E \diff{x}\diff{y}\diff{z}=\int_{E'} r\;\diff{r}\diff{\theta}\diff{z}=\int_0^{2\pi}\int_D r\;\diff{r}\diff{z}\diff{\theta}=\int_D 2\pi r\;\diff{r}\diff{z}\;.
\end{equation}
Questo procedimento è detto \textit{integrazione per circonferenze}.
\begin{exm}[Volume del toro]  Sia $r$ il raggio della sezione del toro $T$ e $R$ la sua distanza dall'asse $z$ di rotazione, con $r<R$. Allora si ha:
\begin{equation}
m_3(T)=2\pi\int_{B((R,0),r)} x\;\diff{x}\diff{z}\;.
\end{equation}
Passando in coordinate cilindriche $\rho,\theta$:
\begin{equation}
\begin{cases}
 x-R=\rho\cos\theta \\
\\
z=\rho\sin\theta
\end{cases},\qquad \rho\in [0,r],\;\theta\in[0,2\pi]\;,
\end{equation}
si ottiene:
\begin{align}
m_3(T) &= 2\pi\int_0^r\int_0^{2\pi} (R+\rho\cos\theta)\rho\;\diff{\theta}\diff{\rho}=2\pi\cdot 2\pi\int_0^r R\rho\;\diff{\rho} \notag \\
&=4\pi^2R\frac{r^2}{2}=(2\pi R)(\pi r^2)\;.
\end{align}
\end{exm}
\section{Cambiamento di variabili in coordinate sferiche}
\begin{equation}
\begin{cases}
 x=r\sin\theta\cos\varphi \\
\\
y=r\sin\theta\sin\varphi \\
\\
z=r\cos\theta
\end{cases}\;,
\end{equation}
con $r\in [0,+\infty[,\theta\in [0,\pi],\varphi\in [0,2\pi]$. La matrice delle derivate sarà:
\begin{align}
Dg(r,\theta,\varphi)&=\left(\begin{matrix}
                            \sin\theta\cos\varphi & r\cos\theta\cos\varphi & -r\sin\theta\sin\varphi \\
\sin\theta\sin\varphi & r\cos\theta\sin\varphi & r\sin\theta\cos\varphi \\
\cos\theta & -r\sin\theta & 0
                           \end{matrix}\right)\;, \notag \\
J_g(r,\theta,\varphi)&=r^2\sin\theta\;.
\end{align}
\begin{prop} Se $E\subseteq \mathbb{R}^3$ è misurabile ed $E'=\{(r,\theta,\varphi)\;|\;(r\sin\theta\cos\varphi,r\sin\theta\sin\varphi,r\cos\theta)\in E\}$, allora:
\begin{equation}
\int_E f(x,y,z)\;\diff{x}\diff{y}\diff{z}=\int_{E'} f(r\sin\theta\cos\varphi,r\sin\theta\sin\varphi,r\cos\theta)r^2\sin\theta\;\diff{r}\diff{\theta} \diff{\varphi}\;.
\end{equation}
\end{prop}
\begin{exm} Sia $A=\{(x,y,z)\;|\;x^2+y^2+z^2\le 1,0\le z\le \sqrt{x^2+y^2}\}$. Calcolare:
\begin{equation}
\int_A (x^2+y^2+z^2)^{2}\; \diff{x}\diff{y}\diff{z}\;.
\end{equation}
In coordinate sferiche, si ha:
\begin{equation}
A'=\left\{(r,\theta,\varphi)\;|\; r\in [0,1],\; \theta\in \left[\frac{\pi}{4},\frac{\pi}{2}\right],\;\varphi\in [0,2\pi]\right\}
\end{equation}
Dunque:
\begin{align}
\int_A(x^2+y^2+z^2)^2\;\diff{x}\diff{y}\diff{z} &=\int_0^{2\pi}\int_{\pi/4}^{\pi/2}\int_0^1 r^4r^2\sin\theta\;\diff{r}\diff{\theta}\diff{\varphi} \notag \\
&=2\pi\int_{\pi/4}^{\pi/2}\sin\theta\;\diff{\theta}\int_0^1 r^6\;\diff{r}=2\pi[-\cos\theta]^{\pi/2}_{\pi/4}\left[\frac{r^7}{7}\right]^1_0 \\
&=\frac{\pi\sqrt{2}}{7}\;.
\end{align}
\end{exm}
\section{Curve e lunghezza di una curva}
\begin{defn} Si definisce \textit{curva} un'applicazione $\boldsymbol{\varphi}:I\to \mathbb{R}^N$ continua, con $I$ intervallo. Si definisce inoltre \textit{sostengo} di una curva l'insieme:
\begin{equation}
\Gamma=\boldsymbol{\varphi}(I)=\{\mathbf{x}=\boldsymbol{\varphi}(t),t\in I\}\;.
\end{equation}
\end{defn}
\begin{defn} Una curva $\boldsymbol{\varphi}$ si dice \textit{regolare} se $\boldsymbol{\varphi}\in C^1$ e $\boldsymbol{\varphi}\ne 0$.
\end{defn}
Sia $\boldsymbol{\varphi}:I\to\mathbb{R}^N$ una curva di classe $C^1$, con $I$ non necessariamente limitato. Fissata una partizione di $I$ $\sigma:\inf I\le t_0<t_1<\ldots<t_{k-1}<t_k\le\sup I$, consideriamo la spezzata unione dei segmenti di estremi $\boldsymbol{\varphi}
(t_{i-1}),\boldsymbol{\varphi}(t_i)$, $i=1,\ldots,k$. La lunghezza della spezzata $S_{\sigma}$ sarà:
\begin{equation}
l(S_{\sigma})=\sum_{i=1}^k |\boldsymbol{\varphi}(t_i)-\boldsymbol{\varphi}(t_{i-1})|_N\;.
\end{equation}
\begin{defn} Si definisce \textit{lunghezza} di una curva $\boldsymbol{\varphi}$ la quantità:
\begin{equation}
l(\boldsymbol{\varphi})=\sup_{\sigma} l(S_{\sigma})\;,
\end{equation}
al variare delle partizioni $\sigma$.
\end{defn}
\begin{thm} Sia $\boldsymbol{\varphi}:I\to\mathbb{R}^N$ una curva di classe $C^1$. Allora si ha:
\begin{equation}
l(\boldsymbol{\varphi})=\int_I |\boldsymbol{\varphi}'(t)|_N\;\diff{t}\;.
\end{equation}
\end{thm}
\proof $(\le)$ \\

Per ogni suddivisione $\sigma$ di $I$ si ha:
\begin{align}
l(S_{\sigma}) &= \sum_{i=1}^k|\boldsymbol{\varphi}(t_i)-\boldsymbol{\varphi}(t_{i-1})|_N=\sum_{i=1}^k\left|\int_{t_{i-1}}^{t_i}\boldsymbol{\varphi}'(t)\;\diff{t}\right|_N \notag \\
&\le \sum_{i=1}^k\int_{t_{i-1}}^{t_i}|\boldsymbol{\varphi}'(t)|_N\;\diff{t}=\int_{t_0}^{t_k}|\boldsymbol{\varphi}'(t)|_N\;\diff{t} \notag \\
&\le \int_I |\boldsymbol{\varphi}'(t)|_N\;\diff{t}\;.
\end{align}
Passando al sup si ottiene:
\begin{equation}
l(\boldsymbol{\varphi})=\sup_{\sigma}l(S_{\sigma})\le\int_I |\boldsymbol{\varphi}'(t)|_N\;\diff{t}\;.
\end{equation}
\endproof
\proof $(\ge)$ \\

Sia $[a,b]\subseteq I$ e $\epsilon>0$. La funzione $\boldsymbol{\varphi}'(t)$ è uniformemente continua nelle sue componenti, allora $
\exists\delta>0$ tale che $\forall \tau,t\in[a,b]$ con $|\tau-t|<\delta$ si abbia $|\boldsymbol{\varphi}'(\tau)-\boldsymbol{\varphi}'(t)|_
N<\epsilon$. Sia $\sigma$ una suddivisione di $[a,b]$ con $a=t_0<t_1<\ldots<t_k=b$ e $t_i-t_{i-1}<\delta\;\forall i$. Fissato $i$, considero:
\begin{align}
\int_{t_{i-1}}^{t_i} |\BF{\varphi}'(t)|_N\;\diff{t} &= \int_{t_{i-1}}^{t_i} |\BF{\varphi}'(t)-\BF{\varphi}'(t_i)+\BF{\varphi}'(t_i)|_N\;\diff{t} \notag \\
&\le \int_{t_{i-1}}^{t_i}|\BF{\varphi}'(t)-\BF{\varphi}'(t_i)|_N\;\diff{t}+\int_{t_{i-1}}^{t_i}|\BF{\varphi}'(t_i)|_N\;\diff{t}\;.
\end{align}
Ma:
\begin{equation}
\int_{t_{i-1}}^{t_i} |\boldsymbol{\varphi}'(t_i)|_N\;\diff{t}=|\boldsymbol{\varphi}'(t_i)|_N(t_i-t_{i-1})\;.
\end{equation}
Poiché $t_i-t_{i-1}>0$ per costruzione, si ha:
\begin{equation}
\int_{t_{i-1}}^{t_i}|\BF{\varphi}'(t_i)|_N\;\diff{t}=|\boldsymbol{\varphi}'(t_i)(t_i-t_{i-1})|_N=\left|\int_{t_{i-1}}^{t_i}\boldsymbol{\varphi}'(t_i)\;\diff{t}\right|_N\;.
\end{equation}
Sostituiamo quanto trovato nella relazione precedente, aggiungiamo e sottraiamo al secondo integrando $\boldsymbol{\varphi}'(t)$ e 
applichiamo la diseguaglianza triangolare, ottenendo così:
\begin{align}
&\int_{t_{i-1}}^{t_i}|\BF{\varphi}'(t)-\BF{\varphi}'(t_i)|_N\;\diff{t}+\int_{t_{i-1}}^{t_i}|\BF{\varphi}'(t_i)|_N\;\diff{t} \notag \\
&\le \int_{t_{i-1}}^{t_i}|\BF{\varphi}'(t)-\BF{\varphi}'(t_i)|_N\;\diff{t}+\int_{t_{i-1}}^{t_i}|\BF{\varphi}'(t_i)-\BF{\varphi}'(t)|_N\;\diff{t}+\left|\int_{t_{i-1}}^{t_i}\BF{\varphi}'(t)\;\diff{t}\right|_N\notag \\
&=2\int_{t_{i-1}}^{t_i}|\BF{\varphi}'(t)-\BF{\varphi}'(t_i)|_N\;\diff{t}+\left|\int_{t_{i-1}}^{t_i}\BF{\varphi}'(t)\;\diff{t}\right|_N\notag \\
&\le 2\epsilon(t_i-t_{i-1})+|\BF{\varphi}(t_i)-\BF{\varphi}(t_{i-1})|_N\;.
\end{align}
Allora:
\begin{align}
\int_a^b |\BF{\varphi}'(t)|_N\;\diff{t}&=\sum_{i=1}^k\int_{t_{i-1}}^{t_i}|\BF{\varphi}'(t)|_N\;\diff{t}\le 2\epsilon\sum_{i=1}^k (t_i-t_{i-1})+l(S_{\sigma}) \notag \\
&=2\epsilon(b-a)+l(S_{\sigma})\le 2\epsilon(b-a)+l(\BF{\varphi})\;.
\end{align}
Per l'arbitrarietà di $\epsilon$, per ogni intervallo $[a,b]\subseteq I$ si ha dunque:
\begin{equation}
\int_a^b |\boldsymbol{\varphi}'(t)|_N\;\diff{t}\le l(\boldsymbol{\varphi})\;,
\end{equation}
da cui segue:
\begin{equation}
\int_I |\boldsymbol{\varphi}'(t)|_N\;\diff{t} \le l(\boldsymbol{\varphi})\;.
\end{equation}
\endproof
\begin{defn} Una curva $\boldsymbol{\varphi}:I\to\mathbb{R}^N$ si definisce \textit{regolare a tratti} se è possibile decomporre $I$ in sottoinsiemi adiacenti $I_1,\ldots,I_k$ tali che $\forall i=1,\ldots,k$ $\boldsymbol{\varphi}|_{I_i}$ sia regolare semplice.
\end{defn}
\begin{oss} La lunghezza di una curva $\boldsymbol{\varphi}$ regolare a tratti sarà:
\begin{equation}
l(\boldsymbol{\varphi})=\sum_{i=1}^k l\left(\boldsymbol{\varphi}|_{I_i}\right)\;.
\end{equation}
\end{oss}
\begin{defn} Data una curva $\boldsymbol{\varphi}:[a,b]\to\mathbb{R}^N$ regolare data da $\mathbf{x}=\boldsymbol{\varphi}(t),\;t\in[a,b]$, definiamo:
\begin{equation}
s=\lambda(t)=\int_a^t |\boldsymbol{\varphi}'(\tau)|_N\;\diff{\tau},\qquad s\in[0,l(\boldsymbol{\varphi})]\;.
\end{equation}
Poiché l'integrando è sempre positivo, $\lambda(t)$ sarà strettamente crescente. Esisterà dunque $\lambda'(t)=|\boldsymbol{\varphi}'(t)|
_N>0$. Di conseguenza, $\lambda(t)$ sarà invertibile e la sua inversa sarà data da:
\begin{equation}
t=\lambda^{-1}(s):[0,l(\boldsymbol{\varphi})]\to[a,b]\;,
\end{equation}
e la sua derivata prima sarà:
\begin{equation}
\left(\lambda^{-1}\right)'(s)=\frac{1}{\left|\boldsymbol{\varphi}'(\lambda^{-1}(s))\right|_N}\;.
\end{equation}
Si definisce \textit{ascissa curvilinea} della curva $\boldsymbol{\varphi}$ la funzione:
\begin{equation}
\boldsymbol{\alpha}(s)=\boldsymbol{\varphi}((\lambda^{-1}(s)):[0,l(\boldsymbol{\varphi})]\to\Gamma=\{\mathbf{x}=\boldsymbol{\alpha}(s),s\in[0,l(\boldsymbol{\varphi})]\}\;.
\end{equation}
\end{defn}
\begin{oss} L'ascissa curvilinea percorre la curva con velocità unitaria. Infatti:
\begin{equation}
\boldsymbol{\alpha}'(s)=\boldsymbol{\varphi}'(\lambda^{-1}(s))(\lambda^{-1})'(s)=\frac{\boldsymbol{\varphi}'(\lambda^{-1}(s))}{|
\boldsymbol{\varphi}'(\lambda^{-1}(s))|_N}\;,
\end{equation}
che è appunto un vettore unitario.
\end{oss}
\chapter{Integrale curvilineo e di superficie}
\section{Integrale curvilineo}
\begin{defn} Siano $\Gamma\subseteq\mathbb{R}^N$ e $f:A\subseteq\mathbb{R}^N\to\mathbb{R}$, con $A$ aperto e $\Gamma\subseteq A$. Allora:
\begin{equation}
\int_{\Gamma}f\;\diff{s}=\int_a^b f(\boldsymbol{\varphi}(t))|\boldsymbol{\varphi}'(t)|_N\;\diff{t}\;.
\end{equation}
\end{defn}
\begin{oss} L'integrale curvilineo rispetta le proprietà di monotonia e linearità. In più, vale l'additività su funzioni regolari a tratti.
\end{oss}
\section{Integrale di superficie}
\begin{defn} Si definisce \textit{superficie} una funzione $\boldsymbol{\sigma}:D\subseteq \mathbb{R}^2\to\mathbb{R}^3$, dove $D=\overline{A}$ con $A$ aperto connesso, $\boldsymbol{\sigma}$ continua su $D$, di classe $C^1$ e iniettiva su $A$. La superficie si dice \textit{regolare} se la matrice delle derivate:
\begin{equation}
D\boldsymbol{\sigma}(u,v)=\left(\begin{matrix}
                                 (\sigma_1)_u & (\sigma_1)_v \\
(\sigma_2)_u & (\sigma_2)_v \\
(\sigma_3)_u & (\sigma_3)_v \\
                                \end{matrix}\right)
\end{equation}
ha rango 2. Indicando con $\Sigma=\boldsymbol{\sigma}(D)$ il sostegno della superficie, si osserva che i vettori $\boldsymbol{\sigma}_u(
u,v),\boldsymbol{\sigma}_v(u,v)$ sono vettori non nulli tangenti a $\Sigma$ nel punto $\boldsymbol{\sigma}(u,v)$ e tra loro linearmente 
indipendenti. Se $\boldsymbol{\sigma}$ è regolare, allora $\Sigma$ ha piano tangente nel punto $\boldsymbol{\sigma}(u,v)$ dato dall'
equazione:
\begin{equation}
\mathbf{x}=\boldsymbol{\sigma}(u,v)+s\boldsymbol{\sigma}_u(u,v)+t\boldsymbol{\sigma}_v(u,v),\qquad (s,t)\in\mathbb{R}^2\;.
\end{equation}
\end{defn}
\begin{defn} Sia $R$ il rettangolo $R=[u,u+h]\times[v,v+k]\subseteq D$. Si ha che $\boldsymbol{\sigma}(R)$ è il parallelogrammo generato da $\boldsymbol{\sigma}_u(u,v)$ e $\boldsymbol{\sigma}_v(u,v)$, e la sua area sarà data da:
\begin{equation}
|k\boldsymbol{\sigma}_u(u,v)|_3|h\boldsymbol{\sigma}_v(u,v)|_3\sin\theta=|hk||(\boldsymbol{\sigma}_u\times\boldsymbol{\sigma}_v)(u,v)|_3\;.
\end{equation}
Per $(h,k)\to(0,0)$ si ha:
\begin{equation}
\diff{S}=|(\boldsymbol{\sigma}_u\times\boldsymbol{\sigma}_v)(u,v)|_3\;\diff{u}\diff{v}\;.
\end{equation}
L'area di $\Sigma=\boldsymbol{\sigma}(D)$ sarà allora:
\begin{equation}
a(\Sigma)=\int_D |(\boldsymbol{\sigma}_u\times\boldsymbol{\sigma}_v)(u,v)|_3\;\diff{u}\diff{v}\;.
\end{equation}
\end{defn}
\begin{exm}[Area della superficie sferica] 
\begin{equation}
\begin{cases}
 x=r\sin\theta\cos\varphi \\
y=r\sin\theta\sin\varphi \\
z=r\cos\theta
\end{cases}
\qquad r\;\mathrm{costante},\theta\in[0,\pi],\varphi\in[0,2\pi]\;.
\end{equation}
La matrice delle derivate sarà:
\begin{equation}
D\sigma(\theta,\varphi)=\left(\begin{matrix}
                               r\cos\theta\cos\varphi & -r\cos\theta\sin\varphi \\
r\cos\theta\sin\varphi & r\sin\theta\cos\varphi \\
-r\sin\theta & 0
                              \end{matrix}\right)\;.
\end{equation}
Dunque:
\begin{equation}
|\sigma_{\theta}\times\sigma_{\varphi}|_3=\sqrt{(r^4\cos^2\theta\sin^2\theta+r^4\sin^4\theta)}=r^2\sin\theta\;.
\end{equation}
Allora:
\begin{equation}
a(\Sigma)=\int_0^{\pi}\int_0^{2\pi} r^2\sin\theta\;\diff{\varphi} \diff{\theta}=2\pi r^2[-\cos\theta]^{\pi}_0=4\pi r^2\;.
\end{equation}
\end{exm}
\chapter{Campi vettoriali}
\section{Campi vettoriali e linee di forza}
\begin{defn} Si definisce \textit{campo vettoriale} una funzione $\mathbf{F}:A\subseteq\mathbb{R}^N\to\mathbb{R}^N$ continua.
\end{defn}
\begin{defn} Le \textit{linee di forza} di un campo vettoriale $\mathbf{F}$ sono definite da:
\begin{equation}
\begin{cases}
 \mathbf{u}'(t)=\mathbf{F}(\mathbf{u}(t)) \\
\\
\mathbf{u}(0)=\mathbf{x}
\end{cases}\;.
\end{equation}
\end{defn}
\section{Integrazione di un campo vettoriale}
\begin{defn} L'integrale di un campo vettoriale $\mathbf{F}$ su una curva orientata $\Gamma\subseteq A$ è dato da:
\begin{equation}
\int_{\Gamma} \langle \mathbf{F},\boldsymbol{\tau}\rangle_N\;\diff{s}\;,
\end{equation}
dove $\boldsymbol{\tau}$ è il versore tangente alla curva. Equivalentemente, parametrizzando la curva come:
\begin{equation}
\Gamma=\{\mathbf{x}=\boldsymbol{\varphi}(t),\;t\in[a,b]\}\;,
\end{equation}
si ha:
\begin{equation}
\int_{\Gamma}\langle\mathbf{F},\boldsymbol{\tau}\rangle_N\;\diff{s}=\int_a^b\left\langle\mathbf{F}(\boldsymbol{\varphi}(t)),\frac{\boldsymbol{\varphi}'(t)}{|\boldsymbol{\varphi}'(t)|_N}\right\rangle_N\;|\boldsymbol{\varphi}'(t)|_N\;\diff{t}=\int_a^b \langle\mathbf{F}(\boldsymbol{\varphi}(t)),\boldsymbol{\varphi}'(t)\rangle_N\;\diff{t}\;.
\end{equation}
Se è $\boldsymbol{\varphi}(t)\equiv(x_1(t),\ldots,x_N(t))$, allora possiamo ancora scrivere:
\begin{equation}
\int_{\Gamma}\langle \mathbf{F},\BF{\tau}\rangle\diff{s}=\sum_{i=1}^N\int_a^b F_i(x_1(t),\ldots,x_N(t))x_i'\;\diff{t}\;.
\end{equation}
\end{defn}
\begin{defn}[Forme differenziali lineari] Una \textit{forma differenziale lineare} è un'applicazione $\omega:A\subseteq\mathbb{R}^N\to(\mathbb{R}^N)'$ (duale) data da:
\begin{align}
\omega(\mathbf{x})&=\sum_{i=1}^N \omega_i(\mathbf{x})\diff{x}_i\;, \notag \\
\mathbf{h} &\longmapsto \sum_{i=1}^N \omega_i(\mathbf{x})h_i\;.
\end{align}
Consideriamo:
\begin{align}
\mathbf{F}(\mathbf{x}) &=\sum_{i=1}^N F_i(\mathbf{x})\mathbf{e}_i\notag \;, \\
\mathbf{F} &\longmapsto \omega=\sum_{i=1}^N F_i\diff{x}_i\;, \notag \\
\mathbf{h} &\longmapsto \omega(\mathbf{h})=\langle \mathbf{F},\mathbf{h}\rangle_N\;.
\end{align}
Allora:
\begin{equation}
\int_{\Gamma} \langle\mathbf{F},\boldsymbol{\tau}\rangle_N\;\diff{s}=\int_{+\Gamma}\sum_{i=1}^N F_i(\mathbf{x})\;\diff{x}_i\;.
\end{equation}
\end{defn}
\begin{oss} L'integrale di un campo vettoriale $\mathbf{F}$ su una curva orientata $\Gamma\subseteq A$, ovvero l'integrale della forma differenziale $\omega=\sum_{i=1}^N F_i(\mathbf{x})\;\diff{x}_i$ su $\Gamma$ può essere scritto come:
\begin{equation}
\int_{+\Gamma}\sum_{i=1}^N F_i(\mathbf{x})\;\diff{x}_i=\int_{+\Gamma}\omega=\int_{\Gamma}\langle\mathbf{F},\boldsymbol{\tau}\rangle_N=\sum_{i=1}^N\int_a^b F_i(\boldsymbol{\varphi}(t))\varphi_i'(t)\;\diff{t}\;.
\end{equation}
\end{oss}
\begin{defn} Un campo vettoriale $\mathbf{F}:A\subseteq\mathbb{R}^N\to\mathbb{R}^N$ continuo si dice \textit{conservativo} se esiste una funzione scalare $f\in C^1(A)$ tale che $\nabla f(\mathbf{x})=\mathbf{F}(\mathbf{x})$ per ogni $\mathbf{x}\in A$.
\end{defn}
\begin{oss} Se $\mathbf{F}$ è un campo vettoriale conservativo di classe $C^1$, allora $D_if=F_i$ e, per il teorema di Schwartz, vale:
\begin{equation}
D_jD_if=\frac{\partial F_i}{\partial x_j}=\frac{\partial F_j}{\partial x_i}=D_iD_j f\;.
\end{equation}
\end{oss}
\begin{oss} La forma differenziale lineare $\omega$ associata al campo vettoriale $\mathbf{F}$ verifica $omega(\mathbf{x})=df(\mathbf{x})$. In questo caso, la forma $\omega$ si dice \textit{esatta}.
\end{oss}
\begin{thm} Sia $A\subseteq\mathbb{R}^N$ aperto e $\mathbf{F}\in C^0(A,\mathbb{R}^N)$ un campo vettoriale. Sono fatti equivalenti:
\begin{enumerate}
 \item $\mathbf{F}$ è conservativo, cioè $\omega$ è esatta;
 \item per ogni curva chiusa $\Gamma\subseteq A$ di classe $C^1$ a tratti si ha:
\begin{equation}
\int_{\pm\Gamma}\omega=0\;;
\end{equation}
\item $\forall \Gamma_1,\Gamma_2\subseteq A$ curve di classe $C^1$ a tratti aventi gli stessi estremi, si ha:
\begin{equation}
\int_{+\Gamma_1}\omega=\int_{+\Gamma_2}\omega\;.
\end{equation}
\end{enumerate}
\end{thm}
\proof $(1)\Longrightarrow(2)$ \\

Sia $\Gamma$ una curva chiusa contenuta in $A$, parametrizzata da $\Gamma=\{\mathbf{x}=\boldsymbol{\varphi}(t),t\in[a,b]\}$. Allora:
\begin{equation}
\int_{+\Gamma}\omega=\sum_{i=1}^N\int_{+\Gamma}D_if(\mathbf{x})\diff{x}_i=\sum_{i=1}^N\int_a^b D_if(\boldsymbol{\varphi}(t))\varphi_i'\;\diff{t}\;.
\end{equation}
Osservando che l'ultimo integrando è la derivata di funzione composta, otteniamo:
\begin{equation}
\int_{+\Gamma}\omega=\int_a^b\frac{\diff}{\diff{t}}f(\boldsymbol{\varphi}(t))\;\diff{t}=f(\boldsymbol{\varphi}(b))-f(\boldsymbol{\varphi}(a))\;.
\end{equation}
Poiché per ipotesi la curva $\Gamma$ è chiusa, si ha $\boldsymbol{\varphi}(b)=\boldsymbol{\varphi}(a)$ e quindi l'integrale risulta nullo.
\endproof
\proof $(2)\Longrightarrow(3)$ \\

Siano $\Gamma_1,\Gamma_2$ come da ipotesi, parametrizzate da:
\begin{align}
&\Gamma_1=\{\mathbf{x}=\BF{\varphi}_1(t), t\in [a_1,b_1]\}, &\Gamma_2=\{\mathbf{x}=\BF{\varphi}_2(t), t\in [a_2,b_2]\}\;.
\end{align}
Definiamo:
\begin{equation}
\boldsymbol{\varphi}(t)=\begin{cases}
                         \boldsymbol{\varphi}_1(t),\quad t\in[a_1,b_1] \\
\\
\boldsymbol{\varphi}_2(-t+b_1+b_2),\quad t\in[b_1,b_1+b_2-a_2]
                        \end{cases}\;.
\end{equation}
Allora $\boldsymbol{\varphi}:[a_1,b_1+b_2-a_2]\to A$ è una curva chiusa che ha come immagine $+\Gamma_1\cup(-\Gamma_2)$. Dunque:
\begin{align}
0 &= \int_{+\Gamma_1\cup(-\Gamma_2)}\omega =\int_{a_1}^{b_1+b_2-a_2}\sum_{i=1}^N F_i(\BF{\varphi}(t))\varphi'_i\;\diff{t}\notag \\
&=\int_{a_1}^{b_1}\sum_{i=1}^NF_i(\BF{\varphi}_1(t))\varphi_{1i}'(t)\diff{t}-\int_{b_1}^{b_1+b_2-a_2}\sum_{i=1}^NF_i(\BF{\varphi_2}(-t+b_2+b_1))\varphi_{2i}'(-t+b_2+b_1)\diff{t}
\end{align}
Posto $s=-t+b_1+b_2$ e $\diff{s}=-\diff{t}$ si ottiene:
\begin{equation}
0=\int_{a_1}^{b_1}\sum_{i=1}^N F_i(\boldsymbol{\varphi}_1(t))\varphi'_{1i}(t)\;\diff{t}-\int_{a_2}^{b_2}\sum_{i=1}^NF_i(\boldsymbol{\varphi}_2(s))\varphi'_{2i}(s)\;\diff{s}=\int_{+\Gamma_1}\omega-\int_{+\Gamma_2}\omega\;.
\end{equation}
Da cui:
\begin{equation}
\int_{+\Gamma_1}\omega=\int_{+\Gamma_2}\omega\;.
\end{equation}
\endproof
\proof $(3)\Longrightarrow(1)$ \\

Sia $\mathbf{x}_0\in A$ fisso e $\mathbf{x}\in A$ congiungibile a $\mathbf{x}_0$ con una curva $C^1$ a tratti contenuta in $A$. Definiamo:
\begin{equation}
f(\mathbf{x})=\int_{+\Gamma_{\mathbf{x}_0,\mathbf{x}}} \omega\;,
\end{equation}
dove $\Gamma_{\mathbf{x}_0,\mathbf{x}}$ è una qualunque curva che ha per estremi $\mathbf{x}_0$ e $\mathbf{x}$. Sia $h\in\mathbb{R}$ non nullo e sufficientemente piccolo. Allora:
\begin{equation}
\frac{f(\mathbf{x}+h\mathbf{e}_i)-f(\mathbf{x})}{h}=\frac{1}{h}\left[\int_{+\Gamma_{\mathbf{x}_0,\mathbf{x}+h\mathbf{e}_i}} \omega -
\int_{+\Gamma_{\mathbf{x}_0,\mathbf{x}}} \omega\right]\;.
\end{equation}
Poniamo $T_{\mathbf{x},\mathbf{x}+h\mathbf{e}_i}$ la curva $+\Gamma_{\mathbf{x}_0,\mathbf{x}+h\mathbf{e}_i}-\Gamma_{\mathbf{x}_0,\mathbf{x}}$, allora:
\begin{equation}
\frac{f(\mathbf{x}+h\mathbf{e}_i)-f(\mathbf{e}_i)}{h}=\frac{1}{h}\int_{T_{\mathbf{x},\mathbf{x}+h\mathbf{e}_i}}\omega\;.
\end{equation}
Parametrizziamo $\omega$ con $\boldsymbol{\varphi}(t)=\mathbf{x}+th\mathbf{e}_j,\;t\in[0,1]$:
\begin{equation}
\frac{f(\mathbf{x}+h\mathbf{e}_i)-f(\mathbf{x})}{h}=\frac{1}{h}\int_0^1\sum_{j=1}^N F_j(\mathbf{x}+th\mathbf{e}_j)\delta_{ij}h\;\diff{t}=\int_0^1 F_i(\mathbf{x}+th\mathbf{e}_i)\;\diff{t}\;.
\end{equation}
Per $h\to 0$:
\begin{equation}
\int_0^1 F_i(\mathbf{x}+th\mathbf{e}_i)\;\diff{t} \to F_i(\mathbf{x})\;.
\end{equation}
Dunque per ogni $i$ esiste continua la derivata parziale rispetto all'$i$-esima componente di $f$ e vale $D_if=F_i$. Per il teorema del 
differenziale totale, segue che $f$ è differenziabile e $\nabla f=\mathbf{F}$.
\endproof
\begin{defn} Una forma differenziale $\omega=\sum_{i=1}^N F_i\;\diff{x}_i$ si dice \textit{chiusa} se $\mathbf{F}\in C^1$ e $D_iF_j=D_jF_i$.
\end{defn}
\begin{oss} Se $\omega$ è esatta allora è chiusa. Il viceversa non è sempre vero.
\end{oss}
\begin{defn} Un aperto $A$ si definisce \textit{semplicemente connesso} se:
\begin{itemize}
 \item $A$ è connesso;
 \item ogni curva chiusa contenuta in $A$ può essere deformata con continuità ad un punto senza uscire da $A$.
\end{itemize}
\end{defn}
\begin{oss} Se $\omega$ è una forma differenziale lineare chiusa su un aperto $A$ semplicemente connesso, allora $\omega$ è esatta.
\end{oss}
\section{Formule di Gauss-Green}
Sia $A\subseteq\mathbb{R}^2$ un aperto limitato con $\partial A$ di classe $C^1$. $\forall \mathbf{x}_0\in\partial A$ esiste un intorno 
$U\subseteq\mathbb{R}^2$ di $\mathbf{x}_0$ tale che $U\cap\partial A$ è grafico di una funzione $C^1$. In tal caso si avrà:
\begin{equation}
\overline{A}=\bigcup_{i=1}^k A_i\;,
\end{equation}
con gli $A_i$ privi di punti interni comuni ed insiemi normali rispetto a uno degli assi. Denotiamo con $+\partial A$ il verso antiorario.
\begin{thm}[Gauss-Green]  Sia $A\subseteq\mathbb{R}^2$ soddisfacente le condizioni di cui sopra e sia $f\in C^1(\overline{A})$. Allora:
\begin{enumerate}
\item
\begin{equation}
\int_A \frac{\partial f}{\partial x}\;\diff{x}\diff{y}=\int_{+\partial A} f\;\diff{y}=\int_{\partial A} f\;n_1\;\diff{s}\;;
\end{equation}
\item
\begin{equation}
\int_A \frac{\partial f}{\partial y}\;\diff{x}\diff{y}=\int_{-\partial A} f\;\diff{x}=\int_{\partial A} f\;n_2\;\diff{s}\;;
\end{equation}
\end{enumerate}
dove $\mathbf{n}=(n_1,n_2)$ è il vettore normale esterno alla frontiera.
\end{thm}
\proof Possiamo ridurci (non è restrittivo) al caso in cui $A$ sia un insieme normale, dato da:
\begin{equation}
A=\left\{(x,y)\in\mathbb{R}^2\;|\; x\in[a,b],y\in[\alpha(x),\beta(x)],\alpha,\beta\in C^1([a,b]),\alpha\le\beta\right\}\;.
\end{equation}
Siano $\Gamma_1,\Gamma_2,\Gamma_3,\Gamma_4$ i quattro pezzi della frontiera di $A$, parametrizzati da:
\begin{align}
&\Gamma_1=\begin{cases}
x=a \\
\\
y=y
\end{cases}, &y\in [\alpha(a),\beta(a)],\; \mathbf{n}=(-1,0)\;, \notag \\
&\Gamma_2=\begin{cases}
x=x \\
\\
y=\alpha(x)
\end{cases}, &x\in[a,b],\; \mathbf{n}=(\alpha'(x),-1)\;, \notag \\
&\Gamma_3=\begin{cases}
x=b \\
\\
y=y
\end{cases}, &y\in [\alpha(b),\beta(b)],\; \mathbf{n}=(1,0)\;, \notag \\
&\Gamma_4=\begin{cases}
x=x \\
\\
y=\beta(x)
\end{cases}, & x\in [a,b],\; \mathbf{n}=(-\beta'(x),1)\;.
\end{align}
Il secondo membro della seconda uguaglianza diventa allora:
\begin{equation}
\int_{\partial A} f n_2\;\diff{s}=-\int_a^b f(x,\alpha(x))\;\diff{x}+\int_a^b f(x,\beta(x))\;\diff{x}=\int_a^b\left[f(x,\beta(x))-f(x,\alpha(x))\right]\diff{x}\;.
\end{equation}
Mentre il primo membro diventa:
\begin{equation}
\int_A \frac{\partial f}{\partial y}\diff{x}\diff{y}=\int_a^b\int_{\alpha(x)}^{\beta(x)} \frac{\partial f}{\partial y}\diff{y}\diff{x}\;.
\end{equation}
Per il teorema fondamentale del calcolo integrale, segue:
\begin{equation}
\int_A\pdev{f}{y}\diff{x}\diff{y}=\int_a^b\left[f(x,\beta(x))-f(x,\alpha(x))\right]\diff{x}\;,
\end{equation}
che dimostra la seconda uguaglianza. Per la prima invece, a secondo membro si ha:
\begin{align}
\int_A fn_1\;\diff{s}= &\int_{\alpha(a)}^{\beta(a)}-f(a,y)\diff{y}+\int_a^bf(x,\alpha(x))\alpha'(x)\diff{x}\notag \\
&+\int_{\alpha(b)}^{\beta(b)}f(b,y)\;\diff{y}+\int_a^b-f(x,\beta(x))\beta'(x)\;\diff{x}\;.
\end{align}
Mentre a primo membro:
\begin{equation}
\int_A\frac{\partial f}{\partial x}\;\diff{x}\diff{y}=\int_a^b\int_{\alpha(x)}^{\beta(x)}\frac{\partial f}{\partial x}\;\diff{x}\diff{y}\;.\label{ch8_gg1}
\end{equation}
Consideriamo le funzioni:
\begin{align}
&G(x)=\int_{\alpha(x)}^{\beta(x)}f(x,y)\;\diff{y}, & F(u,v,x)=\int_u^v f(x,y)\;\diff{y}\;.
\end{align}
Avremo allora:
\begin{align}
F_v &= f(x,v)\;, \notag \\
F_u &= -f(x,u)\;, \notag \\
F_x &= \int_u^v \pdev{f}{x}(x,y)\;\diff{y}\;.
\end{align}
Inoltre, $G(x)=F(\alpha(x),\beta(x),x)$. Dunque $G$ è composizione di funzioni di classe $C^1$, e dunque anch'essa sarà di classe $C^1$. 
In virtù di ciò, possiamo riscrivere le derivate parziali come:
\begin{align}
F_v &= f(x,\beta(x))\beta'(x)\;, \notag \\
F_u &=-f(x,\alpha(x))\alpha'(x)\;, \notag \\
F_x &= \int_{\alpha(x)}^{\beta(x)}\pdev{f}{x}\;\diff{y}\;.
\end{align}
Pertanto, si ottiene:
\begin{equation}
\int_{\alpha(x)}^{\beta(x)}\frac{\partial f}{\partial x}\;\diff{y} =\frac{\diff}{\diff{x}}\left[\int_{\alpha(x)}^{\beta(x)} f(x,y)\;\diff{y}\right]-
f(x,\beta(x))\beta'(x)+f(x,\alpha(x))\alpha'(x)\;.
\end{equation}
Sostituendo l'espressione appena trovata nella \eqref{ch8_gg1}, otteniamo:
\begin{align}
\int_A\pdev{f}{x}\;\diff{x}\diff{y} = &\int_a^b\frac{\diff}{\diff{x}}\left[\int_{\alpha(x)}^{\beta(x)}f(x,y)\;\diff{y}\right]\diff{x}-\int_a^bf(x,\beta(x))\beta'(x)\;\diff{x} \notag \\
&+\int_a^bf(x,\alpha(x))\alpha'(x)\;\diff{x}\;.
\end{align}
Dal teorema fondamentale, segue che:
\begin{align}
\int_A\pdev{f}{x}\;\diff{x}\diff{y}= &\int_{\alpha(b)}^{\beta(b)} f(b,y)\;\diff{y}-\int_{\alpha(a)}^{\beta(a)}f(a,y)\;\diff{y} \notag \\
&+\int_a^b f(x,\alpha(x))\alpha'(x)\;\diff{x}-\int_a^b f(x,\beta(x))\beta'(x)\;\diff{x}\;.
\end{align}
\endproof
\begin{exm} Sia $A\subseteq\mathbb{R}^2$ un aperto limitato, con $\partial A\in C^1$. Allora:
\begin{equation}
m_2(A)=\int_{+\partial A}x\;\diff{y}=\int_{-\partial A}y\;\diff{x}=\frac{1}{2}\int_{+\partial A}(x\;\diff{y}-y\;\diff{x})\;.
\end{equation}
\end{exm}
\begin{exm} Consideriamo la cicloide $\boldsymbol{\varphi}(t)=(t\sin t,1-\cos t),t\in[0,2\pi]$ e sia $\partial A=\boldsymbol{\varphi}([0,2\pi])$. Allora, per il teorema di Gauss-Green, si ha:
\begin{equation}
m_2(A)=-\int_0^{2\pi} (t-\sin t)\sin t\;\diff{t}=-\int_0^{2\pi}t\sin t\;dt+\int_0^{2\pi}\sin^2 t\;\diff{t} =2\pi+\pi=3\pi\;.
\end{equation}
\end{exm}
\section{Divergenza, rotore e teorema di Stokes}
\begin{defn} Sia $\mathbf{F}=(f,g)$ un campo vettoriale di classe $C^1$. Si definisce \textit{divergenza} di $\mathbf{F}$ la quantità:
\begin{equation}
\Div \mathbf{F}\equiv \frac{\partial f}{\partial x}+\frac{\partial g}{\partial y}\;.
\end{equation}
\end{defn}
\begin{thm}[della divergenza] Sia $\mathbf{F}(f,g)$ un campo vettoriale di classe $C^1(A,\mathbb{R}^2)$, con $A\subseteq\mathbb{R}^2$ aperto e limitato, avente frontiera $\partial A$ di classe $C^1$. Allora:
\begin{equation}
\int_A \Div \mathbf{F}\;\diff{x}\diff{y}=\int_{\partial A}\langle \mathbf{F},\mathbf{n}\rangle_2\;\diff{s}\;.
\end{equation}
\end{thm}
\proof Dalla definizione di divergenza, si ha:
\begin{equation}
\int_A \Div \mathbf{F}\;\diff{x}\diff{y}=\int_A \left(\frac{\partial f}{\partial x}+\frac{\partial g}{\partial y}\right)\diff{x}\diff{y}=\int_A\frac{\partial f}{\partial x}\;\diff{x}\diff{y}+\int_A \frac{\partial g}{\partial y}\;\diff{x}\diff{y}\;.
\end{equation}
Applicando il teorema di Gauss-Green, si ottiene:
\begin{equation}
\int_A\Div\mathbf{F}\;\diff{x}\diff{y}=\int_{\partial A}fn_1\;\diff{s}+\int_{\partial A}gn_2\;\diff{s}=\int_{\partial A}(fn_1+gn_2)\diff{s}=\int_{\partial A}\langle \mathbf{F},\mathbf{n}\rangle_2\;\diff{s}\;.
\end{equation}
\endproof
\begin{thm}[Stokes] Siano $A\subseteq\mathbb{R}^3$ aperto, $\mathbf{F}:A\to\mathbb{R}^3$ un campo vettoriale di classe $C^1$ e $\Sigma$ una superficie orientata, dotata di bordo $\Gamma$ orientato in modo coerente. Allora:
\begin{equation}
\int_{\Sigma} \langle \Rot \mathbf{F},\mathbf{n}\rangle_3\;\diff{\sigma}=\int_{\Gamma} \langle\mathbf{F},\BF{\tau}\rangle_3\;\diff{s}\;,
\end{equation}
dove $\BF{\tau}$ è il versore tangente a $\Gamma$, $\mathbf{n}$ è il versore normale a $\Sigma$ e:
\begin{equation}
\Rot \mathbf{F}\equiv \det\left(\begin{matrix}
                            \mathbf{i} & \mathbf{j} & \mathbf{k} \\
D_x & D_y & D_z \\
F_1 & F_2 & F_3
                           \end{matrix}\right)=\left(\begin{matrix}
(F_3)_y-(F_2)_z \\
(F_1)_z-(F_3)_x \\
(F_2)_x-(F_1)_y
\end{matrix}\right)\;.
\end{equation}
\end{thm}
\proof Poniamo $\Sigma=\BF{\sigma}(T),T\subseteq \mathbb{R}^2$ e $\BF{\sigma}\in C^2$, con $\partial T=\BF{\gamma}([a,b])$, essendo $\BF{\sigma},\BF{\gamma}$ date da:
\begin{align}
&\BF{\sigma}(u,v)=\left(x(u,v),y(u,v),z(u,v)\right), &\BF{\gamma}(t)=\left(\xi(t),\eta(t)\right)\;.
\end{align}
Supponiamo inoltre $\Gamma=\BF{\sigma}(\partial T)$ (non è restrittivo). Sia $\mathbf{F}=(P,Q,R)$, con $P,Q,R$ funzioni scalari. 
Definiamo:
\begin{align}
&\overline{\mathbf{F}}=\mathbf{F}(\BF{\sigma}(u,v)), &\hat{\mathbf{F}}=\mathbf{F}(\BF{\sigma}(\BF{\gamma}(t)))\;,
\end{align}
da cui segue che $\hat{\mathbf{F}}=\ov{\mathbf{F}}\circ \BF{\gamma}$. Essendo $\Gamma=\{\BF{\sigma}(\BF{\gamma}(t)),t\in [a,b]\}$, si ha:
\begin{align}
\int_{\Gamma}\langle\mathbf{F},\BF{\tau}\rangle_3\;\diff{s} &=\int_a^b\langle\hat{\mathbf{F}},(\BF{\sigma}\circ\BF{\gamma})'\rangle_3\;\diff{t}\notag \\
&=\int_a^b\left[\hat{P}(x_u\xi'+x_v\eta')+\hat{Q}(y_u\xi'+y_v\eta')+\hat{R}(z_u\xi'+z_v\eta')\right]\diff{t} \label{ch8_stokes}\;.
\end{align}
Il vettore tangente esterno a $T$ normalizzato è:
\begin{equation}
\BF{\nu}=\frac{(\eta',-\xi')}{|\BF{\gamma}'|_2}\;.
\end{equation}
Sostituendo nella \eqref{ch8_stokes}, si ottiene:
\begin{align}
\int_{\Gamma}\langle\mathbf{F},\BF{\tau}\rangle_3\;\diff{s} &=\int_a^b\left[\hat{P}(-x_u\nu_2+x_v\nu_1)+\hat{Q}(-y_u\nu_2+y_v\nu_1)+\hat{R}(-z_u\nu_2+z_v\nu_1)\right]\BF{\gamma}'|_2\;\diff{t} \notag \\
&=\int_a^b\left[\nu_1(\hat{P}x_v+\hat{Q}y_v+\hat{R}z_v)-\nu_2(\hat{P}x_u+\hat{Q}y_u+\hat{R}z_u)\right]\BF{\gamma}|_2\;\diff{t} \notag \\
&=\int_{\partial T}\left(\langle\hat{\mathbf{F}},\BF{\sigma}_v\circ\BF{\gamma}\rangle_3\nu_1-\langle\hat{\mathbf{F}},\BF{\sigma}_u\circ\BF{\gamma}\rangle_3\nu_2\right)\;\diff{s}\;.
\end{align}
Dal teorema della divergenza, segue che:
\begin{align}
\int_{\Gamma}\langle\mathbf{F},\BF{\tau}\rangle_3\diff{s}&=\int_T\left(\frac{\partial}{\partial u}\langle\ov{\mathbf{F}},\BF{\sigma}_v\rangle_3-\frac{\partial}{\partial v}\langle \ov{\mathbf{F}},\BF{\sigma}_u\rangle_3\right)\diff{u}\diff{v} \notag \\
&=\int_T \left[\langle\mathbf{F}_xx_u+\mathbf{F}_yy_u+\mathbf{F}_zz_u,\BF{\sigma}_v\rangle_3+\langle \ov{\mathbf{F}},\BF{\sigma}_{uv}
\rangle_3-\right. \notag \\
&-\left.\langle\mathbf{F}_xx_v+\mathbf{F}_yy_v+\mathbf{F}_zz_v,\BF{\sigma}_u\rangle_3-\langle\ov{\mathbf{F}},\BF{\sigma}_{vu}\rangle_3
\right]\diff{u}\diff{v}\;.
\end{align}
Poiché $\BF{\sigma}\in C^2$, allora per il teorema di Schwartz $\BF{\sigma}_{uv}=\BF{\sigma}_{vu}$ e quindi i termini contenenti le 
derivate seconde miste risultano opposti e si elidono. Sviluppando infine i prodotti scalari, si ha:
\begin{align}
\int_{\Gamma}\langle\mathbf{F},\BF{\tau}\rangle_3\diff{s}&=\int_T\left[(\ov{P}_xx_u+\ov{P}_yy_u+\ov{P}_zz_u)x_v+(\ov{Q}_xx_u+\ov{Q}_yy_u+\ov{Q}_zz_u)y_v\right. \notag \\
&+(\ov{R}_xx_u+\ov{R}_yy_u+\ov{R}_zz_u)z_v-(\ov{P}_xx_v+\ov{P}_yy_v+\ov{P}_zz_v)x_u \notag \\
&-\left.(\ov{Q}_xx_v+\ov{Q}_yy_v+\ov{Q}_zz_v)z_u-(\ov{R}_xx_v+\ov{R}_yy_v+\ov{R}_zz_u)\right]\diff{u}\diff{v} \notag \\
&=\int_T\left[(\ov{Q}_x-\ov{P}_y)(x_uy_v-x_vy_u)+(\ov{P}_z-\ov{R}_x)(z_ux_v-z_vx_u)+\right. \notag \\
&\left.+(\ov{R}_y-\ov{Q}_z)(y_uz_v-y_vz_u)\right]\diff{u}\diff{v}=\int_T\langle \Rot\mathbf{F},\BF{\sigma}_u\times\BF{\sigma}_v\rangle_3\;\diff{u}\diff{v}\;.
\end{align}
Poiché:
\begin{align}
&\mathbf{n}=\frac{\BF{\sigma}_u\times\BF{\sigma}_v}{|\BF{\sigma}_u\times\BF{\sigma}_v|_3}, &\diff{\sigma}=\diff{u}\diff{v}|\BF{\sigma}_U\times\BF{\sigma}_v|_3\;,
\end{align}
si ottiene:
\begin{equation}
\int_{\Gamma}\langle\mathbf{F},\BF{\tau}\rangle_3\diff{s}=\int_{\Sigma} \langle \Rot \mathbf{F},\mathbf{n}\rangle_3\;\diff{\sigma}\;.
\end{equation}
\endproof
\begin{thm}[della divergenza in tre dimensioni] Siano $A\subseteq\mathbb{R}^3$ aperto e limitato con $\partial A\in C^1$ e $\mathbf{F}\in C^1(A,\mathbb{R}^3)$ un campo vettoriale. Allora:
\begin{equation}
\int_A \Div \mathbf{F}\;\diff{x}\diff{y}\diff{z}=\int_{\partial A} \langle \mathbf{F},\mathbf{n}\rangle_3\;\diff{s}\;,
\end{equation}
dove $\mathbf{n}$ è il vettore normale esterno.
\end{thm}
\end{document}
